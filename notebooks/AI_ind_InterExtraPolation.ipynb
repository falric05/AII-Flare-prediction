{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 07:18:24.695558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 07:18:24.835191: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-28 07:18:24.835220: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-28 07:18:25.738076: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 07:18:25.738336: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 07:18:25.738351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, auc, roc_curve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from Libs.models import make_lstm\n",
    "from Libs.config import inter_extra_data_folder, models_InterExtra_folder\n",
    "from Libs.load_data import DataLoader, get_dataset_split\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biggest assumption when training ANNs is the following: \n",
    "\n",
    "\"We assume that training sets and test sets contains independent and identically distributed samples from the same unknown distribution $p_{data}(x,y)$\"\n",
    "\n",
    "This is a very important assumption that in general affect the performance ANNs, in particular classifier ones. We could, indeed, explore what can happen if we violete the following assumption. This a relevant application case, for exaple in cases when the generation parameters are not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1, 4, 1, 1, 1000), (100, 1, 4, 1, 1, 1000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flag which says if we need to re-train the models for this experiment, default is False\n",
    "override_lstm = False\n",
    "# flag which says if the time series need to be standardize or not, default is False \n",
    "F_std = False\n",
    "\n",
    "# initialize data loader\n",
    "data_loader = DataLoader(run=100, N=1000, s=0.5, t=[0.01, 0.1, 0.5, 3], d=0.2, m=1, \n",
    "                         override=False, folder=inter_extra_data_folder)\n",
    "# get the grid\n",
    "grid_X, grid_y = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "\n",
    "grid_X.shape, grid_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to split the dataset in training, validation and test set according to a given parameter modality (i.e. `mide`). If this parameter is not `all` the training and validation data and test data will be generater in two different ways (i.e. for inteporlation extreme values of theta will be used for training and internal parameters for test, while for extrapolation extreme values of theta will be used for test and internal parameters for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(data1, mode, data2=None, F_std=False, overlap_size=15):\n",
    "    assert mode in ['all', 'interpolation', 'extrapolation']    \n",
    "\n",
    "    # params commons\n",
    "    dataset_split_params = {\n",
    "        'window_size': 20, # how large is the window\n",
    "        'overlap_size': overlap_size, # how many time interval of overlap there is between the windows\n",
    "        'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "        'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "        'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "        'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "        'test_size': 0.3, # size of the test set expressed in percentage\n",
    "        'val_size': 0.2, # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "        'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "        'random_state': 42, # sets the seed for reproducibility\n",
    "        'get_info':False, # Extends windows dataframe with infos on the params, window_range and label_range\n",
    "        'params': params # needed when get_info is True\n",
    "    }\n",
    "\n",
    "    if mode in ['interpolation', 'extrapolation']:\n",
    "        assert not data2 is None\n",
    "        grid_X_train, grid_y_train = data1\n",
    "        grid_X_test, grid_y_test   = data2\n",
    "        # get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "        # notice that theta is the third parameter\n",
    "        df_train, df_val, _ = get_dataset_split(grid_X_train, grid_y_train, **dataset_split_params)\n",
    "        # get the test set, selecting the index for grid given the interpolation assuption\n",
    "        # notice that theta is the third parameter\n",
    "        _, _, df_test = get_dataset_split(grid_X_test, grid_y_test, **dataset_split_params)\n",
    "    elif mode in ['all']:\n",
    "        grid_X, grid_y = data1\n",
    "        # get all the dataset from a single list\n",
    "        df_train, df_val, df_test = get_dataset_split(grid_X, grid_y, **dataset_split_params)\n",
    "    else:\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    # number of classes\n",
    "    print('Training set:')\n",
    "    train_counts = df_train['future_flare'].value_counts()\n",
    "    print(train_counts, '\\n')\n",
    "    print('validation set:')\n",
    "    val_counts = df_val['future_flare'].value_counts()\n",
    "    print(val_counts, '\\n')\n",
    "    print('Test set:')\n",
    "    test_counts = df_test['future_flare'].value_counts()\n",
    "    print(test_counts, '\\n')\n",
    "    print('Total:')\n",
    "    total_counts = train_counts.add(val_counts).add(test_counts)\n",
    "    print(total_counts, '\\n')\n",
    "    print()\n",
    "    \n",
    "    # compute the initial bias to pass then to the model\n",
    "    initial_bias = [np.log(train_counts[0]/train_counts[1])]\n",
    "\n",
    "    # check the shape\n",
    "    X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "    X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "    X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "    print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "    print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)\n",
    "\n",
    "    # finally, if requested, standardize the dataset\n",
    "    if F_std:\n",
    "        # Standardize Data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_std = scaler.transform(X_train)\n",
    "        X_val_std = scaler.transform(X_val)\n",
    "        X_test_std = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_std = X_train\n",
    "        X_val_std = X_val\n",
    "        X_test_std = X_test\n",
    "\n",
    "\n",
    "    # finally return the dataset\n",
    "    return X_train_std, y_train, X_val_std, y_val, X_test_std, y_test, initial_bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will evaluate the score obtained with predictions on validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_val, y_val, X_test, y_test):\n",
    "    # Validation set\n",
    "    y_pred = np.round(model.predict(X_val), 0)\n",
    "    print(\"### Evaluation on validation set ###\")\n",
    "    print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "    print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val, average='macro')))\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred, pos_label=1)\n",
    "    print('AUC:', auc(fpr, tpr))\n",
    "    #Create confusion matrix and normalizes it over predicted (columns)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # Test set\n",
    "    y_pred = np.round(model.predict(X_test), 0)\n",
    "    print(\"### Evaluation on test set ###\")\n",
    "    print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "    print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test, average='macro')))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
    "    print('AUC:', auc(fpr, tpr))\n",
    "    #Create confusion matrix and normalizes it over predicted (columns)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model with multiple all theta parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start considering for both training and test set the same data distribution, namely dataset produced with all the four theta parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    25172\n",
      "1    12656\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    10367\n",
      "1     5845\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    15145\n",
      "1     8015\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    50684\n",
      "1    26516\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (37828, 20) Val: (16212, 20) Test: (23160, 20)\n",
      "y ## Train: (37828,) Val: (16212,) Test: (23160,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set((grid_X.copy(), grid_y.copy()), 'all', F_std=F_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 07:18:27.599354: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-28 07:18:27.599452: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-28 07:18:27.599481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (44910f15382a): /proc/driver/nvidia/version does not exist\n",
      "2023-03-28 07:18:27.599801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_all_path = os.path.join(models_InterExtra_folder, 'std', \"LSTM_allTheta_checkpoint.h5\") if F_std else \\\n",
    "                os.path.join(models_InterExtra_folder, 'not_std', \"LSTM_allTheta_checkpoint.h5\")\n",
    "model_all = make_lstm((X_train.shape[1], 1), output_bias=initial_bias)\n",
    "model_all.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=[f1_m, 'accuracy'])\n",
    "print(model_all.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if override_lstm:\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "    # define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            lstm_all_path, save_weights_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "    ]\n",
    "    # fit model\n",
    "    model_all.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "    )\n",
    "model_all.load_weights(lstm_all_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507/507 [==============================] - 3s 6ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.73\n",
      "AUC: 0.7260208434293755\n",
      "[[8262 2105]\n",
      " [2016 3829]]\n",
      "\n",
      "724/724 [==============================] - 4s 5ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.74\n",
      "F1 score: 0.72\n",
      "AUC: 0.7157500988057429\n",
      "[[12098  3047]\n",
      " [ 2944  5071]]\n"
     ]
    }
   ],
   "source": [
    "eval(model_all, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation with using just the extreme parameters: \n",
    "\n",
    "$\\theta=0.01$ and $\\theta=3$\n",
    "\n",
    "and a fraction of the other dataset, coming from $\\theta=0.1$ and $\\theta=0.5$ as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    13576\n",
      "1     5338\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    5630\n",
      "1    2476\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    7015\n",
      "1    4565\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    26221\n",
      "1    12379\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (18914, 20) Val: (8106, 20) Test: (11580, 20)\n",
      "y ## Train: (18914,) Val: (8106,) Test: (11580,)\n"
     ]
    }
   ],
   "source": [
    "p = 'theta'\n",
    "# train configurations\n",
    "theta_train_list     = [0.01, 3]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "data_train = (grid_X[:,:,theta_train_list_idx,:,:,:].copy(), grid_y[:,:,theta_train_list_idx,:,:,:].copy())\n",
    "# test configuration\n",
    "theta_test_list      = [0.1, 0.5]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "data_test = (grid_X[:,:,theta_test_list_idx,:,:,:].copy(), grid_y[:,:,theta_test_list_idx,:,:,:].copy())\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set(data_train, 'interpolation', data2=data_test, F_std=F_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_interp_path = os.path.join(models_InterExtra_folder, \"std\", \"LSTM_intrpTheta_checkpoint.h5\") if F_std else \\\n",
    "                   os.path.join(models_InterExtra_folder, \"not_std\", \"LSTM_intrpTheta_checkpoint.h5\")\n",
    "model_interp = make_lstm((X_train.shape[1], 1), output_bias=initial_bias)\n",
    "model_interp.compile(loss='binary_crossentropy', \n",
    "                     optimizer='adam', \n",
    "                     metrics=[f1_m, 'accuracy'])\n",
    "print(model_interp.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if override_lstm:\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "    # define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            lstm_interp_path, save_weights_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "    ]\n",
    "    # fit model\n",
    "    model_interp.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "    )\n",
    "model_interp.load_weights(lstm_interp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 1s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.74\n",
      "F1 score: 0.72\n",
      "AUC: 0.7451789398474018\n",
      "[[4100 1530]\n",
      " [ 589 1887]]\n",
      "\n",
      "362/362 [==============================] - 2s 5ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.73\n",
      "AUC: 0.7195498770823592\n",
      "[[6000 1015]\n",
      " [1900 2665]]\n"
     ]
    }
   ],
   "source": [
    "eval(model_interp, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation without using the extreme parameters: \n",
    "\n",
    "$\\theta=0.1$ and $\\theta=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    11596\n",
      "1     7318\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    4737\n",
      "1    3369\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    8130\n",
      "1    3450\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    24463\n",
      "1    14137\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (18914, 20) Val: (8106, 20) Test: (11580, 20)\n",
      "y ## Train: (18914,) Val: (8106,) Test: (11580,)\n"
     ]
    }
   ],
   "source": [
    "p = 'theta'\n",
    "# train configurations\n",
    "theta_train_list     = [0.1, 0.5]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "data_train = (grid_X[:,:,theta_train_list_idx,:,:,:].copy(), grid_y[:,:,theta_train_list_idx,:,:,:].copy())\n",
    "# test configuration\n",
    "theta_test_list      = [0.01, 3]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "data_test = (grid_X[:,:,theta_test_list_idx,:,:,:].copy(), grid_y[:,:,theta_test_list_idx,:,:,:].copy())\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set(data_train, 'extrapolation', F_std=F_std,\n",
    "                                                                            data2=data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_extrp_path = os.path.join(models_InterExtra_folder, \"std\", \"LSTM_extrpTheta_checkpoint.h5\") if F_std else \\\n",
    "                   os.path.join(models_InterExtra_folder, \"not_std\", \"LSTM_extrpTheta_checkpoint.h5\")\n",
    "model_extrp = make_lstm((X_train.shape[1], 1), output_bias=initial_bias)\n",
    "model_extrp.compile(loss='binary_crossentropy', \n",
    "                    optimizer='adam', \n",
    "                    metrics=[f1_m, 'accuracy'])\n",
    "print(model_extrp.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if override_lstm:\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "    # define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            lstm_extrp_path, save_weights_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "    ]\n",
    "    # fit model\n",
    "    model_extrp.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "    )\n",
    "model_extrp.load_weights(lstm_extrp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 2s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.73\n",
      "AUC: 0.7277705811903826\n",
      "[[4132  605]\n",
      " [1404 1965]]\n",
      "\n",
      "362/362 [==============================] - 2s 5ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.74\n",
      "F1 score: 0.67\n",
      "AUC: 0.6649606930851917\n",
      "[[6924 1206]\n",
      " [1800 1650]]\n"
     ]
    }
   ],
   "source": [
    "eval(model_extrp, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigation in extrapolation with greater overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    57784\n",
      "1    36394\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    23613\n",
      "1    16749\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    40453\n",
      "1    17207\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    121850\n",
      "1     70350\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (94178, 20) Val: (40362, 20) Test: (57660, 20)\n",
      "y ## Train: (94178,) Val: (40362,) Test: (57660,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set(data_train, 'extrapolation', F_std=F_std, \n",
    "                                                                            data2=data_test, overlap_size=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_3 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                310       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_extrp19_path = os.path.join(models_InterExtra_folder, \"std\", \"LSTM_extrpTheta19_checkpoint.h5\") if F_std else \\\n",
    "                    os.path.join(models_InterExtra_folder, \"not_std\", \"LSTM_extrpTheta19_checkpoint.h5\")\n",
    "model_extrp19 = make_lstm((X_train.shape[1], 1), output_bias=initial_bias)\n",
    "model_extrp19.compile(loss='binary_crossentropy', \n",
    "                      optimizer='adam', \n",
    "                      metrics=[f1_m, 'accuracy'])\n",
    "print(model_extrp19.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if override_lstm:\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "    # define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            lstm_extrp19_path, save_weights_only=True, monitor=\"val_loss\"\n",
    "        ),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "    ]\n",
    "    # fit model\n",
    "    model_extrp19.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "    )\n",
    "model_extrp19.load_weights(lstm_extrp19_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262/1262 [==============================] - 6s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.74\n",
      "AUC: 0.7305525378749167\n",
      "[[20472  3141]\n",
      " [ 6798  9951]]\n",
      "\n",
      "1802/1802 [==============================] - 12s 7ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.74\n",
      "F1 score: 0.69\n",
      "AUC: 0.6815823734257996\n",
      "[[33748  6705]\n",
      " [ 8106  9101]]\n"
     ]
    }
   ],
   "source": [
    "eval(model_extrp19, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\\[1\\] _On the distribution of fluxes of gamma-ray blazars: hints for a stochastic process?_, Tavecchio et al., [https://arxiv.org/pdf/2004.09149.pdf](https://arxiv.org/pdf/2004.09149.pdf)\n",
    "<!-- cite with: [\\[1\\]](https://arxiv.org/pdf/2004.09149.pdf)  -->\n",
    "\\[2\\] _Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline_, Wang et al., [https://arxiv.org/abs/1611.06455](https://arxiv.org/abs/1611.06455)\n",
    "<!-- cite with: [\\[2\\]](https://arxiv.org/abs/1611.06455)  -->\n",
    "\\[3\\] _Solar Flare Prediction Based on the Fusion of Multiple Deep-learning Models_, Tang et al., [https://iopscience.iop.org/article/10.3847/1538-4365/ac249e/meta](https://iopscience.iop.org/article/10.3847/1538-4365/ac249e/meta)\n",
    "<!-- cite with: [\\[3\\]](https://iopscience.iop.org/article/10.3847/1538-4365/ac249e/meta)  -->\n",
    "\\[4\\] _Predicting Solar Energetic Particles Using SDO/HMI Vector Magnetic Data Products and a Bidirectional LSTM Network_, Abduallah et al., [https://iopscience.iop.org/article/10.3847/1538-4365/ac5f56/meta](https://iopscience.iop.org/article/10.3847/1538-4365/ac5f56/meta)\n",
    "<!-- cite with: [\\[4\\]](https://iopscience.iop.org/article/10.3847/1538-4365/ac5f56/meta) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
