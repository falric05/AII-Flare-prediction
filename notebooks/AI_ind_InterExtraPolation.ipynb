{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 11:52:39.346747: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-25 11:52:39.516855: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-25 11:52:39.516882: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-25 11:52:40.456728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 11:52:40.456893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-25 11:52:40.456906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, auc, roc_curve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from Libs.config import inter_extra_data_folder\n",
    "from Libs.load_data import DataLoader, get_dataset_split\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 59.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((30, 1, 4, 1, 1, 1000), (30, 1, 4, 1, 1, 1000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_std = False\n",
    "\n",
    "# initialize data loader\n",
    "data_loader = DataLoader(run=30, N=1000, s=0.5, t=[0.01, 0.1, 0.5, 3], d=0.2, m=1, \n",
    "                         override=False, folder=inter_extra_data_folder)\n",
    "# get the grid\n",
    "grid_X, grid_y = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "\n",
    "grid_X.shape, grid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biggest assumption when training ANNs is the following: \n",
    "\n",
    "\"We assume that training sets and test sets contains independent and identically distributed samples from the same unknown distribution $p_{data}(x,y)$\"\n",
    "\n",
    "This is a very important assumption that in general affect the performance ANNs, in particular classifier ones. We could, indeed, explore what can happen if we violete the following assumption. This a relevant application case, for exaple in cases when the generation parameters are not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(data1, mode, data2=None, F_std=False, overlap_size=15):\n",
    "    assert mode in ['all', 'interpolation', 'extrapolation']    \n",
    "\n",
    "    # params commons\n",
    "    dataset_split_params = {\n",
    "        'window_size': 20, # how large is the window\n",
    "        'overlap_size': overlap_size, # how many time interval of overlap there is between the windows\n",
    "        'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "        'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "        'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "        'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "        'test_size': 0.3, # size of the test set expressed in percentage\n",
    "        'val_size': 0.2, # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "        'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "        'random_state': 42 # sets the seed for reproducibility\n",
    "    }\n",
    "\n",
    "    if mode in ['interpolation', 'extrapolation']:\n",
    "        assert not data2 is None\n",
    "        grid_X_train, grid_y_train = data1\n",
    "        grid_X_test, grid_y_test   = data2\n",
    "        # get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "        # notice that theta is the third parameter\n",
    "        df_train, df_val, _ = get_dataset_split(grid_X_train, grid_y_train, **dataset_split_params)\n",
    "        # get the test set, selecting the index for grid given the interpolation assuption\n",
    "        # notice that theta is the third parameter\n",
    "        _, _, df_test = get_dataset_split(grid_X_test, grid_y_test, **dataset_split_params)\n",
    "    elif mode in ['all']:\n",
    "        grid_X, grid_y = data1\n",
    "        # get all the dataset from a single list\n",
    "        df_train, df_val, df_test = get_dataset_split(grid_X, grid_y, **dataset_split_params)\n",
    "    \n",
    "    # number of classes\n",
    "    print('Training set:')\n",
    "    train_counts = df_train['future_flare'].value_counts()\n",
    "    print(train_counts, '\\n')\n",
    "    print('validation set:')\n",
    "    val_counts = df_val['future_flare'].value_counts()\n",
    "    print(val_counts, '\\n')\n",
    "    print('Test set:')\n",
    "    test_counts = df_test['future_flare'].value_counts()\n",
    "    print(test_counts, '\\n')\n",
    "    print('Total:')\n",
    "    total_counts = train_counts.add(val_counts).add(test_counts)\n",
    "    print(total_counts, '\\n')\n",
    "    print()\n",
    "    \n",
    "    # compute the initial bias to pass then to the model\n",
    "    initial_bias = Constant([np.log(train_counts[0]/train_counts[1])])\n",
    "\n",
    "    # check the shape\n",
    "    X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "    X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "    X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "    X = np.vstack((X_train, X_val, X_test))\n",
    "    y = np.hstack((y_train, y_val, y_test))\n",
    "    print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "    print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)\n",
    "\n",
    "    # finally, if requested, standardize the dataset\n",
    "    if F_std:\n",
    "        # Standardize Data\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_std = scaler.transform(X_train)\n",
    "        X_val_std = scaler.transform(X_val)\n",
    "        X_test_std = scaler.transform(X_test)\n",
    "        # get automatically the number of classes\n",
    "        num_classes = len(np.unique(y))\n",
    "    else:\n",
    "        X_train_std = X_train\n",
    "        X_val_std = X_val\n",
    "        X_test_std = X_test\n",
    "\n",
    "\n",
    "    # finally return the dataset\n",
    "    return X_train_std, y_train, X_val_std, y_val, X_test_std, y_test, initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(X_train, initial_bias):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=[f1_m, 'accuracy'])\n",
    "    # print the summury model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_val, y_val, X_test, y_test):\n",
    "    # Validation set\n",
    "    y_pred = np.round(model.predict(X_val), 0)\n",
    "    print(\"### Evaluation on validation set ###\")\n",
    "    print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "    print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val, average='macro')))\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred, pos_label=1)\n",
    "    print('AUC:', auc(fpr, tpr))\n",
    "    #Create confusion matrix and normalizes it over predicted (columns)\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # Test set\n",
    "    y_pred = np.round(model.predict(X_test), 0)\n",
    "    print(\"### Evaluation on test set ###\")\n",
    "    print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "    print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test, average='macro')))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred, pos_label=1)\n",
    "    print('AUC:', auc(fpr, tpr))\n",
    "    #Create confusion matrix and normalizes it over predicted (columns)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model with multiple all theta parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct now the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    7153\n",
      "1    3655\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    3447\n",
      "1    1957\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    4587\n",
      "1    2361\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    15187\n",
      "1     7973\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (10808, 20) Val: (5404, 20) Test: (6948, 20)\n",
      "y ## Train: (10808,) Val: (5404,) Test: (6948,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set((grid_X, grid_y), 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-25 11:53:05.596201: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-25 11:53:05.596350: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-25 11:53:05.596419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5166cf34c918): /proc/driver/nvidia/version does not exist\n",
      "2023-03-25 11:53:05.596883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model(X_train, initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "343/343 [==============================] - 6s 12ms/step - loss: 0.3836 - f1_m: 0.7206 - accuracy: 0.7792 - val_loss: 0.2857 - val_f1_m: 0.6372 - val_accuracy: 0.8613\n",
      "Epoch 2/20\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.2702 - f1_m: 0.7984 - accuracy: 0.8727 - val_loss: 0.2525 - val_f1_m: 0.6425 - val_accuracy: 0.8885\n",
      "Epoch 3/20\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.2554 - f1_m: 0.8202 - accuracy: 0.8852 - val_loss: 0.2434 - val_f1_m: 0.6553 - val_accuracy: 0.8954\n",
      "Epoch 4/20\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.2416 - f1_m: 0.8346 - accuracy: 0.8960 - val_loss: 0.2261 - val_f1_m: 0.6578 - val_accuracy: 0.9011\n",
      "Epoch 5/20\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 0.2336 - f1_m: 0.8393 - accuracy: 0.8981 - val_loss: 0.2186 - val_f1_m: 0.6633 - val_accuracy: 0.9056\n",
      "Epoch 6/20\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.2286 - f1_m: 0.8453 - accuracy: 0.9019 - val_loss: 0.2241 - val_f1_m: 0.6527 - val_accuracy: 0.9018\n",
      "Epoch 7/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.2193 - f1_m: 0.8487 - accuracy: 0.9056 - val_loss: 0.2142 - val_f1_m: 0.6625 - val_accuracy: 0.9071\n",
      "Epoch 8/20\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 0.2185 - f1_m: 0.8524 - accuracy: 0.9055 - val_loss: 0.2155 - val_f1_m: 0.6678 - val_accuracy: 0.9018\n",
      "Epoch 9/20\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 0.2111 - f1_m: 0.8578 - accuracy: 0.9103 - val_loss: 0.2160 - val_f1_m: 0.6657 - val_accuracy: 0.9093\n",
      "Epoch 10/20\n",
      "343/343 [==============================] - 3s 10ms/step - loss: 0.2081 - f1_m: 0.8620 - accuracy: 0.9119 - val_loss: 0.2043 - val_f1_m: 0.6717 - val_accuracy: 0.9131\n",
      "Epoch 11/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.2028 - f1_m: 0.8631 - accuracy: 0.9145 - val_loss: 0.1956 - val_f1_m: 0.6772 - val_accuracy: 0.9165\n",
      "Epoch 12/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.2010 - f1_m: 0.8669 - accuracy: 0.9160 - val_loss: 0.1973 - val_f1_m: 0.6793 - val_accuracy: 0.9182\n",
      "Epoch 13/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1986 - f1_m: 0.8669 - accuracy: 0.9154 - val_loss: 0.1916 - val_f1_m: 0.6860 - val_accuracy: 0.9207\n",
      "Epoch 14/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1975 - f1_m: 0.8682 - accuracy: 0.9171 - val_loss: 0.2025 - val_f1_m: 0.6794 - val_accuracy: 0.9091\n",
      "Epoch 15/20\n",
      "343/343 [==============================] - 4s 10ms/step - loss: 0.1952 - f1_m: 0.8691 - accuracy: 0.9167 - val_loss: 0.2008 - val_f1_m: 0.6777 - val_accuracy: 0.9178\n",
      "Epoch 16/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1928 - f1_m: 0.8775 - accuracy: 0.9210 - val_loss: 0.1864 - val_f1_m: 0.6876 - val_accuracy: 0.9251\n",
      "Epoch 17/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1865 - f1_m: 0.8776 - accuracy: 0.9221 - val_loss: 0.1829 - val_f1_m: 0.6920 - val_accuracy: 0.9253\n",
      "Epoch 18/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1855 - f1_m: 0.8800 - accuracy: 0.9250 - val_loss: 0.1869 - val_f1_m: 0.6885 - val_accuracy: 0.9193\n",
      "Epoch 19/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1854 - f1_m: 0.8781 - accuracy: 0.9226 - val_loss: 0.1897 - val_f1_m: 0.6792 - val_accuracy: 0.9198\n",
      "Epoch 20/20\n",
      "343/343 [==============================] - 4s 11ms/step - loss: 0.1844 - f1_m: 0.8795 - accuracy: 0.9245 - val_loss: 0.1814 - val_f1_m: 0.6897 - val_accuracy: 0.9267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f3c708eb0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_allTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/169 [==============================] - 1s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.36\n",
      "F1 score: 0.27\n",
      "AUC: 0.5\n",
      "[[   0 3447]\n",
      " [   0 1957]]\n",
      "\n",
      "218/218 [==============================] - 2s 7ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.34\n",
      "F1 score: 0.25\n",
      "AUC: 0.5\n",
      "[[   0 4587]\n",
      " [   0 2361]]\n"
     ]
    }
   ],
   "source": [
    "eval(model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation with using just the extreme parameters: \n",
    "\n",
    "$\\theta=0.01$ and $\\theta=3$\n",
    "\n",
    "and a fraction of the other dataset, coming from $\\theta=0.1$ and $\\theta=0.5$ as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    3758\n",
      "1    1646\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    1910\n",
      "1     792\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    2050\n",
      "1    1424\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    7718\n",
      "1    3862\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (5404, 20) Val: (2702, 20) Test: (3474, 20)\n",
      "y ## Train: (5404,) Val: (2702,) Test: (3474,)\n"
     ]
    }
   ],
   "source": [
    "p = 'theta'\n",
    "# train configurations\n",
    "theta_train_list     = [0.01, 3]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "data_train = (grid_X[:,:,theta_train_list_idx,:,:,:], grid_y[:,:,theta_train_list_idx,:,:,:])\n",
    "# test configuration\n",
    "theta_test_list      = [0.1, 0.5]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "data_test = (grid_X[:,:,theta_test_list_idx,:,:,:], grid_y[:,:,theta_test_list_idx,:,:,:])\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set(data_train, 'interpolation', data2=data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model(X_train, initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "169/169 [==============================] - 8s 20ms/step - loss: 0.6904 - f1_m: 0.3353 - accuracy: 0.5464 - val_loss: 0.4128 - val_f1_m: 0.3415 - val_accuracy: 0.7391\n",
      "Epoch 2/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4126 - f1_m: 0.4568 - accuracy: 0.7265 - val_loss: 0.4178 - val_f1_m: 0.0467 - val_accuracy: 0.7232\n",
      "Epoch 3/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4082 - f1_m: 0.4011 - accuracy: 0.7322 - val_loss: 0.4179 - val_f1_m: 0.3922 - val_accuracy: 0.7406\n",
      "Epoch 4/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4103 - f1_m: 0.4837 - accuracy: 0.7296 - val_loss: 0.4064 - val_f1_m: 0.1977 - val_accuracy: 0.7346\n",
      "Epoch 5/20\n",
      "169/169 [==============================] - 3s 17ms/step - loss: 0.4034 - f1_m: 0.4721 - accuracy: 0.7389 - val_loss: 0.4084 - val_f1_m: 0.2180 - val_accuracy: 0.7398\n",
      "Epoch 6/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4028 - f1_m: 0.4984 - accuracy: 0.7406 - val_loss: 0.4197 - val_f1_m: 0.2622 - val_accuracy: 0.7413\n",
      "Epoch 7/20\n",
      "169/169 [==============================] - 3s 20ms/step - loss: 0.4026 - f1_m: 0.4469 - accuracy: 0.7359 - val_loss: 0.4061 - val_f1_m: 0.2679 - val_accuracy: 0.7428\n",
      "Epoch 8/20\n",
      "169/169 [==============================] - 3s 20ms/step - loss: 0.4001 - f1_m: 0.4704 - accuracy: 0.7374 - val_loss: 0.4016 - val_f1_m: 0.3235 - val_accuracy: 0.7480\n",
      "Epoch 9/20\n",
      "169/169 [==============================] - 4s 26ms/step - loss: 0.3996 - f1_m: 0.4973 - accuracy: 0.7348 - val_loss: 0.4008 - val_f1_m: 0.3229 - val_accuracy: 0.7487\n",
      "Epoch 10/20\n",
      "169/169 [==============================] - 3s 17ms/step - loss: 0.3970 - f1_m: 0.5028 - accuracy: 0.7396 - val_loss: 0.4004 - val_f1_m: 0.2196 - val_accuracy: 0.7383\n",
      "Epoch 11/20\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.3978 - f1_m: 0.4883 - accuracy: 0.7370 - val_loss: 0.4050 - val_f1_m: 0.1673 - val_accuracy: 0.7343\n",
      "Epoch 12/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.3960 - f1_m: 0.4904 - accuracy: 0.7400 - val_loss: 0.4005 - val_f1_m: 0.3092 - val_accuracy: 0.7472\n",
      "Epoch 13/20\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.3971 - f1_m: 0.4923 - accuracy: 0.7311 - val_loss: 0.3998 - val_f1_m: 0.2148 - val_accuracy: 0.7409\n",
      "Epoch 14/20\n",
      "169/169 [==============================] - 3s 15ms/step - loss: 0.3952 - f1_m: 0.4787 - accuracy: 0.7361 - val_loss: 0.4002 - val_f1_m: 0.2006 - val_accuracy: 0.7365\n",
      "Epoch 15/20\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.3929 - f1_m: 0.4888 - accuracy: 0.7426 - val_loss: 0.3985 - val_f1_m: 0.3640 - val_accuracy: 0.7483\n",
      "Epoch 16/20\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.3942 - f1_m: 0.5123 - accuracy: 0.7413 - val_loss: 0.3980 - val_f1_m: 0.2247 - val_accuracy: 0.7369\n",
      "Epoch 17/20\n",
      "169/169 [==============================] - 6s 38ms/step - loss: 0.3955 - f1_m: 0.4666 - accuracy: 0.7367 - val_loss: 0.4028 - val_f1_m: 0.2444 - val_accuracy: 0.7402\n",
      "Epoch 18/20\n",
      "169/169 [==============================] - 4s 25ms/step - loss: 0.3945 - f1_m: 0.4946 - accuracy: 0.7413 - val_loss: 0.4000 - val_f1_m: 0.3251 - val_accuracy: 0.7509\n",
      "Epoch 19/20\n",
      "169/169 [==============================] - 3s 19ms/step - loss: 0.3922 - f1_m: 0.4732 - accuracy: 0.7432 - val_loss: 0.3986 - val_f1_m: 0.3583 - val_accuracy: 0.7483\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f261c34c340>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_intrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 1s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.71\n",
      "AUC: 0.7218460785869163\n",
      "[[1501  409]\n",
      " [ 271  521]]\n",
      "\n",
      "109/109 [==============================] - 1s 7ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.73\n",
      "AUC: 0.7287637023842148\n",
      "[[1711  339]\n",
      " [ 537  887]]\n"
     ]
    }
   ],
   "source": [
    "eval(model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation without using the extreme parameters: \n",
    "\n",
    "$\\theta=0.1$ and $\\theta=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    3395\n",
      "1    2009\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    1537\n",
      "1    1165\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    2537\n",
      "1     937\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    7469\n",
      "1    4111\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (5404, 20) Val: (2702, 20) Test: (3474, 20)\n",
      "y ## Train: (5404,) Val: (2702,) Test: (3474,)\n"
     ]
    }
   ],
   "source": [
    "p = 'theta'\n",
    "# train configurations\n",
    "theta_train_list     = [0.1, 0.5]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "data_train = (grid_X[:,:,theta_train_list_idx,:,:,:], grid_y[:,:,theta_train_list_idx,:,:,:])\n",
    "# test configuration\n",
    "theta_test_list      = [0.01, 3]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "data_test = (grid_X[:,:,theta_test_list_idx,:,:,:], grid_y[:,:,theta_test_list_idx,:,:,:])\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set(data_train, 'extrapolation', data2=data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model(X_train, initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "169/169 [==============================] - 10s 29ms/step - loss: 0.6793 - f1_m: 0.5060 - accuracy: 0.5413 - val_loss: 0.5596 - val_f1_m: 0.5203 - val_accuracy: 0.7087\n",
      "Epoch 2/20\n",
      "169/169 [==============================] - 4s 22ms/step - loss: 0.5316 - f1_m: 0.5861 - accuracy: 0.7402 - val_loss: 0.5399 - val_f1_m: 0.5113 - val_accuracy: 0.7209\n",
      "Epoch 3/20\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.5124 - f1_m: 0.5941 - accuracy: 0.7474 - val_loss: 0.5473 - val_f1_m: 0.4816 - val_accuracy: 0.7191\n",
      "Epoch 4/20\n",
      "169/169 [==============================] - 3s 17ms/step - loss: 0.5400 - f1_m: 0.5808 - accuracy: 0.7439 - val_loss: 0.5305 - val_f1_m: 0.5316 - val_accuracy: 0.7269\n",
      "Epoch 5/20\n",
      "169/169 [==============================] - 3s 17ms/step - loss: 0.5105 - f1_m: 0.5938 - accuracy: 0.7476 - val_loss: 0.5198 - val_f1_m: 0.5598 - val_accuracy: 0.7287\n",
      "Epoch 6/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.5014 - f1_m: 0.5906 - accuracy: 0.7522 - val_loss: 0.5174 - val_f1_m: 0.5560 - val_accuracy: 0.7291\n",
      "Epoch 7/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4984 - f1_m: 0.6032 - accuracy: 0.7589 - val_loss: 0.5364 - val_f1_m: 0.5793 - val_accuracy: 0.7150\n",
      "Epoch 8/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4965 - f1_m: 0.5993 - accuracy: 0.7567 - val_loss: 0.5144 - val_f1_m: 0.5334 - val_accuracy: 0.7309\n",
      "Epoch 9/20\n",
      "169/169 [==============================] - 3s 17ms/step - loss: 0.4950 - f1_m: 0.6007 - accuracy: 0.7561 - val_loss: 0.5230 - val_f1_m: 0.4937 - val_accuracy: 0.7246\n",
      "Epoch 10/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4953 - f1_m: 0.6031 - accuracy: 0.7572 - val_loss: 0.5340 - val_f1_m: 0.4557 - val_accuracy: 0.7165\n",
      "Epoch 11/20\n",
      "169/169 [==============================] - 4s 21ms/step - loss: 0.4935 - f1_m: 0.6025 - accuracy: 0.7596 - val_loss: 0.5111 - val_f1_m: 0.5288 - val_accuracy: 0.7309\n",
      "Epoch 12/20\n",
      "169/169 [==============================] - 4s 21ms/step - loss: 0.4911 - f1_m: 0.6052 - accuracy: 0.7613 - val_loss: 0.5195 - val_f1_m: 0.5490 - val_accuracy: 0.7287\n",
      "Epoch 13/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4894 - f1_m: 0.6041 - accuracy: 0.7604 - val_loss: 0.5049 - val_f1_m: 0.5485 - val_accuracy: 0.7317\n",
      "Epoch 14/20\n",
      "169/169 [==============================] - 3s 17ms/step - loss: 0.4894 - f1_m: 0.6057 - accuracy: 0.7593 - val_loss: 0.5035 - val_f1_m: 0.5390 - val_accuracy: 0.7343\n",
      "Epoch 15/20\n",
      "169/169 [==============================] - 3s 16ms/step - loss: 0.4881 - f1_m: 0.6089 - accuracy: 0.7607 - val_loss: 0.5383 - val_f1_m: 0.4521 - val_accuracy: 0.7165\n",
      "Epoch 16/20\n",
      "169/169 [==============================] - 3s 18ms/step - loss: 0.4894 - f1_m: 0.6083 - accuracy: 0.7639 - val_loss: 0.5096 - val_f1_m: 0.5299 - val_accuracy: 0.7335\n",
      "Epoch 17/20\n",
      "169/169 [==============================] - 3s 20ms/step - loss: 0.4889 - f1_m: 0.6175 - accuracy: 0.7659 - val_loss: 0.5096 - val_f1_m: 0.5379 - val_accuracy: 0.7332\n",
      "Epoch 17: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26243e6fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_extrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 1s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.73\n",
      "F1 score: 0.71\n",
      "AUC: 0.7091516554460642\n",
      "[[1358  179]\n",
      " [ 542  623]]\n",
      "\n",
      "109/109 [==============================] - 1s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.75\n",
      "F1 score: 0.67\n",
      "AUC: 0.6630870585978532\n",
      "[[2184  353]\n",
      " [ 501  436]]\n"
     ]
    }
   ],
   "source": [
    "eval(model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigation in extrapolation with greater overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    16928\n",
      "1     9980\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    7656\n",
      "1    5798\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    12614\n",
      "1     4684\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Total:\n",
      "0    37198\n",
      "1    20462\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "\n",
      "X ## Train: (26908, 20) Val: (13454, 20) Test: (17298, 20)\n",
      "y ## Train: (26908,) Val: (13454,) Test: (17298,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, initial_bias = get_data_set(data_train, 'extrapolation', data2=data_test, overlap_size=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_3 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                310       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model(X_train, initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "841/841 [==============================] - 19s 17ms/step - loss: 0.5488 - f1_m: 0.5736 - accuracy: 0.6935 - val_loss: 0.5183 - val_f1_m: 0.3542 - val_accuracy: 0.7318\n",
      "Epoch 2/20\n",
      "841/841 [==============================] - 15s 18ms/step - loss: 0.4846 - f1_m: 0.6200 - accuracy: 0.7649 - val_loss: 0.5164 - val_f1_m: 0.3493 - val_accuracy: 0.7329\n",
      "Epoch 3/20\n",
      "841/841 [==============================] - 15s 18ms/step - loss: 0.4804 - f1_m: 0.6205 - accuracy: 0.7676 - val_loss: 0.5382 - val_f1_m: 0.3290 - val_accuracy: 0.7284\n",
      "Epoch 4/20\n",
      "841/841 [==============================] - 14s 17ms/step - loss: 0.4799 - f1_m: 0.6195 - accuracy: 0.7669 - val_loss: 0.5149 - val_f1_m: 0.3449 - val_accuracy: 0.7332\n",
      "Epoch 5/20\n",
      "841/841 [==============================] - 13s 15ms/step - loss: 0.4786 - f1_m: 0.6202 - accuracy: 0.7673 - val_loss: 0.5041 - val_f1_m: 0.3656 - val_accuracy: 0.7390\n",
      "Epoch 6/20\n",
      "841/841 [==============================] - 17s 20ms/step - loss: 0.4781 - f1_m: 0.6199 - accuracy: 0.7677 - val_loss: 0.5033 - val_f1_m: 0.3894 - val_accuracy: 0.7383\n",
      "Epoch 7/20\n",
      "841/841 [==============================] - 13s 15ms/step - loss: 0.4781 - f1_m: 0.6190 - accuracy: 0.7676 - val_loss: 0.5087 - val_f1_m: 0.3466 - val_accuracy: 0.7341\n",
      "Epoch 8/20\n",
      "841/841 [==============================] - 13s 15ms/step - loss: 0.4775 - f1_m: 0.6217 - accuracy: 0.7685 - val_loss: 0.5018 - val_f1_m: 0.3813 - val_accuracy: 0.7381\n",
      "Epoch 9/20\n",
      "841/841 [==============================] - 13s 15ms/step - loss: 0.4772 - f1_m: 0.6210 - accuracy: 0.7687 - val_loss: 0.5039 - val_f1_m: 0.3699 - val_accuracy: 0.7389\n",
      "Epoch 10/20\n",
      "841/841 [==============================] - 13s 16ms/step - loss: 0.4771 - f1_m: 0.6180 - accuracy: 0.7679 - val_loss: 0.5137 - val_f1_m: 0.3412 - val_accuracy: 0.7320\n",
      "Epoch 11/20\n",
      "841/841 [==============================] - 13s 16ms/step - loss: 0.4765 - f1_m: 0.6158 - accuracy: 0.7674 - val_loss: 0.5067 - val_f1_m: 0.3549 - val_accuracy: 0.7365\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25fa9e9940>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_extrpTheta19_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421/421 [==============================] - 3s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.74\n",
      "F1 score: 0.71\n",
      "AUC: 0.710636198372011\n",
      "[[6875  781]\n",
      " [2764 3034]]\n",
      "\n",
      "541/541 [==============================] - 3s 5ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.76\n",
      "F1 score: 0.66\n",
      "AUC: 0.6478221438584296\n",
      "[[11135  1479]\n",
      " [ 2750  1934]]\n"
     ]
    }
   ],
   "source": [
    "eval(model, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "\\[1\\] _On the distribution of fluxes of gamma-ray blazars: hints for a stochastic process?_, Tavecchio et al., [https://arxiv.org/pdf/2004.09149.pdf](https://arxiv.org/pdf/2004.09149.pdf)\n",
    "<!-- cite with: [\\[1\\]](https://arxiv.org/pdf/2004.09149.pdf)  -->\n",
    "\\[2\\] _Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline_, Wang et al., [https://arxiv.org/abs/1611.06455](https://arxiv.org/abs/1611.06455)\n",
    "<!-- cite with: [\\[2\\]](https://arxiv.org/abs/1611.06455)  -->\n",
    "\\[3\\] _Solar Flare Prediction Based on the Fusion of Multiple Deep-learning Models_, Tang et al., [https://iopscience.iop.org/article/10.3847/1538-4365/ac249e/meta](https://iopscience.iop.org/article/10.3847/1538-4365/ac249e/meta)\n",
    "<!-- cite with: [\\[3\\]](https://iopscience.iop.org/article/10.3847/1538-4365/ac249e/meta)  -->\n",
    "\\[4\\] _Predicting Solar Energetic Particles Using SDO/HMI Vector Magnetic Data Products and a Bidirectional LSTM Network_, Abduallah et al., [https://iopscience.iop.org/article/10.3847/1538-4365/ac5f56/meta](https://iopscience.iop.org/article/10.3847/1538-4365/ac5f56/meta)\n",
    "<!-- cite with: [\\[4\\]](https://iopscience.iop.org/article/10.3847/1538-4365/ac5f56/meta) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
