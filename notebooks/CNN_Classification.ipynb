{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8fa1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 22:57:14.722827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-08 22:57:14.864724: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-08 22:57:14.864748: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-08 22:57:15.782246: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 22:57:15.782434: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-08 22:57:15.782448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from Libs.load_data import ClassificationDataLoader,DataLoader, get_dataset_split\n",
    "from Libs import flares_plot as fplt\n",
    "from Libs.threshold import get_labels_physic, get_labels_KDE, get_labels_quantile, get_labels_quantile_on_run\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7132ec65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 1000,\n",
       " 'sigma': [0.5],\n",
       " 'theta': [0.01],\n",
       " 'mu': [1],\n",
       " 'delta': [0.2],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = ClassificationDataLoader(run=1000, N=1000, s=0.5, t=0.01, d=0.2, m=1)\n",
    "params = data_loader.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102bcb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 288.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Labels\n",
      "Labels Loaded\n"
     ]
    }
   ],
   "source": [
    "Xs, best_labels = data_loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5eec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>future_flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.097460</td>\n",
       "      <td>0.800273</td>\n",
       "      <td>0.762377</td>\n",
       "      <td>1.139235</td>\n",
       "      <td>1.553440</td>\n",
       "      <td>1.806599</td>\n",
       "      <td>1.645525</td>\n",
       "      <td>1.419684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211269</td>\n",
       "      <td>0.152283</td>\n",
       "      <td>0.167314</td>\n",
       "      <td>0.193723</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.203987</td>\n",
       "      <td>0.250678</td>\n",
       "      <td>0.269018</td>\n",
       "      <td>0.249159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.228656</td>\n",
       "      <td>0.251608</td>\n",
       "      <td>0.162705</td>\n",
       "      <td>0.133498</td>\n",
       "      <td>0.108928</td>\n",
       "      <td>0.142085</td>\n",
       "      <td>0.184496</td>\n",
       "      <td>0.200684</td>\n",
       "      <td>0.228820</td>\n",
       "      <td>0.240179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158942</td>\n",
       "      <td>0.114497</td>\n",
       "      <td>0.120570</td>\n",
       "      <td>0.146734</td>\n",
       "      <td>0.160511</td>\n",
       "      <td>0.174416</td>\n",
       "      <td>0.162416</td>\n",
       "      <td>0.183431</td>\n",
       "      <td>0.169348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.139880</td>\n",
       "      <td>0.147707</td>\n",
       "      <td>0.131194</td>\n",
       "      <td>0.108431</td>\n",
       "      <td>0.075699</td>\n",
       "      <td>0.089714</td>\n",
       "      <td>0.082418</td>\n",
       "      <td>0.098292</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.124675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.390679</td>\n",
       "      <td>0.367069</td>\n",
       "      <td>0.286149</td>\n",
       "      <td>0.250748</td>\n",
       "      <td>0.233916</td>\n",
       "      <td>0.314238</td>\n",
       "      <td>0.273822</td>\n",
       "      <td>0.376699</td>\n",
       "      <td>0.343073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.311468</td>\n",
       "      <td>0.377507</td>\n",
       "      <td>0.503605</td>\n",
       "      <td>0.546112</td>\n",
       "      <td>0.550019</td>\n",
       "      <td>0.559552</td>\n",
       "      <td>0.384622</td>\n",
       "      <td>0.320995</td>\n",
       "      <td>0.261761</td>\n",
       "      <td>0.258866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.045418</td>\n",
       "      <td>0.042112</td>\n",
       "      <td>0.042715</td>\n",
       "      <td>0.045806</td>\n",
       "      <td>0.066110</td>\n",
       "      <td>0.051933</td>\n",
       "      <td>0.045467</td>\n",
       "      <td>0.051471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.048260</td>\n",
       "      <td>0.047994</td>\n",
       "      <td>0.053862</td>\n",
       "      <td>0.044990</td>\n",
       "      <td>0.048068</td>\n",
       "      <td>0.041157</td>\n",
       "      <td>0.034158</td>\n",
       "      <td>0.049034</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>0.035842</td>\n",
       "      <td>...</td>\n",
       "      <td>1.377049</td>\n",
       "      <td>1.204380</td>\n",
       "      <td>1.285629</td>\n",
       "      <td>1.255421</td>\n",
       "      <td>0.832831</td>\n",
       "      <td>0.849710</td>\n",
       "      <td>0.877229</td>\n",
       "      <td>0.925528</td>\n",
       "      <td>0.756486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>0.894873</td>\n",
       "      <td>1.030731</td>\n",
       "      <td>0.898352</td>\n",
       "      <td>0.887923</td>\n",
       "      <td>0.801791</td>\n",
       "      <td>0.728138</td>\n",
       "      <td>0.638125</td>\n",
       "      <td>0.818890</td>\n",
       "      <td>1.011304</td>\n",
       "      <td>1.243419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294739</td>\n",
       "      <td>0.307752</td>\n",
       "      <td>0.397041</td>\n",
       "      <td>0.539180</td>\n",
       "      <td>0.515245</td>\n",
       "      <td>0.295248</td>\n",
       "      <td>0.387283</td>\n",
       "      <td>0.481980</td>\n",
       "      <td>0.494195</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5396</th>\n",
       "      <td>0.593780</td>\n",
       "      <td>0.591490</td>\n",
       "      <td>0.582267</td>\n",
       "      <td>0.561396</td>\n",
       "      <td>1.192204</td>\n",
       "      <td>0.916150</td>\n",
       "      <td>1.287857</td>\n",
       "      <td>1.609656</td>\n",
       "      <td>2.182222</td>\n",
       "      <td>1.856388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979838</td>\n",
       "      <td>1.020965</td>\n",
       "      <td>0.931575</td>\n",
       "      <td>0.786338</td>\n",
       "      <td>0.811534</td>\n",
       "      <td>0.557148</td>\n",
       "      <td>0.476873</td>\n",
       "      <td>0.357465</td>\n",
       "      <td>0.342375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5397</th>\n",
       "      <td>0.298239</td>\n",
       "      <td>0.199687</td>\n",
       "      <td>0.171544</td>\n",
       "      <td>0.196387</td>\n",
       "      <td>0.210877</td>\n",
       "      <td>0.224681</td>\n",
       "      <td>0.274870</td>\n",
       "      <td>0.166389</td>\n",
       "      <td>0.159029</td>\n",
       "      <td>0.221091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230019</td>\n",
       "      <td>0.292738</td>\n",
       "      <td>0.276140</td>\n",
       "      <td>0.256893</td>\n",
       "      <td>0.202953</td>\n",
       "      <td>0.146163</td>\n",
       "      <td>0.159135</td>\n",
       "      <td>0.190019</td>\n",
       "      <td>0.189322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>0.188162</td>\n",
       "      <td>0.143434</td>\n",
       "      <td>0.168813</td>\n",
       "      <td>0.203865</td>\n",
       "      <td>0.289923</td>\n",
       "      <td>0.263040</td>\n",
       "      <td>0.213453</td>\n",
       "      <td>0.216481</td>\n",
       "      <td>0.198572</td>\n",
       "      <td>0.228049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130151</td>\n",
       "      <td>0.158475</td>\n",
       "      <td>0.181278</td>\n",
       "      <td>0.136805</td>\n",
       "      <td>0.130137</td>\n",
       "      <td>0.154444</td>\n",
       "      <td>0.136134</td>\n",
       "      <td>0.117829</td>\n",
       "      <td>0.121386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5399</th>\n",
       "      <td>0.114396</td>\n",
       "      <td>0.061059</td>\n",
       "      <td>0.051766</td>\n",
       "      <td>0.063358</td>\n",
       "      <td>0.066266</td>\n",
       "      <td>0.080330</td>\n",
       "      <td>0.070412</td>\n",
       "      <td>0.056884</td>\n",
       "      <td>0.064135</td>\n",
       "      <td>0.048328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430803</td>\n",
       "      <td>0.559682</td>\n",
       "      <td>0.524077</td>\n",
       "      <td>0.613938</td>\n",
       "      <td>0.665520</td>\n",
       "      <td>0.449335</td>\n",
       "      <td>0.379423</td>\n",
       "      <td>0.362730</td>\n",
       "      <td>0.322396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5400 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           t_0       t_1       t_2       t_3       t_4       t_5       t_6  \\\n",
       "0     1.000000  0.975000  1.097460  0.800273  0.762377  1.139235  1.553440   \n",
       "1     0.228656  0.251608  0.162705  0.133498  0.108928  0.142085  0.184496   \n",
       "2     0.139880  0.147707  0.131194  0.108431  0.075699  0.089714  0.082418   \n",
       "3     0.311468  0.377507  0.503605  0.546112  0.550019  0.559552  0.384622   \n",
       "4     0.048260  0.047994  0.053862  0.044990  0.048068  0.041157  0.034158   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5395  0.894873  1.030731  0.898352  0.887923  0.801791  0.728138  0.638125   \n",
       "5396  0.593780  0.591490  0.582267  0.561396  1.192204  0.916150  1.287857   \n",
       "5397  0.298239  0.199687  0.171544  0.196387  0.210877  0.224681  0.274870   \n",
       "5398  0.188162  0.143434  0.168813  0.203865  0.289923  0.263040  0.213453   \n",
       "5399  0.114396  0.061059  0.051766  0.063358  0.066266  0.080330  0.070412   \n",
       "\n",
       "           t_7       t_8       t_9  ...      t_91      t_92      t_93  \\\n",
       "0     1.806599  1.645525  1.419684  ...  0.211269  0.152283  0.167314   \n",
       "1     0.200684  0.228820  0.240179  ...  0.158942  0.114497  0.120570   \n",
       "2     0.098292  0.149462  0.124675  ...  0.390679  0.367069  0.286149   \n",
       "3     0.320995  0.261761  0.258866  ...  0.046544  0.045418  0.042112   \n",
       "4     0.049034  0.048493  0.035842  ...  1.377049  1.204380  1.285629   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5395  0.818890  1.011304  1.243419  ...  0.294739  0.307752  0.397041   \n",
       "5396  1.609656  2.182222  1.856388  ...  0.979838  1.020965  0.931575   \n",
       "5397  0.166389  0.159029  0.221091  ...  0.230019  0.292738  0.276140   \n",
       "5398  0.216481  0.198572  0.228049  ...  0.130151  0.158475  0.181278   \n",
       "5399  0.056884  0.064135  0.048328  ...  0.430803  0.559682  0.524077   \n",
       "\n",
       "          t_94      t_95      t_96      t_97      t_98      t_99  future_flare  \n",
       "0     0.193723  0.220627  0.203987  0.250678  0.269018  0.249159             0  \n",
       "1     0.146734  0.160511  0.174416  0.162416  0.183431  0.169348             0  \n",
       "2     0.250748  0.233916  0.314238  0.273822  0.376699  0.343073             0  \n",
       "3     0.042715  0.045806  0.066110  0.051933  0.045467  0.051471             0  \n",
       "4     1.255421  0.832831  0.849710  0.877229  0.925528  0.756486             0  \n",
       "...        ...       ...       ...       ...       ...       ...           ...  \n",
       "5395  0.539180  0.515245  0.295248  0.387283  0.481980  0.494195             1  \n",
       "5396  0.786338  0.811534  0.557148  0.476873  0.357465  0.342375             0  \n",
       "5397  0.256893  0.202953  0.146163  0.159135  0.190019  0.189322             0  \n",
       "5398  0.136805  0.130137  0.154444  0.136134  0.117829  0.121386             0  \n",
       "5399  0.613938  0.665520  0.449335  0.379423  0.362730  0.322396             0  \n",
       "\n",
       "[5400 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bindexes = data_loader.get_standard_indexes()\n",
    "df_train, df_val, df_test = get_dataset_split(Xs, best_labels, bindexes, window_size=100, overlap_size=0, label_treshold=1,\n",
    "                                      split_on_run=True, shuffle_run=False, shuffle_window=False,\n",
    "                                      test_size = 0.2, val_size=0.2, get_validation=True, random_state=42)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f117bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>future_flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.794841</td>\n",
       "      <td>0.961714</td>\n",
       "      <td>0.773422</td>\n",
       "      <td>0.858592</td>\n",
       "      <td>0.608158</td>\n",
       "      <td>0.743288</td>\n",
       "      <td>0.776504</td>\n",
       "      <td>0.832919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155015</td>\n",
       "      <td>0.167666</td>\n",
       "      <td>0.131993</td>\n",
       "      <td>0.141247</td>\n",
       "      <td>0.171151</td>\n",
       "      <td>0.165978</td>\n",
       "      <td>0.160206</td>\n",
       "      <td>0.196225</td>\n",
       "      <td>0.210396</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.247564</td>\n",
       "      <td>0.160814</td>\n",
       "      <td>0.228383</td>\n",
       "      <td>0.204594</td>\n",
       "      <td>0.167953</td>\n",
       "      <td>0.160725</td>\n",
       "      <td>0.111309</td>\n",
       "      <td>0.131544</td>\n",
       "      <td>0.155309</td>\n",
       "      <td>0.113408</td>\n",
       "      <td>...</td>\n",
       "      <td>1.834831</td>\n",
       "      <td>1.515792</td>\n",
       "      <td>1.320335</td>\n",
       "      <td>1.590202</td>\n",
       "      <td>2.182392</td>\n",
       "      <td>2.024116</td>\n",
       "      <td>2.212742</td>\n",
       "      <td>3.477812</td>\n",
       "      <td>3.714054</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.838522</td>\n",
       "      <td>4.537823</td>\n",
       "      <td>4.923792</td>\n",
       "      <td>3.788025</td>\n",
       "      <td>3.624347</td>\n",
       "      <td>3.192889</td>\n",
       "      <td>2.461976</td>\n",
       "      <td>2.277469</td>\n",
       "      <td>1.532509</td>\n",
       "      <td>1.551760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233499</td>\n",
       "      <td>0.261160</td>\n",
       "      <td>0.288443</td>\n",
       "      <td>0.339135</td>\n",
       "      <td>0.306360</td>\n",
       "      <td>0.334486</td>\n",
       "      <td>0.255487</td>\n",
       "      <td>0.211829</td>\n",
       "      <td>0.159916</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148766</td>\n",
       "      <td>0.141664</td>\n",
       "      <td>0.105070</td>\n",
       "      <td>0.126404</td>\n",
       "      <td>0.107553</td>\n",
       "      <td>0.110005</td>\n",
       "      <td>0.090352</td>\n",
       "      <td>0.075536</td>\n",
       "      <td>0.090373</td>\n",
       "      <td>0.122284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521858</td>\n",
       "      <td>0.460438</td>\n",
       "      <td>0.529690</td>\n",
       "      <td>0.547569</td>\n",
       "      <td>0.570987</td>\n",
       "      <td>0.655388</td>\n",
       "      <td>0.586328</td>\n",
       "      <td>0.667996</td>\n",
       "      <td>0.564026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.579348</td>\n",
       "      <td>0.698647</td>\n",
       "      <td>0.712695</td>\n",
       "      <td>0.784738</td>\n",
       "      <td>0.723491</td>\n",
       "      <td>0.611856</td>\n",
       "      <td>0.656450</td>\n",
       "      <td>0.591715</td>\n",
       "      <td>0.446420</td>\n",
       "      <td>0.562588</td>\n",
       "      <td>...</td>\n",
       "      <td>1.201690</td>\n",
       "      <td>0.862037</td>\n",
       "      <td>0.891325</td>\n",
       "      <td>0.829132</td>\n",
       "      <td>1.359704</td>\n",
       "      <td>0.914465</td>\n",
       "      <td>0.816779</td>\n",
       "      <td>0.961132</td>\n",
       "      <td>1.099260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.125542</td>\n",
       "      <td>0.137061</td>\n",
       "      <td>0.156362</td>\n",
       "      <td>0.128312</td>\n",
       "      <td>0.083011</td>\n",
       "      <td>0.077568</td>\n",
       "      <td>0.084634</td>\n",
       "      <td>0.130345</td>\n",
       "      <td>0.127974</td>\n",
       "      <td>0.118603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056108</td>\n",
       "      <td>0.047488</td>\n",
       "      <td>0.052894</td>\n",
       "      <td>0.054968</td>\n",
       "      <td>0.061461</td>\n",
       "      <td>0.057568</td>\n",
       "      <td>0.051230</td>\n",
       "      <td>0.050639</td>\n",
       "      <td>0.046315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.053604</td>\n",
       "      <td>0.070879</td>\n",
       "      <td>0.059345</td>\n",
       "      <td>0.075383</td>\n",
       "      <td>0.097257</td>\n",
       "      <td>0.093403</td>\n",
       "      <td>0.121281</td>\n",
       "      <td>0.151271</td>\n",
       "      <td>0.122940</td>\n",
       "      <td>0.103515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>0.017086</td>\n",
       "      <td>0.015039</td>\n",
       "      <td>0.018051</td>\n",
       "      <td>0.015451</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.031795</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.021961</td>\n",
       "      <td>0.025480</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>0.022380</td>\n",
       "      <td>0.019284</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>0.032392</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.018174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020185</td>\n",
       "      <td>0.020176</td>\n",
       "      <td>0.025927</td>\n",
       "      <td>0.026278</td>\n",
       "      <td>0.032224</td>\n",
       "      <td>0.032248</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>0.038699</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0.037464</td>\n",
       "      <td>0.039266</td>\n",
       "      <td>0.037234</td>\n",
       "      <td>0.027514</td>\n",
       "      <td>0.022862</td>\n",
       "      <td>0.027127</td>\n",
       "      <td>0.019360</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>0.018135</td>\n",
       "      <td>0.015881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064680</td>\n",
       "      <td>0.105029</td>\n",
       "      <td>0.142267</td>\n",
       "      <td>0.147179</td>\n",
       "      <td>0.162281</td>\n",
       "      <td>0.122990</td>\n",
       "      <td>0.151431</td>\n",
       "      <td>0.193710</td>\n",
       "      <td>0.161193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>0.155876</td>\n",
       "      <td>0.151816</td>\n",
       "      <td>0.261903</td>\n",
       "      <td>0.241731</td>\n",
       "      <td>0.171918</td>\n",
       "      <td>0.155191</td>\n",
       "      <td>0.158974</td>\n",
       "      <td>0.125346</td>\n",
       "      <td>0.162747</td>\n",
       "      <td>0.135063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124761</td>\n",
       "      <td>0.145682</td>\n",
       "      <td>0.121527</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.118315</td>\n",
       "      <td>0.112081</td>\n",
       "      <td>0.101981</td>\n",
       "      <td>0.071866</td>\n",
       "      <td>0.106639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           t_0       t_1       t_2       t_3       t_4       t_5       t_6  \\\n",
       "0     1.000000  0.975000  0.794841  0.961714  0.773422  0.858592  0.608158   \n",
       "1     0.247564  0.160814  0.228383  0.204594  0.167953  0.160725  0.111309   \n",
       "2     3.838522  4.537823  4.923792  3.788025  3.624347  3.192889  2.461976   \n",
       "3     0.148766  0.141664  0.105070  0.126404  0.107553  0.110005  0.090352   \n",
       "4     0.579348  0.698647  0.712695  0.784738  0.723491  0.611856  0.656450   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1795  0.125542  0.137061  0.156362  0.128312  0.083011  0.077568  0.084634   \n",
       "1796  0.053604  0.070879  0.059345  0.075383  0.097257  0.093403  0.121281   \n",
       "1797  0.027338  0.021961  0.025480  0.023343  0.022380  0.019284  0.020583   \n",
       "1798  0.037464  0.039266  0.037234  0.027514  0.022862  0.027127  0.019360   \n",
       "1799  0.155876  0.151816  0.261903  0.241731  0.171918  0.155191  0.158974   \n",
       "\n",
       "           t_7       t_8       t_9  ...      t_91      t_92      t_93  \\\n",
       "0     0.743288  0.776504  0.832919  ...  0.155015  0.167666  0.131993   \n",
       "1     0.131544  0.155309  0.113408  ...  1.834831  1.515792  1.320335   \n",
       "2     2.277469  1.532509  1.551760  ...  0.233499  0.261160  0.288443   \n",
       "3     0.075536  0.090373  0.122284  ...  0.521858  0.460438  0.529690   \n",
       "4     0.591715  0.446420  0.562588  ...  1.201690  0.862037  0.891325   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1795  0.130345  0.127974  0.118603  ...  0.056108  0.047488  0.052894   \n",
       "1796  0.151271  0.122940  0.103515  ...  0.028792  0.022094  0.017086   \n",
       "1797  0.032392  0.023834  0.018174  ...  0.020185  0.020176  0.025927   \n",
       "1798  0.016040  0.018135  0.015881  ...  0.064680  0.105029  0.142267   \n",
       "1799  0.125346  0.162747  0.135063  ...  0.124761  0.145682  0.121527   \n",
       "\n",
       "          t_94      t_95      t_96      t_97      t_98      t_99  future_flare  \n",
       "0     0.141247  0.171151  0.165978  0.160206  0.196225  0.210396             1  \n",
       "1     1.590202  2.182392  2.024116  2.212742  3.477812  3.714054             1  \n",
       "2     0.339135  0.306360  0.334486  0.255487  0.211829  0.159916             0  \n",
       "3     0.547569  0.570987  0.655388  0.586328  0.667996  0.564026             1  \n",
       "4     0.829132  1.359704  0.914465  0.816779  0.961132  1.099260             1  \n",
       "...        ...       ...       ...       ...       ...       ...           ...  \n",
       "1795  0.054968  0.061461  0.057568  0.051230  0.050639  0.046315             0  \n",
       "1796  0.015039  0.018051  0.015451  0.022964  0.031795  0.027705             0  \n",
       "1797  0.026278  0.032224  0.032248  0.038812  0.038699  0.030643             0  \n",
       "1798  0.147179  0.162281  0.122990  0.151431  0.193710  0.161193             0  \n",
       "1799  0.113500  0.118315  0.112081  0.101981  0.071866  0.106639             0  \n",
       "\n",
       "[1800 rows x 101 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b8fb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_0</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_4</th>\n",
       "      <th>t_5</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_7</th>\n",
       "      <th>t_8</th>\n",
       "      <th>t_9</th>\n",
       "      <th>...</th>\n",
       "      <th>t_91</th>\n",
       "      <th>t_92</th>\n",
       "      <th>t_93</th>\n",
       "      <th>t_94</th>\n",
       "      <th>t_95</th>\n",
       "      <th>t_96</th>\n",
       "      <th>t_97</th>\n",
       "      <th>t_98</th>\n",
       "      <th>t_99</th>\n",
       "      <th>future_flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.864849</td>\n",
       "      <td>0.940855</td>\n",
       "      <td>0.797112</td>\n",
       "      <td>1.136182</td>\n",
       "      <td>1.312568</td>\n",
       "      <td>1.293183</td>\n",
       "      <td>1.708106</td>\n",
       "      <td>1.992753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043055</td>\n",
       "      <td>0.038731</td>\n",
       "      <td>0.038268</td>\n",
       "      <td>0.028723</td>\n",
       "      <td>0.029924</td>\n",
       "      <td>0.031991</td>\n",
       "      <td>0.036987</td>\n",
       "      <td>0.041331</td>\n",
       "      <td>0.053353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051904</td>\n",
       "      <td>0.041102</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>0.053803</td>\n",
       "      <td>0.065880</td>\n",
       "      <td>0.052029</td>\n",
       "      <td>0.058534</td>\n",
       "      <td>0.053135</td>\n",
       "      <td>0.050545</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042048</td>\n",
       "      <td>0.053979</td>\n",
       "      <td>0.046946</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.065051</td>\n",
       "      <td>0.061722</td>\n",
       "      <td>0.072057</td>\n",
       "      <td>0.082646</td>\n",
       "      <td>0.057414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057048</td>\n",
       "      <td>0.048548</td>\n",
       "      <td>0.065221</td>\n",
       "      <td>0.061342</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>0.085187</td>\n",
       "      <td>0.076338</td>\n",
       "      <td>0.066166</td>\n",
       "      <td>0.072643</td>\n",
       "      <td>0.040972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062960</td>\n",
       "      <td>0.071893</td>\n",
       "      <td>0.069668</td>\n",
       "      <td>0.069253</td>\n",
       "      <td>0.063292</td>\n",
       "      <td>0.054340</td>\n",
       "      <td>0.054102</td>\n",
       "      <td>0.043178</td>\n",
       "      <td>0.027668</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049637</td>\n",
       "      <td>0.047778</td>\n",
       "      <td>0.034671</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>0.038247</td>\n",
       "      <td>0.039409</td>\n",
       "      <td>0.032843</td>\n",
       "      <td>0.034893</td>\n",
       "      <td>0.029783</td>\n",
       "      <td>0.033671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051026</td>\n",
       "      <td>0.044779</td>\n",
       "      <td>0.054288</td>\n",
       "      <td>0.064697</td>\n",
       "      <td>0.061627</td>\n",
       "      <td>0.075575</td>\n",
       "      <td>0.062786</td>\n",
       "      <td>0.067390</td>\n",
       "      <td>0.050168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042756</td>\n",
       "      <td>0.043695</td>\n",
       "      <td>0.038555</td>\n",
       "      <td>0.053060</td>\n",
       "      <td>0.063615</td>\n",
       "      <td>0.045082</td>\n",
       "      <td>0.048137</td>\n",
       "      <td>0.047195</td>\n",
       "      <td>0.042249</td>\n",
       "      <td>0.040673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670718</td>\n",
       "      <td>0.655615</td>\n",
       "      <td>0.710235</td>\n",
       "      <td>0.594021</td>\n",
       "      <td>0.593149</td>\n",
       "      <td>0.681582</td>\n",
       "      <td>1.285178</td>\n",
       "      <td>1.356869</td>\n",
       "      <td>0.961953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.033149</td>\n",
       "      <td>0.032264</td>\n",
       "      <td>0.033706</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.046956</td>\n",
       "      <td>0.047962</td>\n",
       "      <td>0.045340</td>\n",
       "      <td>0.042412</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.054696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054186</td>\n",
       "      <td>0.065986</td>\n",
       "      <td>0.066926</td>\n",
       "      <td>0.088580</td>\n",
       "      <td>0.107406</td>\n",
       "      <td>0.148387</td>\n",
       "      <td>0.187585</td>\n",
       "      <td>0.190281</td>\n",
       "      <td>0.248975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.213471</td>\n",
       "      <td>0.265604</td>\n",
       "      <td>0.250802</td>\n",
       "      <td>0.302429</td>\n",
       "      <td>0.391600</td>\n",
       "      <td>0.316887</td>\n",
       "      <td>0.437321</td>\n",
       "      <td>0.467344</td>\n",
       "      <td>0.453241</td>\n",
       "      <td>0.407954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081030</td>\n",
       "      <td>0.095245</td>\n",
       "      <td>0.097329</td>\n",
       "      <td>0.147589</td>\n",
       "      <td>0.144182</td>\n",
       "      <td>0.189755</td>\n",
       "      <td>0.169573</td>\n",
       "      <td>0.161331</td>\n",
       "      <td>0.113477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>0.137755</td>\n",
       "      <td>0.166856</td>\n",
       "      <td>0.211139</td>\n",
       "      <td>0.140109</td>\n",
       "      <td>0.133311</td>\n",
       "      <td>0.111607</td>\n",
       "      <td>0.146526</td>\n",
       "      <td>0.155874</td>\n",
       "      <td>0.144841</td>\n",
       "      <td>0.104751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089023</td>\n",
       "      <td>0.097040</td>\n",
       "      <td>0.077349</td>\n",
       "      <td>0.056044</td>\n",
       "      <td>0.070104</td>\n",
       "      <td>0.059120</td>\n",
       "      <td>0.102902</td>\n",
       "      <td>0.113795</td>\n",
       "      <td>0.102911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0.071844</td>\n",
       "      <td>0.074584</td>\n",
       "      <td>0.071912</td>\n",
       "      <td>0.074552</td>\n",
       "      <td>0.073228</td>\n",
       "      <td>0.072019</td>\n",
       "      <td>0.093640</td>\n",
       "      <td>0.061726</td>\n",
       "      <td>0.067374</td>\n",
       "      <td>0.077197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025430</td>\n",
       "      <td>0.024417</td>\n",
       "      <td>0.021180</td>\n",
       "      <td>0.031455</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.025637</td>\n",
       "      <td>0.031822</td>\n",
       "      <td>0.033097</td>\n",
       "      <td>0.031961</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>0.030278</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.032673</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>0.033880</td>\n",
       "      <td>0.032479</td>\n",
       "      <td>0.030230</td>\n",
       "      <td>0.024974</td>\n",
       "      <td>0.025050</td>\n",
       "      <td>0.025067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206385</td>\n",
       "      <td>0.175075</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>0.273563</td>\n",
       "      <td>0.238035</td>\n",
       "      <td>0.147777</td>\n",
       "      <td>0.165675</td>\n",
       "      <td>0.144336</td>\n",
       "      <td>0.124229</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           t_0       t_1       t_2       t_3       t_4       t_5       t_6  \\\n",
       "0     1.000000  0.975000  0.864849  0.940855  0.797112  1.136182  1.312568   \n",
       "1     0.051904  0.041102  0.043459  0.053803  0.065880  0.052029  0.058534   \n",
       "2     0.057048  0.048548  0.065221  0.061342  0.056631  0.085187  0.076338   \n",
       "3     0.049637  0.047778  0.034671  0.028317  0.038247  0.039409  0.032843   \n",
       "4     0.042756  0.043695  0.038555  0.053060  0.063615  0.045082  0.048137   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1795  0.033149  0.032264  0.033706  0.041612  0.046956  0.047962  0.045340   \n",
       "1796  0.213471  0.265604  0.250802  0.302429  0.391600  0.316887  0.437321   \n",
       "1797  0.137755  0.166856  0.211139  0.140109  0.133311  0.111607  0.146526   \n",
       "1798  0.071844  0.074584  0.071912  0.074552  0.073228  0.072019  0.093640   \n",
       "1799  0.030278  0.029963  0.032673  0.034642  0.033880  0.032479  0.030230   \n",
       "\n",
       "           t_7       t_8       t_9  ...      t_91      t_92      t_93  \\\n",
       "0     1.293183  1.708106  1.992753  ...  0.043055  0.038731  0.038268   \n",
       "1     0.053135  0.050545  0.057436  ...  0.042048  0.053979  0.046946   \n",
       "2     0.066166  0.072643  0.040972  ...  0.062960  0.071893  0.069668   \n",
       "3     0.034893  0.029783  0.033671  ...  0.051026  0.044779  0.054288   \n",
       "4     0.047195  0.042249  0.040673  ...  0.670718  0.655615  0.710235   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1795  0.042412  0.049200  0.054696  ...  0.054186  0.065986  0.066926   \n",
       "1796  0.467344  0.453241  0.407954  ...  0.081030  0.095245  0.097329   \n",
       "1797  0.155874  0.144841  0.104751  ...  0.089023  0.097040  0.077349   \n",
       "1798  0.061726  0.067374  0.077197  ...  0.025430  0.024417  0.021180   \n",
       "1799  0.024974  0.025050  0.025067  ...  0.206385  0.175075  0.232194   \n",
       "\n",
       "          t_94      t_95      t_96      t_97      t_98      t_99  future_flare  \n",
       "0     0.028723  0.029924  0.031991  0.036987  0.041331  0.053353             0  \n",
       "1     0.054593  0.065051  0.061722  0.072057  0.082646  0.057414             0  \n",
       "2     0.069253  0.063292  0.054340  0.054102  0.043178  0.027668             0  \n",
       "3     0.064697  0.061627  0.075575  0.062786  0.067390  0.050168             0  \n",
       "4     0.594021  0.593149  0.681582  1.285178  1.356869  0.961953             0  \n",
       "...        ...       ...       ...       ...       ...       ...           ...  \n",
       "1795  0.088580  0.107406  0.148387  0.187585  0.190281  0.248975             0  \n",
       "1796  0.147589  0.144182  0.189755  0.169573  0.161331  0.113477             0  \n",
       "1797  0.056044  0.070104  0.059120  0.102902  0.113795  0.102911             0  \n",
       "1798  0.031455  0.029070  0.025637  0.031822  0.033097  0.031961             0  \n",
       "1799  0.273563  0.238035  0.147777  0.165675  0.144336  0.124229             1  \n",
       "\n",
       "[1800 rows x 101 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb08a906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4476\n",
       "1     924\n",
       "Name: future_flare, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = df_train['future_flare'].value_counts()\n",
    "train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5d35fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1479\n",
       "1     321\n",
       "Name: future_flare, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_counts = df_val['future_flare'].value_counts()\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9afa92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1478\n",
       "1     322\n",
       "Name: future_flare, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = df_test['future_flare'].value_counts()\n",
    "test_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24f77f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7433\n",
       "1    1567\n",
       "Name: future_flare, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts = train_counts.add(val_counts).add(test_counts)\n",
    "total_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385b899",
   "metadata": {},
   "source": [
    "Following this tensorflow tutorial in order to better train the neural architectures for unbalanced datasets\n",
    "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data?hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e892b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 100) (1800, 100) (1800, 100) (9000, 100)\n",
      "(5400,) (1800,) (1800,) (9000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print(X_train.shape, X_val.shape, X_test.shape, X.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bbc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Make the data uniform to multivariate timeseries\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "# get automatically the number of classes\n",
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186eb83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 1)]          0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 100, 64)           256       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 100, 64)          256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 100, 64)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 100, 64)           12352     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 100, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 100, 64)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 100, 64)           12352     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-08 22:57:55.133005: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-08 22:57:55.133069: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-08 22:57:55.133094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5166cf34c918): /proc/driver/nvidia/version does not exist\n",
      "2023-03-08 22:57:55.133419: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 100, 64)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,793\n",
      "Trainable params: 25,409\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def make_model(input_shape, num_classes, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = keras.initializers.Constant(output_bias)\n",
    "    \n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    if num_classes==2:\n",
    "        output_layer = keras.layers.Dense(1, activation=\"sigmoid\", bias_initializer=output_bias)(gap)\n",
    "    else:\n",
    "        output_layer = keras.layers.Dense(num_classes, activation=\"softmax\", bias_initializer=output_bias)(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "initial_bias = np.log([total_counts[1]/total_counts[0]])\n",
    "model = make_model(input_shape=X_train.shape[1:], num_classes=num_classes, output_bias=initial_bias)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944a9904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcda151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 256 # ~ 20 iteration per epoch\n",
    "# batch_size = 128\n",
    "# batch_size = 64 # ~ 90 iteration per epoch\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"CNN_best_weights.h5\", save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"loss\", factor=0.5, patience=20, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "if num_classes==2:\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "else:\n",
    "    loss = \"sparse_categorical_crossentropy\"\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=loss,\n",
    "    metrics=[f1_m],\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e745e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"loss\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ba1387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 6ms/step\n",
      "Test F1 Macro 0.7295550236726708\n"
     ]
    }
   ],
   "source": [
    "model = make_model(input_shape=X_train.shape[1:], num_classes=num_classes, output_bias=initial_bias)\n",
    "model.load_weights(\"CNN_best_weights.h5\")\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred[y_test_pred>0.5] = 1\n",
    "y_test_pred[y_test_pred<0.5] = 0\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test F1 Macro\", test_f1_macro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
