{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 08:15:36.841342: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-13 08:15:37.117478: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-13 08:15:37.117594: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-13 08:15:38.783967: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 08:15:38.784147: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-13 08:15:38.784166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.initializers import Constant\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Libs.load_data import ClassificationDataLoader, DataLoader, get_dataset_split\n",
    "from Libs.threshold import get_labels_physic\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1, 4, 1, 1, 1000), (100, 1, 4, 1, 1, 1000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data loader\n",
    "data_loader = DataLoader(run=100, N=1000, s=0.5, t=[0.01, 0.1, 0.5, 3], d=0.2, m=1)\n",
    "# get the grid\n",
    "grid_X, grid_y = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "\n",
    "grid_X.shape, grid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape grid_X and grid_y\n",
    "# Xs, labels =  grid_X.reshape((grid_X.shape[0]*grid_X.shape[2], grid_X.shape[-1])), grid_y.reshape((grid_X.shape[0]*grid_X.shape[2], grid_X.shape[-1]))\n",
    "# Xs.shape, labels.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model with multiple all theta parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start seeing what is going to happen with training and testing the NN with all the configurations of theta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192080, 21), (82320, 21), (117600, 21))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train,df_val,df_test = get_dataset_split(grid_X, grid_y, None, window_size=20, overlap_size=19,\n",
    "                                            label_treshold=1, split_on_run=True, shuffle_run=False, \n",
    "                                            shuffle_window=False, test_size = 0.3, val_size=0.2, \n",
    "                                            get_validation=True, random_state=42)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the amounts of class 0 and 1 for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    124288\n",
      "1     67792\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    53781\n",
      "1    28539\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    76596\n",
      "1    41004\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "pos = df_train['future_flare'].value_counts()[0]\n",
    "true = df_train['future_flare'].value_counts()[1]\n",
    "print('validation set:')\n",
    "print(df_val['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (192080, 20) Val: (82320, 20) Test: (117600, 20)\n",
      "y ## Train: (192080,) Val: (82320,) Test: (117600,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct now the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-13 08:18:13.383453: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-13 08:18:13.383909: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-13 08:18:13.384082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (83dc2cdd3c94): /proc/driver/nvidia/version does not exist\n",
      "2023-03-13 08:18:13.392995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = Constant([np.log(true/pos)])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "6003/6003 [==============================] - 149s 23ms/step - loss: 0.1403 - f1_m: 0.9059 - accuracy: 0.9413 - val_loss: 0.1135 - val_f1_m: 0.4724 - val_accuracy: 0.9488\n",
      "Epoch 2/20\n",
      "6003/6003 [==============================] - 139s 23ms/step - loss: 0.0717 - f1_m: 0.9617 - accuracy: 0.9746 - val_loss: 0.0728 - val_f1_m: 0.5044 - val_accuracy: 0.9701\n",
      "Epoch 3/20\n",
      "6003/6003 [==============================] - 139s 23ms/step - loss: 0.0575 - f1_m: 0.9699 - accuracy: 0.9799 - val_loss: 0.0851 - val_f1_m: 0.4993 - val_accuracy: 0.9673\n",
      "Epoch 4/20\n",
      "6003/6003 [==============================] - 142s 24ms/step - loss: 0.0501 - f1_m: 0.9738 - accuracy: 0.9828 - val_loss: 0.0492 - val_f1_m: 0.5249 - val_accuracy: 0.9824\n",
      "Epoch 5/20\n",
      "6003/6003 [==============================] - 143s 24ms/step - loss: 0.0480 - f1_m: 0.9755 - accuracy: 0.9837 - val_loss: 0.0432 - val_f1_m: 0.5335 - val_accuracy: 0.9849\n",
      "Epoch 6/20\n",
      "6003/6003 [==============================] - 141s 23ms/step - loss: 0.0460 - f1_m: 0.9763 - accuracy: 0.9843 - val_loss: 0.0406 - val_f1_m: 0.5330 - val_accuracy: 0.9867\n",
      "Epoch 7/20\n",
      "6003/6003 [==============================] - 141s 24ms/step - loss: 0.0454 - f1_m: 0.9767 - accuracy: 0.9845 - val_loss: 0.0447 - val_f1_m: 0.5274 - val_accuracy: 0.9844\n",
      "Epoch 8/20\n",
      "6003/6003 [==============================] - 173s 29ms/step - loss: 0.0439 - f1_m: 0.9774 - accuracy: 0.9851 - val_loss: 0.0402 - val_f1_m: 0.5326 - val_accuracy: 0.9872\n",
      "Epoch 9/20\n",
      "6003/6003 [==============================] - 146s 24ms/step - loss: 0.0430 - f1_m: 0.9783 - accuracy: 0.9855 - val_loss: 0.0401 - val_f1_m: 0.5330 - val_accuracy: 0.9869\n",
      "Epoch 10/20\n",
      "6003/6003 [==============================] - 149s 25ms/step - loss: 0.0432 - f1_m: 0.9786 - accuracy: 0.9858 - val_loss: 0.0461 - val_f1_m: 0.5324 - val_accuracy: 0.9832\n",
      "Epoch 11/20\n",
      "6003/6003 [==============================] - 146s 24ms/step - loss: 0.0422 - f1_m: 0.9786 - accuracy: 0.9859 - val_loss: 0.0390 - val_f1_m: 0.5321 - val_accuracy: 0.9875\n",
      "Epoch 12/20\n",
      "6003/6003 [==============================] - 273s 45ms/step - loss: 0.0421 - f1_m: 0.9787 - accuracy: 0.9859 - val_loss: 0.0410 - val_f1_m: 0.5336 - val_accuracy: 0.9864\n",
      "Epoch 13/20\n",
      "6003/6003 [==============================] - 211s 35ms/step - loss: 0.0612 - f1_m: 0.9747 - accuracy: 0.9831 - val_loss: 0.0511 - val_f1_m: 0.5254 - val_accuracy: 0.9818\n",
      "Epoch 14/20\n",
      "6003/6003 [==============================] - 151s 25ms/step - loss: 0.0523 - f1_m: 0.9727 - accuracy: 0.9819 - val_loss: 0.0435 - val_f1_m: 0.5327 - val_accuracy: 0.9860\n",
      "Epoch 14: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66df77d3d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_allTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2573/2573 [==============================] - 22s 8ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.98\n",
      "[[53521   260]\n",
      " [  889 27650]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "y_pred = np.round(model.predict(X_val), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675/3675 [==============================] - 34s 9ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.98\n",
      "[[76282   314]\n",
      " [ 1292 39712]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation with using just the extreme parameters: \n",
    "\n",
    "$\\theta=0.01$ and $\\theta=3$\n",
    "\n",
    "and a fraction of the other dataset, coming from $\\theta=0.1$ and $\\theta=0.5$ as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 100,\n",
       " 'sigma': [0.5],\n",
       " 'theta': [0.01, 0.1, 0.5, 3],\n",
       " 'mu': [1],\n",
       " 'delta': [0.2],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96040, 21), (41160, 21), (58800, 21))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 'theta'\n",
    "theta_train_list     = [0.01, 3]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "theta_test_list      = [0.1, 0.5]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "\n",
    "# get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "df_train, df_val, _ = get_dataset_split(grid_X[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        grid_y[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        None, window_size=20, overlap_size=19,\n",
    "                                        label_treshold=1, split_on_run=True, shuffle_run=False, \n",
    "                                        shuffle_window=False, test_size = 0.3, val_size=0.2, \n",
    "                                        get_validation=True, random_state=42)\n",
    "# get the test set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "_, _, df_test = get_dataset_split(grid_X[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  grid_y[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  None, window_size=20, overlap_size=19,\n",
    "                                  label_treshold=1, split_on_run=True, shuffle_run=False, \n",
    "                                  shuffle_window=False, test_size = 0.3, val_size=0.2, \n",
    "                                  get_validation=True, random_state=42)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    67067\n",
      "1    28973\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    28312\n",
      "1    12848\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    35204\n",
      "1    23596\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "pos = df_train['future_flare'].value_counts()[0]\n",
    "true = df_train['future_flare'].value_counts()[1]\n",
    "print('validation set:')\n",
    "print(df_val['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (96040, 20) Val: (41160, 20) Test: (58800, 20)\n",
      "y ## Train: (96040,) Val: (41160,) Test: (58800,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3002/3002 [==============================] - 68s 23ms/step - loss: 0.0524 - f1_m: 0.9698 - accuracy: 0.9830 - val_loss: 0.0462 - val_f1_m: 0.4862 - val_accuracy: 0.9868\n",
      "Epoch 2/20\n",
      "3002/3002 [==============================] - 73s 24ms/step - loss: 0.0503 - f1_m: 0.9708 - accuracy: 0.9837 - val_loss: 0.0657 - val_f1_m: 0.4661 - val_accuracy: 0.9752\n",
      "Epoch 3/20\n",
      "3002/3002 [==============================] - 71s 24ms/step - loss: 0.0491 - f1_m: 0.9719 - accuracy: 0.9844 - val_loss: 0.0432 - val_f1_m: 0.4896 - val_accuracy: 0.9869\n",
      "Epoch 4/20\n",
      "3002/3002 [==============================] - 71s 24ms/step - loss: 0.0486 - f1_m: 0.9719 - accuracy: 0.9845 - val_loss: 0.0455 - val_f1_m: 0.4874 - val_accuracy: 0.9870\n",
      "Epoch 5/20\n",
      "3002/3002 [==============================] - 71s 24ms/step - loss: 0.0475 - f1_m: 0.9741 - accuracy: 0.9854 - val_loss: 0.0512 - val_f1_m: 0.4808 - val_accuracy: 0.9836\n",
      "Epoch 6/20\n",
      "3002/3002 [==============================] - 73s 24ms/step - loss: 0.0468 - f1_m: 0.9736 - accuracy: 0.9853 - val_loss: 0.0433 - val_f1_m: 0.4872 - val_accuracy: 0.9871\n",
      "Epoch 6: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66d40d5f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_intrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287/1287 [==============================] - 7s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.98\n",
      "[[28302    10]\n",
      " [  522 12326]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "y_pred = np.round(model.predict(X_val), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1838/1838 [==============================] - 10s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.99\n",
      "[[35177    27]\n",
      " [  668 22928]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are still similar to the standard case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation without using the extreme parameters: \n",
    "\n",
    "$\\theta=0.1$ and $\\theta=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((96040, 21), (41160, 21), (58800, 21))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 'theta'\n",
    "theta_train_list     = [0.1, 0.5]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "theta_test_list      = [0.01, 3]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "\n",
    "# get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "df_train, df_val, _ = get_dataset_split(grid_X[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        grid_y[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        None, window_size=20, overlap_size=19,\n",
    "                                        label_treshold=1, split_on_run=True, shuffle_run=False, \n",
    "                                        shuffle_window=False, test_size = 0.3, val_size=0.2, \n",
    "                                        get_validation=True, random_state=42)\n",
    "# get the test set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "_, _, df_test = get_dataset_split(grid_X[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  grid_y[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  None, window_size=20, overlap_size=19,\n",
    "                                  label_treshold=1, split_on_run=True, shuffle_run=False, \n",
    "                                  shuffle_window=False, test_size = 0.3, val_size=0.2, \n",
    "                                  get_validation=True, random_state=42)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (96040, 20) Val: (41160, 20) Test: (58800, 20)\n",
      "y ## Train: (96040,) Val: (41160,) Test: (58800,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = Constant([np.log(true/pos)])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3002/3002 [==============================] - 81s 26ms/step - loss: 0.1356 - f1_m: 0.9264 - accuracy: 0.9461 - val_loss: 0.0838 - val_f1_m: 0.5665 - val_accuracy: 0.9662\n",
      "Epoch 2/20\n",
      "3002/3002 [==============================] - 78s 26ms/step - loss: 0.0774 - f1_m: 0.9602 - accuracy: 0.9692 - val_loss: 0.0773 - val_f1_m: 0.5763 - val_accuracy: 0.9699\n",
      "Epoch 3/20\n",
      "3002/3002 [==============================] - 78s 26ms/step - loss: 0.0594 - f1_m: 0.9705 - accuracy: 0.9772 - val_loss: 0.0530 - val_f1_m: 0.5841 - val_accuracy: 0.9800\n",
      "Epoch 4/20\n",
      "3002/3002 [==============================] - 79s 26ms/step - loss: 0.0526 - f1_m: 0.9739 - accuracy: 0.9800 - val_loss: 0.0475 - val_f1_m: 0.5878 - val_accuracy: 0.9829\n",
      "Epoch 5/20\n",
      "3002/3002 [==============================] - 74s 25ms/step - loss: 0.0469 - f1_m: 0.9776 - accuracy: 0.9827 - val_loss: 0.0525 - val_f1_m: 0.5877 - val_accuracy: 0.9799\n",
      "Epoch 6/20\n",
      "3002/3002 [==============================] - 75s 25ms/step - loss: 0.0454 - f1_m: 0.9783 - accuracy: 0.9834 - val_loss: 0.0513 - val_f1_m: 0.5793 - val_accuracy: 0.9805\n",
      "Epoch 7/20\n",
      "3002/3002 [==============================] - 73s 24ms/step - loss: 0.0424 - f1_m: 0.9797 - accuracy: 0.9844 - val_loss: 0.0412 - val_f1_m: 0.5883 - val_accuracy: 0.9860\n",
      "Epoch 8/20\n",
      "3002/3002 [==============================] - 74s 25ms/step - loss: 0.0413 - f1_m: 0.9802 - accuracy: 0.9848 - val_loss: 0.0385 - val_f1_m: 0.5861 - val_accuracy: 0.9859\n",
      "Epoch 9/20\n",
      "3002/3002 [==============================] - 74s 25ms/step - loss: 0.0404 - f1_m: 0.9809 - accuracy: 0.9853 - val_loss: 0.0384 - val_f1_m: 0.5892 - val_accuracy: 0.9853\n",
      "Epoch 10/20\n",
      "3002/3002 [==============================] - 86s 29ms/step - loss: 0.0381 - f1_m: 0.9817 - accuracy: 0.9860 - val_loss: 0.0385 - val_f1_m: 0.5896 - val_accuracy: 0.9863\n",
      "Epoch 11/20\n",
      "3002/3002 [==============================] - 76s 25ms/step - loss: 0.0382 - f1_m: 0.9817 - accuracy: 0.9860 - val_loss: 0.0445 - val_f1_m: 0.5799 - val_accuracy: 0.9842\n",
      "Epoch 12/20\n",
      "3002/3002 [==============================] - 77s 26ms/step - loss: 0.0371 - f1_m: 0.9823 - accuracy: 0.9865 - val_loss: 0.0522 - val_f1_m: 0.5871 - val_accuracy: 0.9819\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f66c502f040>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_extrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287/1287 [==============================] - 7s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.98\n",
      "F1 score: 0.98\n",
      "[[25074   395]\n",
      " [  348 15343]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "y_pred = np.round(model.predict(X_val), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1838/1838 [==============================] - 9s 5ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.98\n",
      "F1 score: 0.96\n",
      "[[40513   879]\n",
      " [  507 16901]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
