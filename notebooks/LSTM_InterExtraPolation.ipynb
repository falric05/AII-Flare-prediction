{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 09:55:28.623175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 09:55:28.833829: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-20 09:55:28.833888: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-20 09:55:29.725593: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 09:55:29.725831: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-20 09:55:29.725849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.initializers import Constant\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Libs.config import inter_extra_data_folder\n",
    "from Libs.load_data import DataLoader, get_dataset_split\n",
    "from Libs.threshold import get_labels_physic\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1, 4, 1, 1, 1000), (30, 1, 4, 1, 1, 1000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data loader\n",
    "data_loader = DataLoader(run=30, N=1000, s=0.5, t=[0.01, 0.1, 0.5, 3], d=0.2, m=1, \n",
    "                         override=False, folder=inter_extra_data_folder)\n",
    "# get the grid\n",
    "grid_X, grid_y = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "\n",
    "grid_X.shape, grid_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biggest assumption when training ANNs is the following: \n",
    "\n",
    "\"We assume that training sets and test sets contains independent and identically distributed samples from the same unknown distribution $p_{data}(x,y)$\"\n",
    "\n",
    "This is a very important assumption that in general affect the performance ANNs, in particular classifier ones. We could, indeed, explore what can happen if we violete the following assumption. This a relevant application case, for exaple in cases when the generation parameters are not known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM model with multiple all theta parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start seeing what is going to happen with training and testing the NN with all the configurations of theta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54880, 21), (27440, 21), (35280, 21))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split_params = {\n",
    "    'window_size': 20, # how large is the window\n",
    "    'overlap_size': 19, # how many time interval of overlap there is between the windows\n",
    "    'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "    'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "    'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "    'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "    'test_size': 0.3, # size of the test set expressed in percentage\n",
    "    'val_size': 0.2, # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "    'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "    'random_state': 42 # sets the seed for reproducibility\n",
    "}\n",
    "df_train,df_val,df_test = get_dataset_split(grid_X, grid_y, **dataset_split_params)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the amounts of class 0 and 1 for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training set:\n",
      "0    35182\n",
      "1    19698\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Validation set:\n",
      "0    16484\n",
      "1    10956\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Test set:\n",
      "0    23247\n",
      "1    12033\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('## Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "pos = df_train['future_flare'].value_counts()[0]\n",
    "true = df_train['future_flare'].value_counts()[1]\n",
    "print('## Validation set:')\n",
    "print(df_val['future_flare'].value_counts(), '\\n')\n",
    "print('## Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (54880, 20) Val: (27440, 20) Test: (35280, 20)\n",
      "y ## Train: (54880,) Val: (27440,) Test: (35280,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "# Make the data uniform to multivariate timeseries\n",
    "X_train_std = X_train_std.reshape((X_train_std.shape[0], X_train_std.shape[1], 1))\n",
    "X_val_std = X_val_std.reshape((X_val_std.shape[0], X_val_std.shape[1], 1))\n",
    "X_test_std = X_test_std.reshape((X_test_std.shape[0], X_test_std.shape[1], 1))\n",
    "# get automatically the number of classes\n",
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct now the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 09:55:31.454317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-20 09:55:31.454391: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-20 09:55:31.454416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (44910f15382a): /proc/driver/nvidia/version does not exist\n",
      "2023-03-20 09:55:31.454678: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = Constant([np.log(true/pos)])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train_std.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "1715/1715 [==============================] - 23s 12ms/step - loss: 0.1638 - f1_m: 0.8883 - accuracy: 0.9311 - val_loss: 0.0865 - val_f1_m: 0.5586 - val_accuracy: 0.9660\n",
      "Epoch 2/20\n",
      "1715/1715 [==============================] - 19s 11ms/step - loss: 0.0780 - f1_m: 0.9579 - accuracy: 0.9718 - val_loss: 0.0727 - val_f1_m: 0.5606 - val_accuracy: 0.9723\n",
      "Epoch 3/20\n",
      "1715/1715 [==============================] - 19s 11ms/step - loss: 0.0671 - f1_m: 0.9654 - accuracy: 0.9765 - val_loss: 0.0543 - val_f1_m: 0.5757 - val_accuracy: 0.9818\n",
      "Epoch 4/20\n",
      "1715/1715 [==============================] - 23s 14ms/step - loss: 0.0611 - f1_m: 0.9685 - accuracy: 0.9788 - val_loss: 0.0542 - val_f1_m: 0.5766 - val_accuracy: 0.9825\n",
      "Epoch 5/20\n",
      "1715/1715 [==============================] - 26s 15ms/step - loss: 0.0565 - f1_m: 0.9712 - accuracy: 0.9804 - val_loss: 0.0472 - val_f1_m: 0.5799 - val_accuracy: 0.9847\n",
      "Epoch 6/20\n",
      "1715/1715 [==============================] - 25s 15ms/step - loss: 0.0545 - f1_m: 0.9730 - accuracy: 0.9812 - val_loss: 0.0490 - val_f1_m: 0.5792 - val_accuracy: 0.9832\n",
      "Epoch 7/20\n",
      "1715/1715 [==============================] - 27s 15ms/step - loss: 0.0501 - f1_m: 0.9744 - accuracy: 0.9829 - val_loss: 0.0440 - val_f1_m: 0.5820 - val_accuracy: 0.9864\n",
      "Epoch 8/20\n",
      "1715/1715 [==============================] - 28s 16ms/step - loss: 0.0481 - f1_m: 0.9750 - accuracy: 0.9834 - val_loss: 0.0423 - val_f1_m: 0.5804 - val_accuracy: 0.9858\n",
      "Epoch 9/20\n",
      "1715/1715 [==============================] - 42s 24ms/step - loss: 0.0462 - f1_m: 0.9766 - accuracy: 0.9843 - val_loss: 0.0417 - val_f1_m: 0.5812 - val_accuracy: 0.9871\n",
      "Epoch 10/20\n",
      "1715/1715 [==============================] - 33s 19ms/step - loss: 0.0459 - f1_m: 0.9775 - accuracy: 0.9847 - val_loss: 0.0407 - val_f1_m: 0.5811 - val_accuracy: 0.9866\n",
      "Epoch 11/20\n",
      "1715/1715 [==============================] - 30s 18ms/step - loss: 0.0446 - f1_m: 0.9779 - accuracy: 0.9850 - val_loss: 0.0401 - val_f1_m: 0.5811 - val_accuracy: 0.9868\n",
      "Epoch 12/20\n",
      "1715/1715 [==============================] - 34s 20ms/step - loss: 0.0439 - f1_m: 0.9785 - accuracy: 0.9853 - val_loss: 0.0412 - val_f1_m: 0.5819 - val_accuracy: 0.9866\n",
      "Epoch 13/20\n",
      "1715/1715 [==============================] - 33s 19ms/step - loss: 0.0437 - f1_m: 0.9779 - accuracy: 0.9852 - val_loss: 0.0567 - val_f1_m: 0.5708 - val_accuracy: 0.9786\n",
      "Epoch 14/20\n",
      "1715/1715 [==============================] - 30s 17ms/step - loss: 0.0430 - f1_m: 0.9785 - accuracy: 0.9854 - val_loss: 0.0383 - val_f1_m: 0.5828 - val_accuracy: 0.9879\n",
      "Epoch 15/20\n",
      "1715/1715 [==============================] - 30s 17ms/step - loss: 0.0429 - f1_m: 0.9788 - accuracy: 0.9856 - val_loss: 0.0462 - val_f1_m: 0.5769 - val_accuracy: 0.9843\n",
      "Epoch 16/20\n",
      "1715/1715 [==============================] - 29s 17ms/step - loss: 0.0419 - f1_m: 0.9789 - accuracy: 0.9859 - val_loss: 0.0382 - val_f1_m: 0.5823 - val_accuracy: 0.9878\n",
      "Epoch 17/20\n",
      "1715/1715 [==============================] - 29s 17ms/step - loss: 0.0410 - f1_m: 0.9800 - accuracy: 0.9865 - val_loss: 0.0444 - val_f1_m: 0.5824 - val_accuracy: 0.9852\n",
      "Epoch 18/20\n",
      "1715/1715 [==============================] - 29s 17ms/step - loss: 0.0415 - f1_m: 0.9797 - accuracy: 0.9862 - val_loss: 0.0463 - val_f1_m: 0.5806 - val_accuracy: 0.9836\n",
      "Epoch 19/20\n",
      "1715/1715 [==============================] - 29s 17ms/step - loss: 0.0395 - f1_m: 0.9810 - accuracy: 0.9871 - val_loss: 0.0386 - val_f1_m: 0.5801 - val_accuracy: 0.9869\n",
      "Epoch 19: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d6f469760>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_allTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val_std, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858/858 [==============================] - 6s 7ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.99\n",
      "[[16478     6]\n",
      " [  353 10603]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_val_std), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103/1103 [==============================] - 7s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.98\n",
      "[[23233    14]\n",
      " [  465 11568]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test_std), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation with using just the extreme parameters: \n",
    "\n",
    "$\\theta=0.01$ and $\\theta=3$\n",
    "\n",
    "and a fraction of the other dataset, coming from $\\theta=0.1$ and $\\theta=0.5$ as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'theta'\n",
    "theta_train_list     = [0.01, 3]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "theta_test_list      = [0.1, 0.5]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "# params commons\n",
    "dataset_split_params = {\n",
    "    'window_size': 20, # how large is the window\n",
    "    'overlap_size': 19, # how many time interval of overlap there is between the windows\n",
    "    'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "    'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "    'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "    'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "    'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "    'random_state': 42 # sets the seed for reproducibility\n",
    "}\n",
    "# params for training and validation set\n",
    "train_split = {\n",
    "    'test_size': 0, # size of the test set expressed in percentage\n",
    "    'val_size': 0.2 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "    }\n",
    "dataset_split_params_train = {**dataset_split_params, **train_split}\n",
    "# params for test set\n",
    "test_split =  {\n",
    "    'test_size': 0.3, # size of the test set expressed in percentage\n",
    "    'val_size': 0 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "}                            \n",
    "dataset_split_params_test  = {**dataset_split_params, **test_split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47040, 21), (11760, 21), (17640, 21))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "df_train, df_val, _ = get_dataset_split(grid_X[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        grid_y[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        **dataset_split_params_train)\n",
    "# get the test set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "_, _, df_test = get_dataset_split(grid_X[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  grid_y[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  **dataset_split_params_test)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training set:\n",
      "0    32726\n",
      "1    14314\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Validation set:\n",
      "0    8059\n",
      "1    3701\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Test set:\n",
      "0    10768\n",
      "1     6872\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('## Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "pos = df_train['future_flare'].value_counts()[0]\n",
    "true = df_train['future_flare'].value_counts()[1]\n",
    "print('## Validation set:')\n",
    "print(df_val['future_flare'].value_counts(), '\\n')\n",
    "print('## Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (47040, 20) \n",
      "     Val: (11760, 20) \n",
      "     Test: (17640, 20)\n",
      "y ## Train: (47040,) \n",
      "     Val: (11760,) \n",
      "     Test: (17640,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, '\\n     Val:', X_val.shape, '\\n     Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, '\\n     Val:', y_val.shape, '\\n     Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "# Make the data uniform to multivariate timeseries\n",
    "X_train_std = X_train_std.reshape((X_train_std.shape[0], X_train_std.shape[1], 1))\n",
    "X_val_std = X_val_std.reshape((X_val_std.shape[0], X_val_std.shape[1], 1))\n",
    "X_test_std = X_test_std.reshape((X_test_std.shape[0], X_test_std.shape[1], 1))\n",
    "# get automatically the number of classes\n",
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = Constant([np.log(true/pos)])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train_std.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1470/1470 [==============================] - 26s 16ms/step - loss: 0.2254 - f1_m: 0.7905 - accuracy: 0.8952 - val_loss: 0.1080 - val_f1_m: 0.4620 - val_accuracy: 0.9599\n",
      "Epoch 2/20\n",
      "1470/1470 [==============================] - 23s 16ms/step - loss: 0.0974 - f1_m: 0.9360 - accuracy: 0.9634 - val_loss: 0.0714 - val_f1_m: 0.4717 - val_accuracy: 0.9756\n",
      "Epoch 3/20\n",
      "1470/1470 [==============================] - 23s 16ms/step - loss: 0.0784 - f1_m: 0.9499 - accuracy: 0.9719 - val_loss: 0.0840 - val_f1_m: 0.4493 - val_accuracy: 0.9656\n",
      "Epoch 4/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0709 - f1_m: 0.9575 - accuracy: 0.9759 - val_loss: 0.0659 - val_f1_m: 0.4767 - val_accuracy: 0.9770\n",
      "Epoch 5/20\n",
      "1470/1470 [==============================] - 23s 16ms/step - loss: 0.0645 - f1_m: 0.9616 - accuracy: 0.9781 - val_loss: 0.0622 - val_f1_m: 0.4677 - val_accuracy: 0.9777\n",
      "Epoch 6/20\n",
      "1470/1470 [==============================] - 23s 15ms/step - loss: 0.0587 - f1_m: 0.9666 - accuracy: 0.9808 - val_loss: 0.0765 - val_f1_m: 0.4525 - val_accuracy: 0.9680\n",
      "Epoch 7/20\n",
      "1470/1470 [==============================] - 26s 18ms/step - loss: 0.0556 - f1_m: 0.9677 - accuracy: 0.9818 - val_loss: 0.0501 - val_f1_m: 0.4843 - val_accuracy: 0.9823\n",
      "Epoch 8/20\n",
      "1470/1470 [==============================] - 24s 17ms/step - loss: 0.0529 - f1_m: 0.9691 - accuracy: 0.9828 - val_loss: 0.0550 - val_f1_m: 0.4697 - val_accuracy: 0.9800\n",
      "Epoch 9/20\n",
      "1470/1470 [==============================] - 23s 16ms/step - loss: 0.0499 - f1_m: 0.9714 - accuracy: 0.9839 - val_loss: 0.0489 - val_f1_m: 0.4766 - val_accuracy: 0.9845\n",
      "Epoch 10/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0491 - f1_m: 0.9732 - accuracy: 0.9845 - val_loss: 0.0474 - val_f1_m: 0.4827 - val_accuracy: 0.9848\n",
      "Epoch 11/20\n",
      "1470/1470 [==============================] - 25s 17ms/step - loss: 0.0476 - f1_m: 0.9735 - accuracy: 0.9848 - val_loss: 0.0444 - val_f1_m: 0.4856 - val_accuracy: 0.9862\n",
      "Epoch 12/20\n",
      "1470/1470 [==============================] - 26s 18ms/step - loss: 0.0481 - f1_m: 0.9731 - accuracy: 0.9849 - val_loss: 0.0533 - val_f1_m: 0.4774 - val_accuracy: 0.9810\n",
      "Epoch 13/20\n",
      "1470/1470 [==============================] - 26s 18ms/step - loss: 0.0474 - f1_m: 0.9741 - accuracy: 0.9854 - val_loss: 0.0643 - val_f1_m: 0.4845 - val_accuracy: 0.9812\n",
      "Epoch 14/20\n",
      "1470/1470 [==============================] - 25s 17ms/step - loss: 0.0458 - f1_m: 0.9747 - accuracy: 0.9854 - val_loss: 0.0447 - val_f1_m: 0.4844 - val_accuracy: 0.9861\n",
      "Epoch 14: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d64482340>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_intrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val_std, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/368 [==============================] - 3s 6ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.98\n",
      "[[8032   27]\n",
      " [ 136 3565]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_val_std), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 4s 8ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.99\n",
      "F1 score: 0.99\n",
      "[[10744    24]\n",
      " [  192  6680]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test_std), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are still similar to the standard case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation without using the extreme parameters: \n",
    "\n",
    "$\\theta=0.1$ and $\\theta=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'theta'\n",
    "theta_train_list     = [0.1, 0.5]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "theta_test_list      = [0.01, 3]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "\n",
    "# params commons\n",
    "dataset_split_params = {\n",
    "    'window_size': 20, # how large is the window\n",
    "    'overlap_size': 19, # how many time interval of overlap there is between the windows\n",
    "    'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "    'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "    'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "    'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "    'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "    'random_state': 42 # sets the seed for reproducibility\n",
    "}\n",
    "# params for training and validation set\n",
    "train_split = {\n",
    "    'test_size': 0, # size of the test set expressed in percentage\n",
    "    'val_size': 0.2 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "    }\n",
    "dataset_split_params_train = {**dataset_split_params, **train_split}\n",
    "# params for test set\n",
    "test_split =  {\n",
    "    'test_size': 0.3, # size of the test set expressed in percentage\n",
    "    'val_size': 0 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "}                            \n",
    "dataset_split_params_test  = {**dataset_split_params, **test_split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47040, 21), (11760, 21), (17640, 21))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "df_train, df_val, _ = get_dataset_split(grid_X[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        grid_y[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        **dataset_split_params_train)\n",
    "# get the test set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "_, _, df_test = get_dataset_split(grid_X[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  grid_y[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  **dataset_split_params_test)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training set:\n",
      "0    26984\n",
      "1    20056\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Validation set:\n",
      "0    7144\n",
      "1    4616\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Test set:\n",
      "0    12479\n",
      "1     5161\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('## Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "pos = df_train['future_flare'].value_counts()[0]\n",
    "true = df_train['future_flare'].value_counts()[1]\n",
    "print('## Validation set:')\n",
    "print(df_val['future_flare'].value_counts(), '\\n')\n",
    "print('## Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (47040, 20) Val: (11760, 20) Test: (17640, 20)\n",
      "y ## Train: (47040,) Val: (11760,) Test: (17640,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_val_std = scaler.transform(X_val)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "# Make the data uniform to multivariate timeseries\n",
    "X_train_std = X_train_std.reshape((X_train_std.shape[0], X_train_std.shape[1], 1))\n",
    "X_val_std = X_val_std.reshape((X_val_std.shape[0], X_val_std.shape[1], 1))\n",
    "X_test_std = X_test_std.reshape((X_test_std.shape[0], X_test_std.shape[1], 1))\n",
    "# get automatically the number of classes\n",
    "num_classes = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = Constant([np.log(true/pos)])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train_std.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1470/1470 [==============================] - 27s 17ms/step - loss: 0.1433 - f1_m: 0.9050 - accuracy: 0.9349 - val_loss: 0.1011 - val_f1_m: 0.6052 - val_accuracy: 0.9568\n",
      "Epoch 2/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0752 - f1_m: 0.9635 - accuracy: 0.9700 - val_loss: 0.0784 - val_f1_m: 0.6208 - val_accuracy: 0.9661\n",
      "Epoch 3/20\n",
      "1470/1470 [==============================] - 24s 17ms/step - loss: 0.0728 - f1_m: 0.9667 - accuracy: 0.9728 - val_loss: 0.0720 - val_f1_m: 0.6246 - val_accuracy: 0.9711\n",
      "Epoch 4/20\n",
      "1470/1470 [==============================] - 25s 17ms/step - loss: 0.0587 - f1_m: 0.9714 - accuracy: 0.9764 - val_loss: 0.0748 - val_f1_m: 0.6286 - val_accuracy: 0.9702\n",
      "Epoch 5/20\n",
      "1470/1470 [==============================] - 25s 17ms/step - loss: 0.0546 - f1_m: 0.9740 - accuracy: 0.9785 - val_loss: 0.0550 - val_f1_m: 0.6326 - val_accuracy: 0.9790\n",
      "Epoch 6/20\n",
      "1470/1470 [==============================] - 25s 17ms/step - loss: 0.0509 - f1_m: 0.9755 - accuracy: 0.9801 - val_loss: 0.0658 - val_f1_m: 0.6349 - val_accuracy: 0.9765\n",
      "Epoch 7/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0495 - f1_m: 0.9763 - accuracy: 0.9808 - val_loss: 0.0528 - val_f1_m: 0.6348 - val_accuracy: 0.9800\n",
      "Epoch 8/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0462 - f1_m: 0.9792 - accuracy: 0.9830 - val_loss: 0.0652 - val_f1_m: 0.6359 - val_accuracy: 0.9766\n",
      "Epoch 9/20\n",
      "1470/1470 [==============================] - 27s 19ms/step - loss: 0.0461 - f1_m: 0.9793 - accuracy: 0.9832 - val_loss: 0.0501 - val_f1_m: 0.6412 - val_accuracy: 0.9822\n",
      "Epoch 10/20\n",
      "1470/1470 [==============================] - 24s 17ms/step - loss: 0.0445 - f1_m: 0.9798 - accuracy: 0.9836 - val_loss: 0.0609 - val_f1_m: 0.6290 - val_accuracy: 0.9770\n",
      "Epoch 11/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0440 - f1_m: 0.9796 - accuracy: 0.9834 - val_loss: 0.0535 - val_f1_m: 0.6395 - val_accuracy: 0.9809\n",
      "Epoch 12/20\n",
      "1470/1470 [==============================] - 24s 16ms/step - loss: 0.0420 - f1_m: 0.9809 - accuracy: 0.9844 - val_loss: 0.0623 - val_f1_m: 0.6381 - val_accuracy: 0.9746\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d6eca2670>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_extrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val_std, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/368 [==============================] - 3s 7ms/step\n",
      "### Evaluation on validation set ###\n",
      "F1 score: 0.97\n",
      "[[6950  194]\n",
      " [ 105 4511]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_val_std), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_val, y_pred, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552/552 [==============================] - 4s 7ms/step\n",
      "### Evaluation on test set ###\n",
      "F1 score: 0.97\n",
      "[[12166   313]\n",
      " [  127  5034]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test_std), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_test, y_pred, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
