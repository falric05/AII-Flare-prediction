{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 18:23:57.680476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-04 18:23:58.053698: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-04 18:23:58.053778: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-04 18:24:00.155305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 18:24:00.155945: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-04 18:24:00.155989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Libs.load_data import DataLoader\n",
    "from Libs.threshold import get_labels_physic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 30,\n",
       " 'sigma': [0.3, 0.4, 0.5, 0.6],\n",
       " 'theta': [0.01, 0.1, 0.5, 3],\n",
       " 'mu': [0.8, 0.9, 1, 1.1],\n",
       " 'delta': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data loader\n",
    "data_loader = DataLoader()\n",
    "# get the grid\n",
    "grid_X = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "# get physic labels\n",
    "grid_y = get_labels_physic(grid_X, params, alpha=2)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "test_size = 0.33\n",
    "random_state = 42\n",
    "\n",
    "# select best params\n",
    "idx = data_loader.get_standard_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 18020.64it/s]\n",
      "16it [00:00, 29799.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: parametrizzare split on run e shuffle\n",
    "def get_dataset_split(grid_x, grid_y, idx, test_size = 0.33, window_size = 10, overlap_size = 0):\n",
    "    def build_df(X_configuration, y_configuration, window_size=window_size, overlap_size=overlap_size, label_treshold = 1):\n",
    "        stride = window_size - overlap_size\n",
    "        num_windows = (X_configuration.shape[-1]-window_size)//stride + 1\n",
    "\n",
    "        windows = np.zeros((X_configuration.shape[0]*(num_windows-1),window_size))\n",
    "        windows_label = np.zeros((y_configuration.shape[0]*(num_windows-1),window_size), dtype='bool')\n",
    "\n",
    "\n",
    "        for i in range(X_configuration.shape[0]):\n",
    "            tmp_windows = np.array([X_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            tmp_windows_labels = np.array([y_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            windows[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows[:-1,:]\n",
    "            windows_label[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows_labels[1:,:]\n",
    "\n",
    "        windows_label = np.sum(windows_label, axis=-1)\n",
    "        windows_label[windows_label<label_treshold] = 0\n",
    "        windows_label[windows_label>=label_treshold] = 1\n",
    "\n",
    "        df = pd.DataFrame(windows, columns=[f't_{i}' for i in range(windows.shape[-1])]).sample(frac=1)\n",
    "        y_df = pd.DataFrame({'future_flare':windows_label})\n",
    "        df = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    run_test_index = int((1-test_size) * params['run'])\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(params['sigma'], params['theta'], [0.2], [1])):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[:run_test_index, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[:run_test_index, ti, mi, si, di, :])\n",
    "\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df training\n",
    "    df_train = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(params['sigma'], params['theta'], [0.2], [1])):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[run_test_index:, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[run_test_index:, ti, mi, si, di, :])\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df test\n",
    "    df_test = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    119858\n",
      "1     39942\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    60062\n",
      "1    19838\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 159800 159800\n",
      "Test shape:   79900  79900\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',len(X_train), len(y_train))\n",
    "print('Test shape:  ',len(X_test), '', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (159800, 20) (159800,)\n",
      "Test:  (79900, 20) (79900,)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 18:24:04.972308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-04 18:24:04.972616: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-04 18:24:04.972693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (83dc2cdd3c94): /proc/driver/nvidia/version does not exist\n",
      "2023-03-04 18:24:04.973521: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4994/4994 [==============================] - 106s 20ms/step - loss: 0.2558 - f1_m: 0.7414 - accuracy: 0.8898\n",
      "Epoch 2/20\n",
      "4994/4994 [==============================] - 96s 19ms/step - loss: 0.2280 - f1_m: 0.7734 - accuracy: 0.8995\n",
      "Epoch 3/20\n",
      "4994/4994 [==============================] - 95s 19ms/step - loss: 0.2216 - f1_m: 0.7775 - accuracy: 0.9027\n",
      "Epoch 4/20\n",
      "4994/4994 [==============================] - 96s 19ms/step - loss: 0.2159 - f1_m: 0.7849 - accuracy: 0.9053\n",
      "Epoch 5/20\n",
      "4994/4994 [==============================] - 96s 19ms/step - loss: 0.2118 - f1_m: 0.7869 - accuracy: 0.9064\n",
      "Epoch 6/20\n",
      "4994/4994 [==============================] - 97s 19ms/step - loss: 0.2088 - f1_m: 0.7898 - accuracy: 0.9081\n",
      "Epoch 7/20\n",
      "4994/4994 [==============================] - 98s 20ms/step - loss: 0.2078 - f1_m: 0.7933 - accuracy: 0.9093\n",
      "Epoch 8/20\n",
      "4994/4994 [==============================] - 95s 19ms/step - loss: 0.2058 - f1_m: 0.7953 - accuracy: 0.9099\n",
      "Epoch 9/20\n",
      "4994/4994 [==============================] - 93s 19ms/step - loss: 0.2051 - f1_m: 0.7960 - accuracy: 0.9103\n",
      "Epoch 10/20\n",
      "4994/4994 [==============================] - 94s 19ms/step - loss: 0.2041 - f1_m: 0.7963 - accuracy: 0.9103\n",
      "Epoch 11/20\n",
      "4994/4994 [==============================] - 99s 20ms/step - loss: 0.2035 - f1_m: 0.7982 - accuracy: 0.9110\n",
      "Epoch 12/20\n",
      "4994/4994 [==============================] - 92s 18ms/step - loss: 0.2027 - f1_m: 0.7975 - accuracy: 0.9111\n",
      "Epoch 13/20\n",
      "4994/4994 [==============================] - 93s 19ms/step - loss: 0.2023 - f1_m: 0.7970 - accuracy: 0.9110\n",
      "Epoch 14/20\n",
      "4994/4994 [==============================] - 92s 18ms/step - loss: 0.2017 - f1_m: 0.7990 - accuracy: 0.9110\n",
      "Epoch 15/20\n",
      "4994/4994 [==============================] - 94s 19ms/step - loss: 0.2012 - f1_m: 0.7994 - accuracy: 0.9116\n",
      "Epoch 16/20\n",
      "4994/4994 [==============================] - 93s 19ms/step - loss: 0.2026 - f1_m: 0.7986 - accuracy: 0.9113\n",
      "Epoch 17/20\n",
      "4994/4994 [==============================] - 92s 19ms/step - loss: 0.2037 - f1_m: 0.7985 - accuracy: 0.9105\n",
      "Epoch 18/20\n",
      "4994/4994 [==============================] - 96s 19ms/step - loss: 0.2002 - f1_m: 0.7997 - accuracy: 0.9118\n",
      "Epoch 19/20\n",
      "4994/4994 [==============================] - 92s 19ms/step - loss: 0.2959 - f1_m: 0.7992 - accuracy: 0.9110\n",
      "Epoch 20/20\n",
      "4994/4994 [==============================] - 99s 20ms/step - loss: 0.2028 - f1_m: 0.7971 - accuracy: 0.9107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6585d37c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497/2497 [==============================] - 20s 8ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.91\n",
      "F1 score: 0.80\n",
      "[[57191  2871]\n",
      " [ 4683 15155]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "154/154 [==============================] - 2s 9ms/step\n",
    "### Evaluation on test set ###\n",
    "Accuracy: 1.00\n",
    "F1 score: 0.94\n",
    "[[4728    5]\n",
    " [  13  154]]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
