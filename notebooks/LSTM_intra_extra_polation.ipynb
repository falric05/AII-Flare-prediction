{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 15:14:30.664263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 15:14:33.090953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:14:33.091058: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-07 15:14:39.213501: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:14:39.214151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:14:39.214180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Libs.load_data import DataLoader\n",
    "from Libs.threshold import get_labels_physic\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 30,\n",
       " 'sigma': [0.3, 0.4, 0.5, 0.6],\n",
       " 'theta': [0.01, 0.1, 0.5, 3],\n",
       " 'mu': [0.8, 0.9, 1, 1.1],\n",
       " 'delta': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data loader\n",
    "data_loader = DataLoader()\n",
    "# get the grid\n",
    "grid_X = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "# get physic labels\n",
    "grid_y = get_labels_physic(grid_X, params, alpha=2)\n",
    "\n",
    "params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model with multiple all theta parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start seeing what is going to happen with training and testing the NN with all the configurations of theta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "test_size = 0.33\n",
    "random_state = 42\n",
    "\n",
    "sigma_list = [0.5]\n",
    "mu_list    = [1]\n",
    "delta_list = [0.2]\n",
    "theta_train_list = [0.01, 0.1, 0.5, 3]\n",
    "theta_test_list  = [0.01, 0.1, 0.5, 3]\n",
    "\n",
    "# select best params\n",
    "idx = data_loader.get_standard_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 13066.37it/s]\n",
      "2it [00:00, 10824.01it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_split(grid_x, grid_y, idx, test_size = 0.33, window_size = 10, overlap_size = 0):\n",
    "    ###\n",
    "    def build_df(X_configuration, y_configuration, window_size=window_size, overlap_size=overlap_size, label_treshold = 1):\n",
    "        stride = window_size - overlap_size\n",
    "        num_windows = (X_configuration.shape[-1]-window_size)//stride + 1\n",
    "\n",
    "        windows = np.zeros((X_configuration.shape[0]*(num_windows-1),window_size))\n",
    "        windows_label = np.zeros((y_configuration.shape[0]*(num_windows-1),window_size), dtype='bool')\n",
    "\n",
    "\n",
    "        for i in range(X_configuration.shape[0]):\n",
    "            tmp_windows = np.array([X_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            tmp_windows_labels = np.array([y_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            windows[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows[:-1,:]\n",
    "            windows_label[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows_labels[1:,:]\n",
    "\n",
    "        windows_label = np.sum(windows_label, axis=-1)\n",
    "        windows_label[windows_label<label_treshold] = 0\n",
    "        windows_label[windows_label>=label_treshold] = 1\n",
    "\n",
    "        df = pd.DataFrame(windows, columns=[f't_{i}' for i in range(windows.shape[-1])]).sample(frac=1)\n",
    "        y_df = pd.DataFrame({'future_flare':windows_label})\n",
    "        df = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "        return df\n",
    "    ###\n",
    "    run_test_index = int((1-test_size) * params['run'])\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(sigma_list, theta_train_list, delta_list, mu_list)):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[:run_test_index, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[:run_test_index, ti, mi, si, di, :])\n",
    "\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df training\n",
    "    df_train = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(sigma_list, theta_test_list, delta_list, mu_list)):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[run_test_index:, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[run_test_index:, ti, mi, si, di, :])\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df test\n",
    "    df_test = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the amounts of class 0 and 1 for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    29719\n",
      "1    10081\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    15067\n",
      "1     4833\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (39800, 20) (39800,)\n",
      "Test:  (19900, 20) (19900,)\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct now the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 15:21:09.828521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-07 15:21:09.829747: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-07 15:21:09.829921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (83dc2cdd3c94): /proc/driver/nvidia/version does not exist\n",
      "2023-03-07 15:21:09.834536: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "1244/1244 [==============================] - 43s 25ms/step - loss: 0.2583 - f1_m: 0.7822 - accuracy: 0.9090\n",
      "Epoch 2/20\n",
      "1244/1244 [==============================] - 29s 23ms/step - loss: 0.1884 - f1_m: 0.8366 - accuracy: 0.9219\n",
      "Epoch 3/20\n",
      "1244/1244 [==============================] - 33s 27ms/step - loss: 0.1789 - f1_m: 0.8438 - accuracy: 0.9265\n",
      "Epoch 4/20\n",
      "1244/1244 [==============================] - 29s 23ms/step - loss: 0.1751 - f1_m: 0.8451 - accuracy: 0.9267\n",
      "Epoch 5/20\n",
      "1244/1244 [==============================] - 33s 26ms/step - loss: 0.1704 - f1_m: 0.8490 - accuracy: 0.9287\n",
      "Epoch 6/20\n",
      "1244/1244 [==============================] - 33s 26ms/step - loss: 0.1666 - f1_m: 0.8461 - accuracy: 0.9290\n",
      "Epoch 7/20\n",
      "1244/1244 [==============================] - 27s 22ms/step - loss: 0.1625 - f1_m: 0.8511 - accuracy: 0.9312\n",
      "Epoch 8/20\n",
      "1244/1244 [==============================] - 31s 25ms/step - loss: 0.1595 - f1_m: 0.8553 - accuracy: 0.9322\n",
      "Epoch 9/20\n",
      "1244/1244 [==============================] - 67s 54ms/step - loss: 0.1582 - f1_m: 0.8551 - accuracy: 0.9319\n",
      "Epoch 10/20\n",
      "1244/1244 [==============================] - 51s 41ms/step - loss: 0.1561 - f1_m: 0.8555 - accuracy: 0.9331\n",
      "Epoch 11/20\n",
      "1244/1244 [==============================] - 62s 50ms/step - loss: 0.1544 - f1_m: 0.8561 - accuracy: 0.9336\n",
      "Epoch 12/20\n",
      "1244/1244 [==============================] - 47s 38ms/step - loss: 0.1530 - f1_m: 0.8614 - accuracy: 0.9353\n",
      "Epoch 13/20\n",
      "1244/1244 [==============================] - 49s 40ms/step - loss: 0.1522 - f1_m: 0.8608 - accuracy: 0.9359\n",
      "Epoch 14/20\n",
      "1244/1244 [==============================] - 41s 33ms/step - loss: 0.1506 - f1_m: 0.8622 - accuracy: 0.9361\n",
      "Epoch 15/20\n",
      "1244/1244 [==============================] - 49s 39ms/step - loss: 0.1492 - f1_m: 0.8616 - accuracy: 0.9362\n",
      "Epoch 16/20\n",
      "1244/1244 [==============================] - 53s 43ms/step - loss: 0.1469 - f1_m: 0.8656 - accuracy: 0.9375\n",
      "Epoch 17/20\n",
      "1244/1244 [==============================] - 56s 45ms/step - loss: 0.1447 - f1_m: 0.8679 - accuracy: 0.9389\n",
      "Epoch 18/20\n",
      "1244/1244 [==============================] - 63s 51ms/step - loss: 0.1425 - f1_m: 0.8719 - accuracy: 0.9408\n",
      "Epoch 19/20\n",
      "1244/1244 [==============================] - 43s 34ms/step - loss: 0.1398 - f1_m: 0.8741 - accuracy: 0.9409\n",
      "Epoch 20/20\n",
      "1244/1244 [==============================] - 45s 36ms/step - loss: 0.1370 - f1_m: 0.8773 - accuracy: 0.9426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e6854bc40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1244/1244 [==============================] - 20s 16ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.94\n",
      "F1 score: 0.88\n",
      "[[29097   622]\n",
      " [ 1641  8440]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "y_pred = np.round(model.predict(X_train), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_train)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_train)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_train, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622/622 [==============================] - 10s 15ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.94\n",
      "F1 score: 0.87\n",
      "[[14702   365]\n",
      " [  833  4000]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 9157.87it/s]\n",
      "2it [00:00, 5143.23it/s]\n"
     ]
    }
   ],
   "source": [
    "theta_train_list = [0.01, 3]\n",
    "theta_test_list  = [0.1, 0.5]\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (19800, 20) (19800,)\n",
      "Test:  (9900, 20) (9900,)\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "619/619 [==============================] - 30s 37ms/step - loss: 0.2952 - f1_m: 0.7891 - accuracy: 0.8998\n",
      "Epoch 2/20\n",
      "619/619 [==============================] - 24s 39ms/step - loss: 0.2077 - f1_m: 0.8108 - accuracy: 0.9127\n",
      "Epoch 3/20\n",
      "619/619 [==============================] - 22s 35ms/step - loss: 0.2022 - f1_m: 0.8140 - accuracy: 0.9142\n",
      "Epoch 4/20\n",
      "619/619 [==============================] - 19s 31ms/step - loss: 0.1955 - f1_m: 0.8182 - accuracy: 0.9160\n",
      "Epoch 5/20\n",
      "619/619 [==============================] - 22s 35ms/step - loss: 0.1926 - f1_m: 0.8235 - accuracy: 0.9181\n",
      "Epoch 6/20\n",
      "619/619 [==============================] - 15s 24ms/step - loss: 0.1875 - f1_m: 0.8287 - accuracy: 0.9192\n",
      "Epoch 7/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1859 - f1_m: 0.8278 - accuracy: 0.9192\n",
      "Epoch 8/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1834 - f1_m: 0.8351 - accuracy: 0.9214\n",
      "Epoch 9/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1813 - f1_m: 0.8321 - accuracy: 0.9222\n",
      "Epoch 10/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1757 - f1_m: 0.8354 - accuracy: 0.9227\n",
      "Epoch 11/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1739 - f1_m: 0.8397 - accuracy: 0.9239\n",
      "Epoch 12/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1707 - f1_m: 0.8399 - accuracy: 0.9251\n",
      "Epoch 13/20\n",
      "619/619 [==============================] - 15s 24ms/step - loss: 0.1677 - f1_m: 0.8457 - accuracy: 0.9280\n",
      "Epoch 14/20\n",
      "619/619 [==============================] - 14s 22ms/step - loss: 0.1657 - f1_m: 0.8480 - accuracy: 0.9285\n",
      "Epoch 15/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1625 - f1_m: 0.8533 - accuracy: 0.9314\n",
      "Epoch 16/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1638 - f1_m: 0.8518 - accuracy: 0.9292\n",
      "Epoch 17/20\n",
      "619/619 [==============================] - 15s 25ms/step - loss: 0.1613 - f1_m: 0.8568 - accuracy: 0.9319\n",
      "Epoch 18/20\n",
      "619/619 [==============================] - 15s 25ms/step - loss: 0.1596 - f1_m: 0.8578 - accuracy: 0.9318\n",
      "Epoch 19/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1555 - f1_m: 0.8620 - accuracy: 0.9340\n",
      "Epoch 20/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1561 - f1_m: 0.8623 - accuracy: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e685f4310>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 7s 11ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.94\n",
      "F1 score: 0.88\n",
      "[[14166   550]\n",
      " [  691  4393]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "y_pred = np.round(model.predict(X_train), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_train)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_train)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_train, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 3s 10ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.93\n",
      "F1 score: 0.85\n",
      "[[7115  407]\n",
      " [ 302 2076]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are still approximately similar to the standard case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 2299.51it/s]\n",
      "2it [00:00, 10118.95it/s]\n"
     ]
    }
   ],
   "source": [
    "theta_train_list = [0.1, 0.5]\n",
    "theta_test_list  = [0.01, 3]\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (19800, 20) (19800,)\n",
      "Test:  (9900, 20) (9900,)\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "619/619 [==============================] - 22s 26ms/step - loss: 0.2536 - f1_m: 0.7645 - accuracy: 0.9054\n",
      "Epoch 2/20\n",
      "619/619 [==============================] - 12s 19ms/step - loss: 0.1739 - f1_m: 0.8495 - accuracy: 0.9319\n",
      "Epoch 3/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1567 - f1_m: 0.8603 - accuracy: 0.9379\n",
      "Epoch 4/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1485 - f1_m: 0.8703 - accuracy: 0.9402\n",
      "Epoch 5/20\n",
      "619/619 [==============================] - 14s 22ms/step - loss: 0.1429 - f1_m: 0.8787 - accuracy: 0.9441\n",
      "Epoch 6/20\n",
      "619/619 [==============================] - 13s 21ms/step - loss: 0.1406 - f1_m: 0.8782 - accuracy: 0.9434\n",
      "Epoch 7/20\n",
      "619/619 [==============================] - 11s 19ms/step - loss: 0.1337 - f1_m: 0.8797 - accuracy: 0.9457\n",
      "Epoch 8/20\n",
      "619/619 [==============================] - 12s 19ms/step - loss: 0.1324 - f1_m: 0.8814 - accuracy: 0.9458\n",
      "Epoch 9/20\n",
      "619/619 [==============================] - 11s 19ms/step - loss: 0.1307 - f1_m: 0.8890 - accuracy: 0.9484\n",
      "Epoch 10/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1307 - f1_m: 0.8836 - accuracy: 0.9467\n",
      "Epoch 11/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1292 - f1_m: 0.8873 - accuracy: 0.9473\n",
      "Epoch 12/20\n",
      "619/619 [==============================] - 14s 23ms/step - loss: 0.1251 - f1_m: 0.8879 - accuracy: 0.9482\n",
      "Epoch 13/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1268 - f1_m: 0.8878 - accuracy: 0.9484\n",
      "Epoch 14/20\n",
      "619/619 [==============================] - 12s 19ms/step - loss: 0.1268 - f1_m: 0.8875 - accuracy: 0.9492\n",
      "Epoch 15/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1258 - f1_m: 0.8897 - accuracy: 0.9490\n",
      "Epoch 16/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1241 - f1_m: 0.8901 - accuracy: 0.9496\n",
      "Epoch 17/20\n",
      "619/619 [==============================] - 12s 19ms/step - loss: 0.1248 - f1_m: 0.8921 - accuracy: 0.9498\n",
      "Epoch 18/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1221 - f1_m: 0.8931 - accuracy: 0.9499\n",
      "Epoch 19/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1224 - f1_m: 0.8909 - accuracy: 0.9493\n",
      "Epoch 20/20\n",
      "619/619 [==============================] - 11s 18ms/step - loss: 0.1207 - f1_m: 0.8939 - accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e26f06730>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619/619 [==============================] - 5s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.95\n",
      "F1 score: 0.90\n",
      "[[14447   370]\n",
      " [  572  4411]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, y_train, verbose=0)\n",
    "y_pred = np.round(model.predict(X_train), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_train)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_train)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_train, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 2s 8ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.91\n",
      "F1 score: 0.81\n",
      "[[7146  305]\n",
      " [ 562 1887]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
