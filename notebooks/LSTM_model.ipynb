{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 18:48:03.983305: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-06 18:48:04.624614: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-06 18:48:04.624684: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-06 18:48:06.621842: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-06 18:48:06.622116: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-06 18:48:06.622134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Libs.load_data import DataLoader\n",
    "from Libs.threshold import get_labels_physic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 30,\n",
       " 'sigma': [0.3, 0.4, 0.5, 0.6],\n",
       " 'theta': [0.01, 0.1, 0.5, 3],\n",
       " 'mu': [0.8, 0.9, 1, 1.1],\n",
       " 'delta': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data loader\n",
    "data_loader = DataLoader()\n",
    "# get the grid\n",
    "grid_X = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "# get physic labels\n",
    "grid_y = get_labels_physic(grid_X, params, alpha=2)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "test_size = 0.33\n",
    "random_state = 42\n",
    "\n",
    "# select best params\n",
    "idx = data_loader.get_standard_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parametrizzare split on run e shuffle\n",
    "def get_dataset_split(grid_x, grid_y, idx, test_size = 0.33, window_size = 10, overlap_size = 0):\n",
    "    def build_df(X_configuration, y_configuration, window_size=window_size, overlap_size=overlap_size, label_treshold = 1):\n",
    "        # f1 ~ 87% su test set\n",
    "        # window_size = 100\n",
    "        # overlap_size = 90\n",
    "        #------\n",
    "        # f1 ~ 80% su test set\n",
    "\n",
    "        stride = window_size - overlap_size\n",
    "        num_windows = (X_configuration.shape[-1]-window_size)//stride + 1\n",
    "\n",
    "        windows = np.zeros((X_configuration.shape[0]*(num_windows-1),window_size))\n",
    "        windows_label = np.zeros((y_configuration.shape[0]*(num_windows-1),window_size), dtype='bool')\n",
    "\n",
    "\n",
    "        for i in range(X_configuration.shape[0]):\n",
    "            tmp_windows = np.array([X_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            tmp_windows_labels = np.array([y_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            windows[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows[:-1,:]\n",
    "            windows_label[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows_labels[1:,:]\n",
    "\n",
    "        windows_label = np.sum(windows_label, axis=-1)\n",
    "        windows_label[windows_label<label_treshold] = 0\n",
    "        windows_label[windows_label>=label_treshold] = 1\n",
    "\n",
    "        df = pd.DataFrame(windows, columns=[f't_{i}' for i in range(windows.shape[-1])]).sample(frac=1)\n",
    "        y_df = pd.DataFrame({'future_flare':windows_label})\n",
    "        df = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    run_test_index = int((1-test_size) * params['run'])\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = grid_X[:run_test_index, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    y_configuration = grid_y[:run_test_index, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    # df training\n",
    "    df_train = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = grid_X[run_test_index:, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    y_configuration = grid_y[run_test_index:, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    # df test\n",
    "    df_test = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    8873\n",
      "1     927\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    4733\n",
      "1     167\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 9800 9800\n",
      "Test shape:   4900  4900\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',len(X_train), len(y_train))\n",
    "print('Test shape:  ',len(X_test), '', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "n_steps = 3\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9800, 20) (9800,)\n",
      "Test:  (4900, 20) (4900,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test set\n",
    "# X_reshaped = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# X_reshaped = X\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=test_size, random_state=random_state)\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 18:48:13.815294: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-06 18:48:13.816200: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-06 18:48:13.816263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (83dc2cdd3c94): /proc/driver/nvidia/version does not exist\n",
      "2023-03-06 18:48:13.819690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "# model.add(Bidirectional(LSTM(30, return_sequences=True), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "# model.add(LSTM(100))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "# model.compile(loss='binary_crossentropy', optimizer='RMSProp', metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "# weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "307/307 [==============================] - 14s 20ms/step - loss: 0.1586 - f1_m: 0.8446 - accuracy: 0.9768\n",
      "Epoch 2/20\n",
      "307/307 [==============================] - 6s 19ms/step - loss: 0.0271 - f1_m: 0.9002 - accuracy: 0.9888\n",
      "Epoch 3/20\n",
      "307/307 [==============================] - 6s 20ms/step - loss: 0.0262 - f1_m: 0.8807 - accuracy: 0.9889\n",
      "Epoch 4/20\n",
      "307/307 [==============================] - 6s 20ms/step - loss: 0.0275 - f1_m: 0.8733 - accuracy: 0.9881\n",
      "Epoch 5/20\n",
      "307/307 [==============================] - 7s 21ms/step - loss: 0.0262 - f1_m: 0.8801 - accuracy: 0.9887\n",
      "Epoch 6/20\n",
      "307/307 [==============================] - 6s 19ms/step - loss: 0.0256 - f1_m: 0.8902 - accuracy: 0.9900\n",
      "Epoch 7/20\n",
      "307/307 [==============================] - 6s 19ms/step - loss: 0.0256 - f1_m: 0.8947 - accuracy: 0.9894\n",
      "Epoch 8/20\n",
      "307/307 [==============================] - 7s 24ms/step - loss: 0.0280 - f1_m: 0.8860 - accuracy: 0.9896\n",
      "Epoch 9/20\n",
      "307/307 [==============================] - 8s 25ms/step - loss: 0.0232 - f1_m: 0.9034 - accuracy: 0.9909\n",
      "Epoch 10/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0223 - f1_m: 0.8931 - accuracy: 0.9902\n",
      "Epoch 11/20\n",
      "307/307 [==============================] - 6s 21ms/step - loss: 0.0226 - f1_m: 0.8943 - accuracy: 0.9904\n",
      "Epoch 12/20\n",
      "307/307 [==============================] - 6s 20ms/step - loss: 0.0222 - f1_m: 0.8923 - accuracy: 0.9903\n",
      "Epoch 13/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0207 - f1_m: 0.8875 - accuracy: 0.9916\n",
      "Epoch 14/20\n",
      "307/307 [==============================] - 7s 21ms/step - loss: 0.0207 - f1_m: 0.9148 - accuracy: 0.9906\n",
      "Epoch 15/20\n",
      "307/307 [==============================] - 6s 21ms/step - loss: 0.0206 - f1_m: 0.9087 - accuracy: 0.9903\n",
      "Epoch 16/20\n",
      "307/307 [==============================] - 7s 21ms/step - loss: 0.0197 - f1_m: 0.8980 - accuracy: 0.9913\n",
      "Epoch 17/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0188 - f1_m: 0.9134 - accuracy: 0.9920\n",
      "Epoch 18/20\n",
      "307/307 [==============================] - 6s 20ms/step - loss: 0.0194 - f1_m: 0.9134 - accuracy: 0.9919\n",
      "Epoch 19/20\n",
      "307/307 [==============================] - 6s 21ms/step - loss: 0.0191 - f1_m: 0.9335 - accuracy: 0.9912\n",
      "Epoch 20/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0189 - f1_m: 0.9062 - accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5bc5d3580>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 2s 9ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 1.00\n",
      "F1 score: 0.92\n",
      "[[4730    3]\n",
      " [  21  146]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "154/154 [==============================] - 2s 9ms/step\n",
    "### Evaluation on test set ###\n",
    "Accuracy: 1.00\n",
    "F1 score: 0.94\n",
    "[[4728    5]\n",
    " [  13  154]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 1s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.5834242e-04],\n",
       "       [5.1624955e-08],\n",
       "       [2.7284100e-08],\n",
       "       ...,\n",
       "       [2.5183694e-05],\n",
       "       [1.2459799e-09],\n",
       "       [1.6701762e-09]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9800, 20) (9800,)\n",
      "Test shape:   (4900, 20)  (4900,)\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',X_train.shape, y_train.shape)\n",
    "print('Test shape:  ',X_test.shape, '', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9800, 1, 20, 1) (9800,)\n",
      "Test:  (4900, 1, 20, 1) (4900,)\n"
     ]
    }
   ],
   "source": [
    "n_features = 1\n",
    "n_seq = 1\n",
    "n_steps = 4\n",
    "timesteps = X_train.shape[1]\n",
    "X_train = X_train.reshape((X_train.shape[0], n_seq, timesteps, n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], n_seq, timesteps, n_features))\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(TimeDistributed(Conv1D(filters=16, kernel_size=1, activation='relu'), input_shape=(None, X_train.shape[2], n_features)))\n",
    "model2.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "model2.add(TimeDistributed(Flatten()))\n",
    "model2.add(LSTM(15, activation='relu'))\n",
    "model2.add(Dense(30, activation='relu'))\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "307/307 [==============================] - 7s 7ms/step - loss: 0.1698 - f1_m: 0.8657 - accuracy: 0.9847\n",
      "Epoch 2/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0284 - f1_m: 0.8881 - accuracy: 0.9880\n",
      "Epoch 3/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0268 - f1_m: 0.9110 - accuracy: 0.9893\n",
      "Epoch 4/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0263 - f1_m: 0.8785 - accuracy: 0.9888\n",
      "Epoch 5/20\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 0.0244 - f1_m: 0.9003 - accuracy: 0.9896\n",
      "Epoch 6/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0246 - f1_m: 0.8976 - accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 0.0257 - f1_m: 0.8909 - accuracy: 0.9900\n",
      "Epoch 8/20\n",
      "307/307 [==============================] - 2s 6ms/step - loss: 0.0251 - f1_m: 0.8970 - accuracy: 0.9892\n",
      "Epoch 9/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0236 - f1_m: 0.8933 - accuracy: 0.9901\n",
      "Epoch 10/20\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 0.0231 - f1_m: 0.9153 - accuracy: 0.9902\n",
      "Epoch 11/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0217 - f1_m: 0.9051 - accuracy: 0.9905\n",
      "Epoch 12/20\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0209 - f1_m: 0.8958 - accuracy: 0.9910\n",
      "Epoch 13/20\n",
      "307/307 [==============================] - 3s 9ms/step - loss: 0.0199 - f1_m: 0.9028 - accuracy: 0.9917\n",
      "Epoch 14/20\n",
      "307/307 [==============================] - 3s 9ms/step - loss: 0.0204 - f1_m: 0.9252 - accuracy: 0.9914\n",
      "Epoch 15/20\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0196 - f1_m: 0.9170 - accuracy: 0.9914\n",
      "Epoch 16/20\n",
      "307/307 [==============================] - 3s 8ms/step - loss: 0.0194 - f1_m: 0.9231 - accuracy: 0.9919\n",
      "Epoch 17/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0195 - f1_m: 0.9122 - accuracy: 0.9914\n",
      "Epoch 18/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0197 - f1_m: 0.8894 - accuracy: 0.9915\n",
      "Epoch 19/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0190 - f1_m: 0.8994 - accuracy: 0.9916\n",
      "Epoch 20/20\n",
      "307/307 [==============================] - 2s 7ms/step - loss: 0.0193 - f1_m: 0.9186 - accuracy: 0.9922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd582fb5550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 2s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 1.00\n",
      "F1 score: 0.96\n",
      "[[4725    8]\n",
      " [   7  160]]\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model2.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Multi Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 100387.23it/s]\n",
      "64it [00:00, 133483.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_split(grid_x, grid_y, idx, test_size = 0.33, window_size = 10, overlap_size = 0):\n",
    "    def build_df(X_configuration, y_configuration, window_size=window_size, overlap_size=overlap_size, label_treshold = 1):\n",
    "        stride = window_size - overlap_size\n",
    "        num_windows = (X_configuration.shape[-1]-window_size)//stride + 1\n",
    "\n",
    "        windows = np.zeros((X_configuration.shape[0]*(num_windows-1),window_size))\n",
    "        windows_label = np.zeros((y_configuration.shape[0]*(num_windows-1),window_size), dtype='bool')\n",
    "\n",
    "\n",
    "        for i in range(X_configuration.shape[0]):\n",
    "            tmp_windows = np.array([X_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            tmp_windows_labels = np.array([y_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            windows[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows[:-1,:]\n",
    "            windows_label[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows_labels[1:,:]\n",
    "\n",
    "        windows_label = np.sum(windows_label, axis=-1)\n",
    "        windows_label[windows_label<label_treshold] = 0\n",
    "        windows_label[windows_label>=label_treshold] = 1\n",
    "\n",
    "        df = pd.DataFrame(windows, columns=[f't_{i}' for i in range(windows.shape[-1])]).sample(frac=1)\n",
    "        y_df = pd.DataFrame({'future_flare':windows_label})\n",
    "        df = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    run_test_index = int((1-test_size) * params['run'])\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(params['sigma'], params['theta'], [0.2], params['mu'])):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[:run_test_index, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[:run_test_index, ti, mi, si, di, :])\n",
    "\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df training\n",
    "    df_train = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(params['sigma'], params['theta'], [0.2], params['mu'])):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[run_test_index:, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[run_test_index:, ti, mi, si, di, :])\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df test\n",
    "    df_test = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    540001\n",
      "1     99799\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    270158\n",
      "1     49742\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 639800 639800\n",
      "Test shape:   319900  319900\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',len(X_train), len(y_train))\n",
    "print('Test shape:  ',len(X_test), '', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (639800, 20) (639800,)\n",
      "Test:  (319900, 20) (319900,)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19994/19994 [==============================] - 453s 22ms/step - loss: 0.1428 - f1_m: 0.7814 - accuracy: 0.9445\n",
      "Epoch 2/20\n",
      "19994/19994 [==============================] - 341s 17ms/step - loss: 0.1267 - f1_m: 0.7996 - accuracy: 0.9485\n",
      "Epoch 3/20\n",
      "19994/19994 [==============================] - 378s 19ms/step - loss: 0.1220 - f1_m: 0.8041 - accuracy: 0.9501\n",
      "Epoch 4/20\n",
      "19994/19994 [==============================] - 405s 20ms/step - loss: 0.1179 - f1_m: 0.8124 - accuracy: 0.9516\n",
      "Epoch 5/20\n",
      "19994/19994 [==============================] - 496s 25ms/step - loss: 0.1161 - f1_m: 0.8153 - accuracy: 0.9523\n",
      "Epoch 6/20\n",
      "19994/19994 [==============================] - 500s 25ms/step - loss: 0.1147 - f1_m: 0.8178 - accuracy: 0.9530\n",
      "Epoch 7/20\n",
      "19994/19994 [==============================] - 454s 23ms/step - loss: 0.1142 - f1_m: 0.8178 - accuracy: 0.9532\n",
      "Epoch 8/20\n",
      "19994/19994 [==============================] - 402s 20ms/step - loss: 0.1198 - f1_m: 0.8180 - accuracy: 0.9530\n",
      "Epoch 9/20\n",
      "19994/19994 [==============================] - 402s 20ms/step - loss: 0.1134 - f1_m: 0.8190 - accuracy: 0.9534\n",
      "Epoch 10/20\n",
      "19994/19994 [==============================] - 405s 20ms/step - loss: 0.1127 - f1_m: 0.8201 - accuracy: 0.9535\n",
      "Epoch 11/20\n",
      "19994/19994 [==============================] - 411s 21ms/step - loss: 0.1128 - f1_m: 0.8190 - accuracy: 0.9536\n",
      "Epoch 12/20\n",
      "19994/19994 [==============================] - 408s 20ms/step - loss: 0.1116 - f1_m: 0.8193 - accuracy: 0.9537\n",
      "Epoch 13/20\n",
      "19994/19994 [==============================] - 410s 21ms/step - loss: 0.1116 - f1_m: 0.8198 - accuracy: 0.9539\n",
      "Epoch 14/20\n",
      "19994/19994 [==============================] - 406s 20ms/step - loss: 0.1110 - f1_m: 0.8209 - accuracy: 0.9540\n",
      "Epoch 15/20\n",
      "19994/19994 [==============================] - 414s 21ms/step - loss: 0.1113 - f1_m: 0.8207 - accuracy: 0.9541\n",
      "Epoch 16/20\n",
      "19994/19994 [==============================] - 415s 21ms/step - loss: 0.2310 - f1_m: 0.8199 - accuracy: 0.9539\n",
      "Epoch 17/20\n",
      "19994/19994 [==============================] - 407s 20ms/step - loss: 0.1110 - f1_m: 0.8208 - accuracy: 0.9540\n",
      "Epoch 18/20\n",
      "19994/19994 [==============================] - 417s 21ms/step - loss: 0.1115 - f1_m: 0.8214 - accuracy: 0.9541\n",
      "Epoch 19/20\n",
      "19994/19994 [==============================] - 411s 21ms/step - loss: 0.1108 - f1_m: 0.8208 - accuracy: 0.9540\n",
      "Epoch 20/20\n",
      "19994/19994 [==============================] - 414s 21ms/step - loss: 0.1112 - f1_m: 0.8215 - accuracy: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd582ec0580>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997/9997 [==============================] - 63s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.95\n",
      "F1 score: 0.84\n",
      "[[266379   3779]\n",
      " [ 11261  38481]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
