{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 16:38:40.776540: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 16:38:41.470247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-11 16:38:41.470340: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-11 16:38:44.698281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-11 16:38:44.698501: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-11 16:38:44.698517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.initializers import Constant\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from Libs.load_data import ClassificationDataLoader, DataLoader, get_dataset_split\n",
    "from Libs.threshold import get_labels_physic\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 100,\n",
       " 'sigma': [0.5],\n",
       " 'theta': [0.01],\n",
       " 'mu': [1],\n",
       " 'delta': [0.2],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = ClassificationDataLoader(run=100, N=1000, s=0.5, t=0.01, d=0.2, m=1)\n",
    "params = data_loader.get_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 322.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Labels\n",
      "Labels Loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Xs, best_labels = data_loader.load_data(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1000), (100, 1000))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.shape, best_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bindexes = data_loader.get_standard_indexes()\n",
    "df_train,df_val,df_test = get_dataset_split(Xs, best_labels, bindexes, window_size=20, overlap_size=19,\n",
    "                                            label_treshold=1, split_on_run=True, shuffle_run=False, \n",
    "                                            shuffle_window=False, test_size = 0.3, val_size=0.2, \n",
    "                                            get_validation=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    43768\n",
      "1     4252\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "validation set:\n",
      "0    18233\n",
      "1     2347\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    26357\n",
      "1     3043\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "pos = df_train['future_flare'].value_counts()[0]\n",
    "true = df_train['future_flare'].value_counts()[1]\n",
    "print('validation set:')\n",
    "print(df_val['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (48020, 20) Val: (20580, 20) Test: (29400, 20)\n",
      "y ## Train: (48020,) Val: (20580,) Test: (29400,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.3315134962818234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(true/pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 16:40:53.192431: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-11 16:40:53.192637: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-11 16:40:53.192689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (83dc2cdd3c94): /proc/driver/nvidia/version does not exist\n",
      "2023-03-11 16:40:53.193367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = Constant([np.log(true/pos)])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "1501/1501 [==============================] - 37s 22ms/step - loss: 0.0517 - f1_m: 0.8051 - accuracy: 0.9811 - val_loss: 0.0219 - val_f1_m: 0.1485 - val_accuracy: 0.9926\n",
      "Epoch 2/20\n",
      "1501/1501 [==============================] - 48s 32ms/step - loss: 0.0277 - f1_m: 0.8642 - accuracy: 0.9883 - val_loss: 0.0182 - val_f1_m: 0.1450 - val_accuracy: 0.9934\n",
      "Epoch 3/20\n",
      "1501/1501 [==============================] - 44s 29ms/step - loss: 0.0254 - f1_m: 0.8643 - accuracy: 0.9893 - val_loss: 0.0196 - val_f1_m: 0.1477 - val_accuracy: 0.9926\n",
      "Epoch 4/20\n",
      "1501/1501 [==============================] - 41s 28ms/step - loss: 0.0240 - f1_m: 0.8847 - accuracy: 0.9900 - val_loss: 0.0211 - val_f1_m: 0.1450 - val_accuracy: 0.9917\n",
      "Epoch 5/20\n",
      "1501/1501 [==============================] - 37s 25ms/step - loss: 0.0217 - f1_m: 0.8854 - accuracy: 0.9907 - val_loss: 0.0142 - val_f1_m: 0.1474 - val_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "1501/1501 [==============================] - 39s 26ms/step - loss: 0.0197 - f1_m: 0.8947 - accuracy: 0.9916 - val_loss: 0.0163 - val_f1_m: 0.1483 - val_accuracy: 0.9937\n",
      "Epoch 7/20\n",
      "1501/1501 [==============================] - 44s 29ms/step - loss: 0.0173 - f1_m: 0.9071 - accuracy: 0.9931 - val_loss: 0.0120 - val_f1_m: 0.1505 - val_accuracy: 0.9954\n",
      "Epoch 8/20\n",
      "1501/1501 [==============================] - 46s 31ms/step - loss: 0.0156 - f1_m: 0.9076 - accuracy: 0.9937 - val_loss: 0.0105 - val_f1_m: 0.1502 - val_accuracy: 0.9963\n",
      "Epoch 9/20\n",
      "1501/1501 [==============================] - 39s 26ms/step - loss: 0.0143 - f1_m: 0.9182 - accuracy: 0.9946 - val_loss: 0.0095 - val_f1_m: 0.1513 - val_accuracy: 0.9967\n",
      "Epoch 10/20\n",
      "1501/1501 [==============================] - 38s 25ms/step - loss: 0.0124 - f1_m: 0.9158 - accuracy: 0.9952 - val_loss: 0.0158 - val_f1_m: 0.1502 - val_accuracy: 0.9932\n",
      "Epoch 11/20\n",
      "1501/1501 [==============================] - 38s 25ms/step - loss: 0.0127 - f1_m: 0.9207 - accuracy: 0.9947 - val_loss: 0.0083 - val_f1_m: 0.1513 - val_accuracy: 0.9970\n",
      "Epoch 12/20\n",
      "1501/1501 [==============================] - 43s 29ms/step - loss: 0.0116 - f1_m: 0.9208 - accuracy: 0.9953 - val_loss: 0.0104 - val_f1_m: 0.1511 - val_accuracy: 0.9961\n",
      "Epoch 13/20\n",
      "1501/1501 [==============================] - 40s 26ms/step - loss: 0.0109 - f1_m: 0.9161 - accuracy: 0.9957 - val_loss: 0.0152 - val_f1_m: 0.1507 - val_accuracy: 0.9944\n",
      "Epoch 14/20\n",
      "1501/1501 [==============================] - 39s 26ms/step - loss: 0.0110 - f1_m: 0.9237 - accuracy: 0.9959 - val_loss: 0.0078 - val_f1_m: 0.1520 - val_accuracy: 0.9975\n",
      "Epoch 15/20\n",
      "1501/1501 [==============================] - 43s 29ms/step - loss: 0.0102 - f1_m: 0.9215 - accuracy: 0.9960 - val_loss: 0.0074 - val_f1_m: 0.1520 - val_accuracy: 0.9974\n",
      "Epoch 16/20\n",
      "1501/1501 [==============================] - 41s 27ms/step - loss: 0.0102 - f1_m: 0.9358 - accuracy: 0.9963 - val_loss: 0.0077 - val_f1_m: 0.1511 - val_accuracy: 0.9970\n",
      "Epoch 17/20\n",
      "1501/1501 [==============================] - 43s 29ms/step - loss: 0.0094 - f1_m: 0.9317 - accuracy: 0.9965 - val_loss: 0.0096 - val_f1_m: 0.1517 - val_accuracy: 0.9960\n",
      "Epoch 18/20\n",
      "1501/1501 [==============================] - 41s 27ms/step - loss: 0.0092 - f1_m: 0.9289 - accuracy: 0.9965 - val_loss: 0.0068 - val_f1_m: 0.1516 - val_accuracy: 0.9978\n",
      "Epoch 19/20\n",
      "1501/1501 [==============================] - 42s 28ms/step - loss: 0.0088 - f1_m: 0.9239 - accuracy: 0.9969 - val_loss: 0.0073 - val_f1_m: 0.1522 - val_accuracy: 0.9972\n",
      "Epoch 20/20\n",
      "1501/1501 [==============================] - 39s 26ms/step - loss: 0.0088 - f1_m: 0.9270 - accuracy: 0.9967 - val_loss: 0.0074 - val_f1_m: 0.1519 - val_accuracy: 0.9975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc1182b0040>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_best_weights.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "919/919 [==============================] - 8s 9ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 1.00\n",
      "F1 score: 0.99\n",
      "[[26313    44]\n",
      " [   45  2998]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644/644 [==============================] - 5s 8ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 1.00\n",
      "F1 score: 0.99\n",
      "[[18206    27]\n",
      " [   25  2322]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_val, y_val, verbose=0)\n",
    "y_pred = np.round(model.predict(X_val), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with Multi Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:00, 100387.23it/s]\n",
      "64it [00:00, 133483.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_split(grid_x, grid_y, idx, test_size = 0.33, window_size = 10, overlap_size = 0):\n",
    "    def build_df(X_configuration, y_configuration, window_size=window_size, overlap_size=overlap_size, label_treshold = 1):\n",
    "        stride = window_size - overlap_size\n",
    "        num_windows = (X_configuration.shape[-1]-window_size)//stride + 1\n",
    "\n",
    "        windows = np.zeros((X_configuration.shape[0]*(num_windows-1),window_size))\n",
    "        windows_label = np.zeros((y_configuration.shape[0]*(num_windows-1),window_size), dtype='bool')\n",
    "\n",
    "\n",
    "        for i in range(X_configuration.shape[0]):\n",
    "            tmp_windows = np.array([X_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            tmp_windows_labels = np.array([y_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            windows[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows[:-1,:]\n",
    "            windows_label[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows_labels[1:,:]\n",
    "\n",
    "        windows_label = np.sum(windows_label, axis=-1)\n",
    "        windows_label[windows_label<label_treshold] = 0\n",
    "        windows_label[windows_label>=label_treshold] = 1\n",
    "\n",
    "        df = pd.DataFrame(windows, columns=[f't_{i}' for i in range(windows.shape[-1])]).sample(frac=1)\n",
    "        y_df = pd.DataFrame({'future_flare':windows_label})\n",
    "        df = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    run_test_index = int((1-test_size) * params['run'])\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(params['sigma'], params['theta'], [0.2], params['mu'])):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[:run_test_index, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[:run_test_index, ti, mi, si, di, :])\n",
    "\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df training\n",
    "    df_train = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = []\n",
    "    y_configuration = []\n",
    "    # for s, t, d, m in tqdm(product(params['sigma'], params['theta'], params['delta'], params['mu'])):\n",
    "    for s, t, d, m in tqdm(product(params['sigma'], params['theta'], [0.2], params['mu'])):\n",
    "        ti = params['theta'].index(t)\n",
    "        mi = params['mu'].index(m)\n",
    "        si = params['sigma'].index(s)\n",
    "        di = params['delta'].index(d)\n",
    "        X_configuration.append(grid_X[run_test_index:, ti, mi, si, di, :])\n",
    "        y_configuration.append(grid_y[run_test_index:, ti, mi, si, di, :])\n",
    "    X_configuration = np.hstack(X_configuration)\n",
    "    y_configuration = np.hstack(y_configuration)\n",
    "    # df test\n",
    "    df_test = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    540001\n",
      "1     99799\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    270158\n",
      "1     49742\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 639800 639800\n",
      "Test shape:   319900  319900\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',len(X_train), len(y_train))\n",
    "print('Test shape:  ',len(X_test), '', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (639800, 20) (639800,)\n",
      "Test:  (319900, 20) (319900,)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19994/19994 [==============================] - 453s 22ms/step - loss: 0.1428 - f1_m: 0.7814 - accuracy: 0.9445\n",
      "Epoch 2/20\n",
      "19994/19994 [==============================] - 341s 17ms/step - loss: 0.1267 - f1_m: 0.7996 - accuracy: 0.9485\n",
      "Epoch 3/20\n",
      "19994/19994 [==============================] - 378s 19ms/step - loss: 0.1220 - f1_m: 0.8041 - accuracy: 0.9501\n",
      "Epoch 4/20\n",
      "19994/19994 [==============================] - 405s 20ms/step - loss: 0.1179 - f1_m: 0.8124 - accuracy: 0.9516\n",
      "Epoch 5/20\n",
      "19994/19994 [==============================] - 496s 25ms/step - loss: 0.1161 - f1_m: 0.8153 - accuracy: 0.9523\n",
      "Epoch 6/20\n",
      "19994/19994 [==============================] - 500s 25ms/step - loss: 0.1147 - f1_m: 0.8178 - accuracy: 0.9530\n",
      "Epoch 7/20\n",
      "19994/19994 [==============================] - 454s 23ms/step - loss: 0.1142 - f1_m: 0.8178 - accuracy: 0.9532\n",
      "Epoch 8/20\n",
      "19994/19994 [==============================] - 402s 20ms/step - loss: 0.1198 - f1_m: 0.8180 - accuracy: 0.9530\n",
      "Epoch 9/20\n",
      "19994/19994 [==============================] - 402s 20ms/step - loss: 0.1134 - f1_m: 0.8190 - accuracy: 0.9534\n",
      "Epoch 10/20\n",
      "19994/19994 [==============================] - 405s 20ms/step - loss: 0.1127 - f1_m: 0.8201 - accuracy: 0.9535\n",
      "Epoch 11/20\n",
      "19994/19994 [==============================] - 411s 21ms/step - loss: 0.1128 - f1_m: 0.8190 - accuracy: 0.9536\n",
      "Epoch 12/20\n",
      "19994/19994 [==============================] - 408s 20ms/step - loss: 0.1116 - f1_m: 0.8193 - accuracy: 0.9537\n",
      "Epoch 13/20\n",
      "19994/19994 [==============================] - 410s 21ms/step - loss: 0.1116 - f1_m: 0.8198 - accuracy: 0.9539\n",
      "Epoch 14/20\n",
      "19994/19994 [==============================] - 406s 20ms/step - loss: 0.1110 - f1_m: 0.8209 - accuracy: 0.9540\n",
      "Epoch 15/20\n",
      "19994/19994 [==============================] - 414s 21ms/step - loss: 0.1113 - f1_m: 0.8207 - accuracy: 0.9541\n",
      "Epoch 16/20\n",
      "19994/19994 [==============================] - 415s 21ms/step - loss: 0.2310 - f1_m: 0.8199 - accuracy: 0.9539\n",
      "Epoch 17/20\n",
      "19994/19994 [==============================] - 407s 20ms/step - loss: 0.1110 - f1_m: 0.8208 - accuracy: 0.9540\n",
      "Epoch 18/20\n",
      "19994/19994 [==============================] - 417s 21ms/step - loss: 0.1115 - f1_m: 0.8214 - accuracy: 0.9541\n",
      "Epoch 19/20\n",
      "19994/19994 [==============================] - 411s 21ms/step - loss: 0.1108 - f1_m: 0.8208 - accuracy: 0.9540\n",
      "Epoch 20/20\n",
      "19994/19994 [==============================] - 414s 21ms/step - loss: 0.1112 - f1_m: 0.8215 - accuracy: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd582ec0580>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9997/9997 [==============================] - 63s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.95\n",
      "F1 score: 0.84\n",
      "[[266379   3779]\n",
      " [ 11261  38481]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
