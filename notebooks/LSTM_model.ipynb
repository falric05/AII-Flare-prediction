{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "from Libs.load_data import DataLoader\n",
    "from Libs import flares_plot as fplt\n",
    "from Libs.threshold import get_labels_physic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run': 30,\n",
       " 'sigma': [0.3, 0.4, 0.5, 0.6],\n",
       " 'theta': [0.01, 0.1, 0.5, 3],\n",
       " 'mu': [0.8, 0.9, 1, 1.1],\n",
       " 'delta': [0.01, 0.05, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7],\n",
       " 'N': 1000}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize data loader\n",
    "data_loader = DataLoader()\n",
    "# get the grid\n",
    "grid_X = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "# get physic labels\n",
    "grid_y = get_labels_physic(grid_X, params, alpha=2)\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 0\n",
    "test_size = 0.33\n",
    "random_state = 42\n",
    "\n",
    "# select best params\n",
    "idx = data_loader.get_standard_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parametrizzare split on run e shuffle\n",
    "def get_dataset_split(grid_x, grid_y, idx, test_size = 0.33, window_size = 10, overlap_size = 0):\n",
    "    def build_df(X_configuration, y_configuration, window_size=window_size, overlap_size=overlap_size, label_treshold = 1):\n",
    "        # f1 ~ 87% su test set\n",
    "        # window_size = 100\n",
    "        # overlap_size = 90\n",
    "        #------\n",
    "        # f1 ~ 80% su test set\n",
    "\n",
    "        stride = window_size - overlap_size\n",
    "        num_windows = (X_configuration.shape[-1]-window_size)//stride + 1\n",
    "\n",
    "        windows = np.zeros((X_configuration.shape[0]*(num_windows-1),window_size))\n",
    "        windows_label = np.zeros((y_configuration.shape[0]*(num_windows-1),window_size), dtype='bool')\n",
    "\n",
    "\n",
    "        for i in range(X_configuration.shape[0]):\n",
    "            tmp_windows = np.array([X_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            tmp_windows_labels = np.array([y_configuration[i,j:j+window_size] for j in range(0,stride*num_windows,stride)])\n",
    "            windows[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows[:-1,:]\n",
    "            windows_label[i*(num_windows-1):(i+1)*(num_windows-1)] = tmp_windows_labels[1:,:]\n",
    "\n",
    "        windows_label = np.sum(windows_label, axis=-1)\n",
    "        windows_label[windows_label<label_treshold] = 0\n",
    "        windows_label[windows_label>=label_treshold] = 1\n",
    "\n",
    "        df = pd.DataFrame(windows, columns=[f't_{i}' for i in range(windows.shape[-1])]).sample(frac=1)\n",
    "        y_df = pd.DataFrame({'future_flare':windows_label})\n",
    "        df = pd.concat([df, y_df], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    run_test_index = int((1-test_size) * params['run'])\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = grid_X[:run_test_index, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    y_configuration = grid_y[:run_test_index, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    # df training\n",
    "    df_train = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    # build the dataframe\n",
    "    X_configuration = grid_X[run_test_index:, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    y_configuration = grid_y[run_test_index:, idx[0], idx[1], idx[2], idx[3], :]\n",
    "    # df test\n",
    "    df_test = build_df(X_configuration, y_configuration)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = get_dataset_split(grid_X, grid_y, idx, window_size=20, overlap_size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "0    8873\n",
      "1     927\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "Test set:\n",
      "0    4733\n",
      "1     167\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "print(df_train['future_flare'].value_counts(), '\\n')\n",
    "print('Test set:')\n",
    "print(df_test['future_flare'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: 9800 9800\n",
      "Test shape:   4900  4900\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',len(X_train), len(y_train))\n",
    "print('Test shape:  ',len(X_test), '', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "n_steps = 3\n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9800, 20) (9800,)\n",
      "Test:  (4900, 20) (4900,)\n"
     ]
    }
   ],
   "source": [
    "# split train and test set\n",
    "# X_reshaped = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# X_reshaped = X\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=test_size, random_state=random_state)\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "# model.add(Bidirectional(LSTM(30, return_sequences=True), input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train.shape[1], 1)))\n",
    "# model.add(LSTM(100))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])\n",
    "# model.compile(loss='binary_crossentropy', optimizer='RMSProp', metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "# Calculate the weights for each class so that we can balance the data\n",
    "# weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "307/307 [==============================] - 8s 14ms/step - loss: 0.2211 - f1_m: 0.8333 - accuracy: 0.9739\n",
      "Epoch 2/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0332 - f1_m: 0.8884 - accuracy: 0.9869\n",
      "Epoch 3/20\n",
      "307/307 [==============================] - 5s 16ms/step - loss: 0.0279 - f1_m: 0.8951 - accuracy: 0.9889\n",
      "Epoch 4/20\n",
      "307/307 [==============================] - 5s 15ms/step - loss: 0.0258 - f1_m: 0.8789 - accuracy: 0.9888\n",
      "Epoch 5/20\n",
      "307/307 [==============================] - 6s 18ms/step - loss: 0.0266 - f1_m: 0.8923 - accuracy: 0.9889\n",
      "Epoch 6/20\n",
      "307/307 [==============================] - 5s 18ms/step - loss: 0.0235 - f1_m: 0.8981 - accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0234 - f1_m: 0.9032 - accuracy: 0.9895\n",
      "Epoch 8/20\n",
      "307/307 [==============================] - 7s 24ms/step - loss: 0.0224 - f1_m: 0.9109 - accuracy: 0.9907\n",
      "Epoch 9/20\n",
      "307/307 [==============================] - 7s 23ms/step - loss: 0.0249 - f1_m: 0.8868 - accuracy: 0.9899\n",
      "Epoch 10/20\n",
      "307/307 [==============================] - 5s 18ms/step - loss: 0.0220 - f1_m: 0.8945 - accuracy: 0.9909\n",
      "Epoch 11/20\n",
      "307/307 [==============================] - 6s 19ms/step - loss: 0.0206 - f1_m: 0.8926 - accuracy: 0.9912\n",
      "Epoch 12/20\n",
      "307/307 [==============================] - 6s 20ms/step - loss: 0.0205 - f1_m: 0.9090 - accuracy: 0.9914\n",
      "Epoch 13/20\n",
      "307/307 [==============================] - 6s 19ms/step - loss: 0.0196 - f1_m: 0.9040 - accuracy: 0.9919\n",
      "Epoch 14/20\n",
      "307/307 [==============================] - 6s 21ms/step - loss: 0.0206 - f1_m: 0.9080 - accuracy: 0.9902\n",
      "Epoch 15/20\n",
      "307/307 [==============================] - 6s 21ms/step - loss: 0.0199 - f1_m: 0.9134 - accuracy: 0.9915\n",
      "Epoch 16/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0216 - f1_m: 0.8894 - accuracy: 0.9915\n",
      "Epoch 17/20\n",
      "307/307 [==============================] - 7s 22ms/step - loss: 0.0225 - f1_m: 0.9246 - accuracy: 0.9901\n",
      "Epoch 18/20\n",
      "307/307 [==============================] - 7s 23ms/step - loss: 0.0201 - f1_m: 0.8906 - accuracy: 0.9912\n",
      "Epoch 19/20\n",
      "307/307 [==============================] - 7s 23ms/step - loss: 0.0199 - f1_m: 0.9243 - accuracy: 0.9917\n",
      "Epoch 20/20\n",
      "307/307 [==============================] - 7s 23ms/step - loss: 0.0194 - f1_m: 0.9139 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa34f3d0940>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 3s 13ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 1.00\n",
      "F1 score: 0.95\n",
      "[[4729    4]\n",
      " [  12  155]]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "154/154 [==============================] - 2s 9ms/step\n",
    "### Evaluation on test set ###\n",
    "Accuracy: 1.00\n",
    "F1 score: 0.94\n",
    "[[4728    5]\n",
    " [  13  154]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 2s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (9800, 20) (9800,)\n",
      "Test shape:   (4900, 20)  (4900,)\n"
     ]
    }
   ],
   "source": [
    "# extract X and y from training dataframe\n",
    "X_train = df_train.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_train = df_train['future_flare'].to_numpy()\n",
    "\n",
    "# extract X and y from test dataframe\n",
    "X_test = df_test.drop(['future_flare'], axis=1).to_numpy()\n",
    "y_test = df_test['future_flare'].to_numpy()\n",
    "\n",
    "print('Train shape:',X_train.shape, y_train.shape)\n",
    "print('Test shape:  ',X_test.shape, '', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9800, 1, 20, 1) (9800,)\n",
      "Test:  (4900, 1, 20, 1) (4900,)\n"
     ]
    }
   ],
   "source": [
    "n_features = 1\n",
    "n_seq = 1\n",
    "n_steps = 4\n",
    "timesteps = X_train.shape[1]\n",
    "X_train = X_train.reshape((X_train.shape[0], n_seq, timesteps, n_features))\n",
    "X_test = X_test.reshape((X_test.shape[0], n_seq, timesteps, n_features))\n",
    "\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(TimeDistributed(Conv1D(filters=16, kernel_size=1, activation='relu'), input_shape=(None, X_train.shape[2], n_features)))\n",
    "model2.add(TimeDistributed(MaxPooling1D(pool_size=4)))\n",
    "model2.add(TimeDistributed(Flatten()))\n",
    "model2.add(LSTM(15, activation='relu'))\n",
    "model2.add(Dense(30, activation='relu'))\n",
    "model2.add(Dense(10, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1_m, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "307/307 [==============================] - 14s 13ms/step - loss: 0.1872 - f1_m: 0.8113 - accuracy: 0.9780\n",
      "Epoch 2/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0273 - f1_m: 0.8962 - accuracy: 0.9887\n",
      "Epoch 3/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0255 - f1_m: 0.9037 - accuracy: 0.9890\n",
      "Epoch 4/20\n",
      "307/307 [==============================] - 5s 16ms/step - loss: 0.0249 - f1_m: 0.9036 - accuracy: 0.9909\n",
      "Epoch 5/20\n",
      "307/307 [==============================] - 5s 18ms/step - loss: 0.0247 - f1_m: 0.9010 - accuracy: 0.9902\n",
      "Epoch 6/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0254 - f1_m: 0.8804 - accuracy: 0.9898\n",
      "Epoch 7/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0240 - f1_m: 0.8860 - accuracy: 0.9897\n",
      "Epoch 8/20\n",
      "307/307 [==============================] - 5s 16ms/step - loss: 0.0249 - f1_m: 0.8929 - accuracy: 0.9893\n",
      "Epoch 9/20\n",
      "307/307 [==============================] - 5s 15ms/step - loss: 0.0239 - f1_m: 0.9050 - accuracy: 0.9905\n",
      "Epoch 10/20\n",
      "307/307 [==============================] - 4s 14ms/step - loss: 0.0238 - f1_m: 0.9134 - accuracy: 0.9902\n",
      "Epoch 11/20\n",
      "307/307 [==============================] - 3s 10ms/step - loss: 0.0230 - f1_m: 0.9083 - accuracy: 0.9905\n",
      "Epoch 12/20\n",
      "307/307 [==============================] - 3s 9ms/step - loss: 0.0229 - f1_m: 0.9062 - accuracy: 0.9907\n",
      "Epoch 13/20\n",
      "307/307 [==============================] - 3s 11ms/step - loss: 0.0221 - f1_m: 0.9052 - accuracy: 0.9901\n",
      "Epoch 14/20\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0225 - f1_m: 0.9113 - accuracy: 0.9892\n",
      "Epoch 15/20\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0201 - f1_m: 0.9036 - accuracy: 0.9917\n",
      "Epoch 16/20\n",
      "307/307 [==============================] - 4s 12ms/step - loss: 0.0202 - f1_m: 0.8964 - accuracy: 0.9912\n",
      "Epoch 17/20\n",
      "307/307 [==============================] - 4s 13ms/step - loss: 0.0206 - f1_m: 0.9257 - accuracy: 0.9907\n",
      "Epoch 18/20\n",
      "307/307 [==============================] - 5s 15ms/step - loss: 0.0209 - f1_m: 0.8977 - accuracy: 0.9915\n",
      "Epoch 19/20\n",
      "307/307 [==============================] - 5s 17ms/step - loss: 0.0189 - f1_m: 0.9140 - accuracy: 0.9919\n",
      "Epoch 20/20\n",
      "307/307 [==============================] - 6s 19ms/step - loss: 0.0196 - f1_m: 0.8847 - accuracy: 0.9912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa318a427f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(X_train, y_train, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 1s 5ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 1.00\n",
      "F1 score: 0.93\n",
      "[[4730    3]\n",
      " [  20  147]]\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "y_pred = np.round(model2.predict(X_test), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test)))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
