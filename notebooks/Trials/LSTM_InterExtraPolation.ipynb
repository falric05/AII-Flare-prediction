{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 20:44:23.775242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 20:44:24.058435: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-21 20:44:24.058496: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-21 20:44:25.284612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 20:44:25.284794: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 20:44:25.284812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from Libs.config import inter_extra_data_folder\n",
    "from Libs.load_data import DataLoader, get_dataset_split\n",
    "from Libs.keras_f1score import f1_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1, 4, 1, 1, 1000), (30, 1, 4, 1, 1, 1000))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_std = False\n",
    "\n",
    "# initialize data loader\n",
    "data_loader = DataLoader(run=30, N=1000, s=0.5, t=[0.01, 0.1, 0.5, 3], d=0.2, m=1, \n",
    "                         override=False, folder=inter_extra_data_folder)\n",
    "# get the grid\n",
    "grid_X, grid_y = data_loader.get_grid()\n",
    "# get params dictionary\n",
    "params = data_loader.get_params()\n",
    "\n",
    "grid_X.shape, grid_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biggest assumption when training ANNs is the following: \n",
    "\n",
    "\"We assume that training sets and test sets contains independent and identically distributed samples from the same unknown distribution $p_{data}(x,y)$\"\n",
    "\n",
    "This is a very important assumption that in general affect the performance ANNs, in particular classifier ones. We could, indeed, explore what can happen if we violete the following assumption. This a relevant application case, for exaple in cases when the generation parameters are not known."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model with multiple all theta parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start seeing what is going to happen with training and testing the NN with all the configurations of theta parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10976, 21), (5488, 21), (7056, 21))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split_params = {\n",
    "    'window_size': 20, # how large is the window\n",
    "    'overlap_size': 15, # how many time interval of overlap there is between the windows\n",
    "    'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "    'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "    'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "    'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "    'test_size': 0.3, # size of the test set expressed in percentage\n",
    "    'val_size': 0.2, # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "    'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "    'random_state': 42 # sets the seed for reproducibility\n",
    "}\n",
    "df_train,df_val,df_test = get_dataset_split(grid_X, grid_y, **dataset_split_params)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows the amounts of class 0 and 1 for each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training set:\n",
      "0    7197\n",
      "1    3779\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Validation set:\n",
      "0    3564\n",
      "1    1924\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Test set:\n",
      "0    4547\n",
      "1    2509\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "train_counts = df_train['future_flare'].value_counts()\n",
    "print(train_counts, '\\n')\n",
    "print('validation set:')\n",
    "val_counts = df_val['future_flare'].value_counts()\n",
    "print(val_counts, '\\n')\n",
    "print('Test set:')\n",
    "test_counts = df_test['future_flare'].value_counts()\n",
    "print(test_counts, '\\n')\n",
    "print('Total:')\n",
    "total_counts = train_counts.add(val_counts).add(test_counts)\n",
    "print(total_counts, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (10976, 20) Val: (5488, 20) Test: (7056, 20)\n",
      "y ## Train: (10976,) Val: (5488,) Test: (7056,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_std:\n",
    "    # Standardize Data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    # get automatically the number of classes\n",
    "    num_classes = len(np.unique(y))\n",
    "else:\n",
    "    X_train_std = X_train\n",
    "    X_val_std = X_val\n",
    "    X_test_std = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct now the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 20:44:27.409507: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-21 20:44:27.409613: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-21 20:44:27.409672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (44910f15382a): /proc/driver/nvidia/version does not exist\n",
      "2023-03-21 20:44:27.410626: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 40)               3520      \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = np.log([train_counts[1]/train_counts[0]])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train_std.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "343/343 [==============================] - 10s 20ms/step - loss: 0.3604 - f1_m: 0.6737 - accuracy: 0.8285 - val_loss: 0.2867 - val_f1_m: 0.6327 - val_accuracy: 0.8741\n",
      "Epoch 2/20\n",
      "343/343 [==============================] - 6s 19ms/step - loss: 0.2769 - f1_m: 0.7986 - accuracy: 0.8717 - val_loss: 0.2666 - val_f1_m: 0.6462 - val_accuracy: 0.8832\n",
      "Epoch 3/20\n",
      "343/343 [==============================] - 6s 18ms/step - loss: 0.2733 - f1_m: 0.7996 - accuracy: 0.8746 - val_loss: 0.2611 - val_f1_m: 0.6506 - val_accuracy: 0.8858\n",
      "Epoch 4/20\n",
      "343/343 [==============================] - 6s 16ms/step - loss: 0.2592 - f1_m: 0.8153 - accuracy: 0.8843 - val_loss: 0.2519 - val_f1_m: 0.6582 - val_accuracy: 0.8888\n",
      "Epoch 5/20\n",
      "343/343 [==============================] - 6s 18ms/step - loss: 0.2472 - f1_m: 0.8300 - accuracy: 0.8915 - val_loss: 0.2593 - val_f1_m: 0.6169 - val_accuracy: 0.8799\n",
      "Epoch 6/20\n",
      "343/343 [==============================] - 5s 16ms/step - loss: 0.2400 - f1_m: 0.8327 - accuracy: 0.8936 - val_loss: 0.2643 - val_f1_m: 0.6022 - val_accuracy: 0.8781\n",
      "Epoch 7/20\n",
      "343/343 [==============================] - 6s 17ms/step - loss: 0.2339 - f1_m: 0.8406 - accuracy: 0.8978 - val_loss: 0.2250 - val_f1_m: 0.6540 - val_accuracy: 0.9018\n",
      "Epoch 8/20\n",
      "343/343 [==============================] - 6s 17ms/step - loss: 0.2303 - f1_m: 0.8425 - accuracy: 0.8992 - val_loss: 0.2634 - val_f1_m: 0.6073 - val_accuracy: 0.8786\n",
      "Epoch 9/20\n",
      "343/343 [==============================] - 6s 17ms/step - loss: 0.2177 - f1_m: 0.8531 - accuracy: 0.9065 - val_loss: 0.2087 - val_f1_m: 0.6675 - val_accuracy: 0.9103\n",
      "Epoch 10/20\n",
      "343/343 [==============================] - 6s 17ms/step - loss: 0.2196 - f1_m: 0.8454 - accuracy: 0.9030 - val_loss: 0.2128 - val_f1_m: 0.6689 - val_accuracy: 0.9105\n",
      "Epoch 11/20\n",
      "343/343 [==============================] - 6s 16ms/step - loss: 0.2128 - f1_m: 0.8561 - accuracy: 0.9077 - val_loss: 0.2211 - val_f1_m: 0.6527 - val_accuracy: 0.9014\n",
      "Epoch 12/20\n",
      "343/343 [==============================] - 6s 18ms/step - loss: 0.2109 - f1_m: 0.8581 - accuracy: 0.9105 - val_loss: 0.2026 - val_f1_m: 0.6696 - val_accuracy: 0.9113\n",
      "Epoch 13/20\n",
      "343/343 [==============================] - 5s 16ms/step - loss: 0.2073 - f1_m: 0.8600 - accuracy: 0.9125 - val_loss: 0.1985 - val_f1_m: 0.6787 - val_accuracy: 0.9131\n",
      "Epoch 14/20\n",
      "343/343 [==============================] - 5s 16ms/step - loss: 0.2001 - f1_m: 0.8697 - accuracy: 0.9169 - val_loss: 0.1954 - val_f1_m: 0.6808 - val_accuracy: 0.9156\n",
      "Epoch 15/20\n",
      "343/343 [==============================] - 6s 16ms/step - loss: 0.1989 - f1_m: 0.8644 - accuracy: 0.9137 - val_loss: 0.2106 - val_f1_m: 0.6577 - val_accuracy: 0.9062\n",
      "Epoch 16/20\n",
      "343/343 [==============================] - 6s 16ms/step - loss: 0.1962 - f1_m: 0.8668 - accuracy: 0.9164 - val_loss: 0.1933 - val_f1_m: 0.6782 - val_accuracy: 0.9140\n",
      "Epoch 17/20\n",
      "343/343 [==============================] - 6s 16ms/step - loss: 0.1896 - f1_m: 0.8735 - accuracy: 0.9206 - val_loss: 0.1908 - val_f1_m: 0.6830 - val_accuracy: 0.9218\n",
      "Epoch 18/20\n",
      "343/343 [==============================] - 6s 17ms/step - loss: 0.1888 - f1_m: 0.8727 - accuracy: 0.9203 - val_loss: 0.1867 - val_f1_m: 0.6875 - val_accuracy: 0.9246\n",
      "Epoch 19/20\n",
      "343/343 [==============================] - 6s 17ms/step - loss: 0.1864 - f1_m: 0.8772 - accuracy: 0.9226 - val_loss: 0.1831 - val_f1_m: 0.6908 - val_accuracy: 0.9238\n",
      "Epoch 20/20\n",
      "343/343 [==============================] - 5s 16ms/step - loss: 0.1831 - f1_m: 0.8789 - accuracy: 0.9233 - val_loss: 0.1765 - val_f1_m: 0.6928 - val_accuracy: 0.9280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f792c7e94f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_allTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val_std, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 1s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.93\n",
      "F1 score: 0.92\n",
      "[[3480   84]\n",
      " [ 311 1613]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_val_std), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 2s 8ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.92\n",
      "F1 score: 0.92\n",
      "[[4432  115]\n",
      " [ 418 2091]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test_std), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation with using just the extreme parameters: \n",
    "\n",
    "$\\theta=0.01$ and $\\theta=3$\n",
    "\n",
    "and a fraction of the other dataset, coming from $\\theta=0.1$ and $\\theta=0.5$ as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'theta'\n",
    "theta_train_list     = [0.01, 3]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "theta_test_list      = [0.1, 0.5]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "# params commons\n",
    "dataset_split_params = {\n",
    "    'window_size': 20, # how large is the window\n",
    "    'overlap_size': 15, # how many time interval of overlap there is between the windows\n",
    "    'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "    'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "    'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "    'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "    'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "    'random_state': 42 # sets the seed for reproducibility\n",
    "}\n",
    "# params for training and validation set\n",
    "train_split = {\n",
    "    'test_size': 0, # size of the test set expressed in percentage\n",
    "    'val_size': 0.2 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "    }\n",
    "dataset_split_params_train = {**dataset_split_params, **train_split}\n",
    "# params for test set\n",
    "test_split =  {\n",
    "    'test_size': 0.3, # size of the test set expressed in percentage\n",
    "    'val_size': 0 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "}                            \n",
    "dataset_split_params_test  = {**dataset_split_params, **test_split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9408, 21), (2352, 21), (3528, 21))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "df_train, df_val, _ = get_dataset_split(grid_X[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        grid_y[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        **dataset_split_params_train)\n",
    "# get the test set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "_, _, df_test = get_dataset_split(grid_X[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  grid_y[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  **dataset_split_params_test)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training set:\n",
      "0    6703\n",
      "1    2705\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Validation set:\n",
      "0    1680\n",
      "1     672\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Test set:\n",
      "0    2042\n",
      "1    1486\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "train_counts = df_train['future_flare'].value_counts()\n",
    "print(train_counts, '\\n')\n",
    "print('validation set:')\n",
    "val_counts = df_val['future_flare'].value_counts()\n",
    "print(val_counts, '\\n')\n",
    "print('Test set:')\n",
    "test_counts = df_test['future_flare'].value_counts()\n",
    "print(test_counts, '\\n')\n",
    "print('Total:')\n",
    "total_counts = train_counts.add(val_counts).add(test_counts)\n",
    "print(total_counts, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (9408, 20) \n",
      "     Val: (2352, 20) \n",
      "     Test: (3528, 20)\n",
      "y ## Train: (9408,) \n",
      "     Val: (2352,) \n",
      "     Test: (3528,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, '\\n     Val:', X_val.shape, '\\n     Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, '\\n     Val:', y_val.shape, '\\n     Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_std:\n",
    "    # Standardize Data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    # get automatically the number of classes\n",
    "    num_classes = len(np.unique(y))\n",
    "else:\n",
    "    X_train_std = X_train\n",
    "    X_val_std = X_val\n",
    "    X_test_std = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = np.log([train_counts[1]/train_counts[0]])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train_std.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "294/294 [==============================] - 7s 17ms/step - loss: 0.3828 - f1_m: 0.5799 - accuracy: 0.8193 - val_loss: 0.3035 - val_f1_m: 0.4774 - val_accuracy: 0.8444\n",
      "Epoch 2/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2967 - f1_m: 0.6980 - accuracy: 0.8485 - val_loss: 0.2872 - val_f1_m: 0.4766 - val_accuracy: 0.8584\n",
      "Epoch 3/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2893 - f1_m: 0.7043 - accuracy: 0.8540 - val_loss: 0.2761 - val_f1_m: 0.4886 - val_accuracy: 0.8661\n",
      "Epoch 4/20\n",
      "294/294 [==============================] - 5s 15ms/step - loss: 0.2782 - f1_m: 0.7241 - accuracy: 0.8600 - val_loss: 0.2675 - val_f1_m: 0.4664 - val_accuracy: 0.8665\n",
      "Epoch 5/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2633 - f1_m: 0.7552 - accuracy: 0.8744 - val_loss: 0.2837 - val_f1_m: 0.5247 - val_accuracy: 0.8605\n",
      "Epoch 6/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2501 - f1_m: 0.7795 - accuracy: 0.8867 - val_loss: 0.2492 - val_f1_m: 0.5112 - val_accuracy: 0.8793\n",
      "Epoch 7/20\n",
      "294/294 [==============================] - 5s 15ms/step - loss: 0.2506 - f1_m: 0.7662 - accuracy: 0.8845 - val_loss: 0.2425 - val_f1_m: 0.5303 - val_accuracy: 0.8895\n",
      "Epoch 8/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2366 - f1_m: 0.7936 - accuracy: 0.8933 - val_loss: 0.2748 - val_f1_m: 0.4436 - val_accuracy: 0.8614\n",
      "Epoch 9/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2340 - f1_m: 0.7906 - accuracy: 0.8958 - val_loss: 0.2348 - val_f1_m: 0.5424 - val_accuracy: 0.8980\n",
      "Epoch 10/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2234 - f1_m: 0.8068 - accuracy: 0.9013 - val_loss: 0.2395 - val_f1_m: 0.5495 - val_accuracy: 0.8941\n",
      "Epoch 11/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2184 - f1_m: 0.8145 - accuracy: 0.9039 - val_loss: 0.2309 - val_f1_m: 0.5478 - val_accuracy: 0.9001\n",
      "Epoch 12/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2159 - f1_m: 0.8156 - accuracy: 0.9059 - val_loss: 0.2357 - val_f1_m: 0.5420 - val_accuracy: 0.8907\n",
      "Epoch 13/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2172 - f1_m: 0.8143 - accuracy: 0.9058 - val_loss: 0.2240 - val_f1_m: 0.5565 - val_accuracy: 0.9005\n",
      "Epoch 14/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2090 - f1_m: 0.8227 - accuracy: 0.9083 - val_loss: 0.2202 - val_f1_m: 0.5440 - val_accuracy: 0.9026\n",
      "Epoch 15/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2065 - f1_m: 0.8281 - accuracy: 0.9114 - val_loss: 0.2148 - val_f1_m: 0.5513 - val_accuracy: 0.9069\n",
      "Epoch 16/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2029 - f1_m: 0.8312 - accuracy: 0.9129 - val_loss: 0.2175 - val_f1_m: 0.5599 - val_accuracy: 0.9052\n",
      "Epoch 17/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.2014 - f1_m: 0.8292 - accuracy: 0.9136 - val_loss: 0.2135 - val_f1_m: 0.5654 - val_accuracy: 0.9065\n",
      "Epoch 18/20\n",
      "294/294 [==============================] - 4s 15ms/step - loss: 0.1991 - f1_m: 0.8375 - accuracy: 0.9144 - val_loss: 0.2133 - val_f1_m: 0.5676 - val_accuracy: 0.9060\n",
      "Epoch 19/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.1976 - f1_m: 0.8372 - accuracy: 0.9150 - val_loss: 0.2123 - val_f1_m: 0.5650 - val_accuracy: 0.9065\n",
      "Epoch 20/20\n",
      "294/294 [==============================] - 6s 20ms/step - loss: 0.1930 - f1_m: 0.8391 - accuracy: 0.9172 - val_loss: 0.2126 - val_f1_m: 0.5718 - val_accuracy: 0.9107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f79354ff550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_intrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val_std, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 6ms/step\n",
      "### Evaluation on validation set ###\n",
      "Accuracy: 0.91\n",
      "F1 score: 0.89\n",
      "[[1601   79]\n",
      " [ 131  541]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_val_std), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_val)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_val, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 8ms/step\n",
      "### Evaluation on test set ###\n",
      "Accuracy: 0.92\n",
      "F1 score: 0.92\n",
      "[[1995   47]\n",
      " [ 220 1266]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test_std), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_pred, y_test)))\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_pred, y_test, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are still similar to the standard case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before, but now we are fitting only in the dataset coming from the generation without using the extreme parameters: \n",
    "\n",
    "$\\theta=0.1$ and $\\theta=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 'theta'\n",
    "theta_train_list     = [0.1, 0.5]\n",
    "theta_train_list_idx = [params[p].index(t) for t in theta_train_list]\n",
    "theta_test_list      = [0.01, 3]\n",
    "theta_test_list_idx  = [params[p].index(t) for t in theta_test_list]\n",
    "\n",
    "# params commons\n",
    "dataset_split_params = {\n",
    "    'window_size': 20, # how large is the window\n",
    "    'overlap_size': 15, # how many time interval of overlap there is between the windows\n",
    "    'label_treshold': 1, # how many labels have to be at 1 in the window_size to consider the current window as a flare\n",
    "    'split_on_run': True, # if True the windows of a run cannot be on different sets\n",
    "    'shuffle_run': False, # if True shuffles the order of the runs before computing the windows\n",
    "    'shuffle_window': False, # if True shuffles the order of the windows in the resulting dataframes\n",
    "    'get_validation': True, # if True the output would be train,val,test set, otherwise it would be train,test\n",
    "    'random_state': 42 # sets the seed for reproducibility\n",
    "}\n",
    "# params for training and validation set\n",
    "train_split = {\n",
    "    'test_size': 0, # size of the test set expressed in percentage\n",
    "    'val_size': 0.2 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "    }\n",
    "dataset_split_params_train = {**dataset_split_params, **train_split}\n",
    "# params for test set\n",
    "test_split =  {\n",
    "    'test_size': 0.3, # size of the test set expressed in percentage\n",
    "    'val_size': 0 # size of the validation set expressed in percentage, considered only if get_validation is True\n",
    "}                            \n",
    "dataset_split_params_test  = {**dataset_split_params, **test_split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9408, 21), (2352, 21), (3528, 21))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the train and validation set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "df_train, df_val, _ = get_dataset_split(grid_X[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        grid_y[:,:,theta_train_list_idx,:,:,:], \n",
    "                                        **dataset_split_params_train)\n",
    "# get the test set, selecting the index for grid given the interpolation assuption\n",
    "# notice that theta is the third parameter\n",
    "_, _, df_test = get_dataset_split(grid_X[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  grid_y[:,:,theta_test_list_idx,:,:,:], \n",
    "                                  **dataset_split_params_test)\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training set:\n",
      "0    5557\n",
      "1    3851\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Validation set:\n",
      "0    1368\n",
      "1     984\n",
      "Name: future_flare, dtype: int64 \n",
      "\n",
      "## Test set:\n",
      "0    2505\n",
      "1    1023\n",
      "Name: future_flare, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# number of classes\n",
    "print('Training set:')\n",
    "train_counts = df_train['future_flare'].value_counts()\n",
    "print(train_counts, '\\n')\n",
    "print('validation set:')\n",
    "val_counts = df_val['future_flare'].value_counts()\n",
    "print(val_counts, '\\n')\n",
    "print('Test set:')\n",
    "test_counts = df_test['future_flare'].value_counts()\n",
    "print(test_counts, '\\n')\n",
    "print('Total:')\n",
    "total_counts = train_counts.add(val_counts).add(test_counts)\n",
    "print(total_counts, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X ## Train: (9408, 20) Val: (2352, 20) Test: (3528, 20)\n",
      "y ## Train: (9408,) Val: (2352,) Test: (3528,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = df_train.iloc[:,:-1].to_numpy(), df_train.future_flare.to_numpy()\n",
    "X_val, y_val = df_val.iloc[:,:-1].to_numpy(), df_val.future_flare.to_numpy()\n",
    "X_test, y_test = df_test.iloc[:,:-1].to_numpy(), df_test.future_flare.to_numpy()\n",
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train, y_val, y_test))\n",
    "print('X ## Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "print('y ## Train:', y_train.shape, 'Val:', y_val.shape, 'Test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_std:\n",
    "    # Standardize Data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_std = scaler.transform(X_train)\n",
    "    X_val_std = scaler.transform(X_val)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    # get automatically the number of classes\n",
    "    num_classes = len(np.unique(y))\n",
    "else:\n",
    "    X_train_std = X_train\n",
    "    X_val_std = X_val\n",
    "    X_test_std = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 40)               3520      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                310       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,071\n",
      "Trainable params: 5,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "initial_bias = np.log([train_counts[1]/train_counts[0]])\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(20, activation='relu'), input_shape=(X_train_std.shape[1], 1)))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid',bias_initializer=initial_bias))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=[f1_m, 'accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "294/294 [==============================] - 8s 21ms/step - loss: 0.3604 - f1_m: 0.8263 - accuracy: 0.8702 - val_loss: 0.2765 - val_f1_m: 0.7459 - val_accuracy: 0.8839\n",
      "Epoch 2/20\n",
      "294/294 [==============================] - 5s 18ms/step - loss: 0.2566 - f1_m: 0.8500 - accuracy: 0.8903 - val_loss: 0.2633 - val_f1_m: 0.7562 - val_accuracy: 0.8912\n",
      "Epoch 3/20\n",
      "294/294 [==============================] - 5s 18ms/step - loss: 0.2481 - f1_m: 0.8575 - accuracy: 0.8918 - val_loss: 0.2589 - val_f1_m: 0.7629 - val_accuracy: 0.8941\n",
      "Epoch 4/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2365 - f1_m: 0.8660 - accuracy: 0.8990 - val_loss: 0.2515 - val_f1_m: 0.7377 - val_accuracy: 0.8886\n",
      "Epoch 5/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2327 - f1_m: 0.8705 - accuracy: 0.9019 - val_loss: 0.2322 - val_f1_m: 0.7668 - val_accuracy: 0.9001\n",
      "Epoch 6/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2252 - f1_m: 0.8758 - accuracy: 0.9060 - val_loss: 0.2441 - val_f1_m: 0.7725 - val_accuracy: 0.8971\n",
      "Epoch 7/20\n",
      "294/294 [==============================] - 5s 17ms/step - loss: 0.2162 - f1_m: 0.8784 - accuracy: 0.9083 - val_loss: 0.2289 - val_f1_m: 0.7660 - val_accuracy: 0.8984\n",
      "Epoch 8/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2171 - f1_m: 0.8832 - accuracy: 0.9117 - val_loss: 0.2352 - val_f1_m: 0.7669 - val_accuracy: 0.8967\n",
      "Epoch 9/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2087 - f1_m: 0.8866 - accuracy: 0.9139 - val_loss: 0.2352 - val_f1_m: 0.7778 - val_accuracy: 0.9014\n",
      "Epoch 10/20\n",
      "294/294 [==============================] - 5s 16ms/step - loss: 0.2068 - f1_m: 0.8863 - accuracy: 0.9151 - val_loss: 0.2259 - val_f1_m: 0.7719 - val_accuracy: 0.9005\n",
      "Epoch 11/20\n",
      "294/294 [==============================] - 5s 18ms/step - loss: 0.2047 - f1_m: 0.8884 - accuracy: 0.9153 - val_loss: 0.2150 - val_f1_m: 0.7731 - val_accuracy: 0.9056\n",
      "Epoch 12/20\n",
      "294/294 [==============================] - 5s 17ms/step - loss: 0.1984 - f1_m: 0.8949 - accuracy: 0.9197 - val_loss: 0.2241 - val_f1_m: 0.7717 - val_accuracy: 0.9014\n",
      "Epoch 13/20\n",
      "294/294 [==============================] - 6s 20ms/step - loss: 0.1996 - f1_m: 0.8914 - accuracy: 0.9170 - val_loss: 0.2140 - val_f1_m: 0.7835 - val_accuracy: 0.9082\n",
      "Epoch 14/20\n",
      "294/294 [==============================] - 5s 17ms/step - loss: 0.1981 - f1_m: 0.8959 - accuracy: 0.9204 - val_loss: 0.2260 - val_f1_m: 0.7749 - val_accuracy: 0.9026\n",
      "Epoch 15/20\n",
      "294/294 [==============================] - 5s 17ms/step - loss: 0.2000 - f1_m: 0.8935 - accuracy: 0.9170 - val_loss: 0.2214 - val_f1_m: 0.7862 - val_accuracy: 0.9065\n",
      "Epoch 16/20\n",
      "294/294 [==============================] - 5s 17ms/step - loss: 0.2182 - f1_m: 0.8773 - accuracy: 0.9075 - val_loss: 0.2365 - val_f1_m: 0.7605 - val_accuracy: 0.8937\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f78f3778850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "# define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(\"models\", \"LSTM_extrpTheta_checkpoint.h5\"), save_weights_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1),\n",
    "]\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train_std,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_val_std, y_val),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 5ms/step\n",
      "### Evaluation on validation set ###\n",
      "F1 score: 0.89\n",
      "[[1283   85]\n",
      " [ 165  819]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_val_std), 0)\n",
    "\n",
    "print(\"### Evaluation on validation set ###\")\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_val, y_pred, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_val, y_pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 1s 6ms/step\n",
      "### Evaluation on test set ###\n",
      "F1 score: 0.82\n",
      "[[2156  349]\n",
      " [ 187  836]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(model.predict(X_test_std), 0)\n",
    "\n",
    "print(\"### Evaluation on test set ###\")\n",
    "print(\"F1 score: %.2f\" % (f1_score(y_test, y_pred, average='macro')))\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
